{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the home of solid-state physics \u00b6 This site exists to enhance the distribution of course information and content for the solid-state physics component of KYA322: Statistical and solid-state physics . All official communication will be though MyLO 1 . A Crocoite sample found near Dundas in western Tasmania. As part of this course, we will investigate the what happens when atoms are no longer considered in isolation, and see that systems of interacting particles can have some pretty incredible outcomes Course expectations: my expectations of you Solid-state physics is where the rubber meets the road: abstract concepts will be applied in an effort to model realistic and complex systems. There are delights are to be had, but like most truly rewarding endeavours, said delights do not come for free. In undertaking this course, it is expected that: You will view the video content before any scheduled face-to-face sessions Prescribed problems and reading will be undertaken before any face-to-face sessions You will attend all face-to-face sessions, and actively contribute to discussions and problem solving If you are experiencing difficulties, that you will get in contact as soon as is reasonably possible Course objectives The purpose of this course is to provide an introduction to the world of solid-state physics. At the conclusion of your journey, you should: Be familiar with the main models of solid-state physics, and their application to real-world systems Have proficiency parsing and extracting information from applied solid-state systems, with an eye to identification the relevant theoretical framework(s) and the concise formulation of physics to be investigated Have experience identifying and applying appropriate approximations and physical insight to make difficult problems more tractable Be familiar with experimental and analytical apparatus relevant to probing solid-state systems, in addition to some of the main systems and devices one might encounter in the wild Have had some fun! Course expectations: my promises to you Rightly, you should have expectations of me. It is my intention that: I will work to communicate my understanding and insight in the course material I will actively seek input to steer and shape the content discussed, and develop relevant resources I will be widely available for consultation and discussion I will do my best to cultivate a safe and open forum for discussion If you think that I am not fulfilling my commitments, or if you have other comments, I would hope that you are comfortable conveying your concerns and/or comments to me, either directly or anonymously. It is my genuine desire to help you navigate learning new concepts and ideas, and I want to do this to the best of my ability, and feedback is the best way to ensure this endeavour proceeds as smoothly as possible. For the curious traveller, MyLO is The University of Tasmania's learning management system, based on Brightspace as developed D2L. \u21a9","title":"Home"},{"location":"#welcome-to-the-home-of-solid-state-physics","text":"This site exists to enhance the distribution of course information and content for the solid-state physics component of KYA322: Statistical and solid-state physics . All official communication will be though MyLO 1 . A Crocoite sample found near Dundas in western Tasmania. As part of this course, we will investigate the what happens when atoms are no longer considered in isolation, and see that systems of interacting particles can have some pretty incredible outcomes Course expectations: my expectations of you Solid-state physics is where the rubber meets the road: abstract concepts will be applied in an effort to model realistic and complex systems. There are delights are to be had, but like most truly rewarding endeavours, said delights do not come for free. In undertaking this course, it is expected that: You will view the video content before any scheduled face-to-face sessions Prescribed problems and reading will be undertaken before any face-to-face sessions You will attend all face-to-face sessions, and actively contribute to discussions and problem solving If you are experiencing difficulties, that you will get in contact as soon as is reasonably possible Course objectives The purpose of this course is to provide an introduction to the world of solid-state physics. At the conclusion of your journey, you should: Be familiar with the main models of solid-state physics, and their application to real-world systems Have proficiency parsing and extracting information from applied solid-state systems, with an eye to identification the relevant theoretical framework(s) and the concise formulation of physics to be investigated Have experience identifying and applying appropriate approximations and physical insight to make difficult problems more tractable Be familiar with experimental and analytical apparatus relevant to probing solid-state systems, in addition to some of the main systems and devices one might encounter in the wild Have had some fun! Course expectations: my promises to you Rightly, you should have expectations of me. It is my intention that: I will work to communicate my understanding and insight in the course material I will actively seek input to steer and shape the content discussed, and develop relevant resources I will be widely available for consultation and discussion I will do my best to cultivate a safe and open forum for discussion If you think that I am not fulfilling my commitments, or if you have other comments, I would hope that you are comfortable conveying your concerns and/or comments to me, either directly or anonymously. It is my genuine desire to help you navigate learning new concepts and ideas, and I want to do this to the best of my ability, and feedback is the best way to ensure this endeavour proceeds as smoothly as possible. For the curious traveller, MyLO is The University of Tasmania's learning management system, based on Brightspace as developed D2L. \u21a9","title":"Welcome to the home of solid-state physics"},{"location":"Lost/","text":"Placeholder \u00b6 Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon! Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Thermal expansion \u00b6 While the quadratic approximation of the interatomic potential is certainly the most important, the anharmonic term \\kappa_3(r-a)^3/6 also has physical significance. Let us examine its role visually by comparing the harmonic and the third order approximations in the plot below. Notice that the second-order approximation is symmetric around the minimum while the third-order term is not. r = np . linspace ( - 2 , 2.5 , 750 ) a = 0 b = 0 c = 1 d = - 0.2 U_quadratic = a + b * r + c * r ** 2 U_cubic = a + b * r + c * r ** 2 + d * r ** 3 r_min = - 2 r_max = 2.5 U_min = - 0.2 U_max = 4 E_t_min = 0.4 E_t_max = 3.5 N_values = 20 l_width = 1.5 N_active = 0 # Create figure fig = go . Figure () def U_c ( r ): return a + b * r + c * r ** 2 + d * r ** 3 def line ( E_t ): right = min ( np . roots ([ c , b , a - E_t ])) roots = np . roots ([ d , c , b , a - E_t ]) roots = np . real ( roots [ np . isreal ( roots )]) roots . sort () left = roots [ 1 ] return [ left , right ] def avg_pos_cubic ( E_t ): Z = integrate . simps ( np . exp ( - U_cubic / E_t ), r ) r_avg = integrate . simps ( r * np . exp ( - U_cubic / E_t ), r ) x = r_avg / Z return x # Add traces, one for each slider step for E_t in np . linspace ( E_t_min , E_t_max , N_values ): avg = avg_pos_cubic ( E_t ) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_quadratic , mode = 'lines' , line_color = 'blue' , name = \"Quadratic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_cubic , mode = 'lines' , line_color = 'red' , name = \"Cubic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = line ( E_t ), y = [ E_t , E_t ], mode = 'lines' , line_color = 'black' , name = r 'Thermal energy level' )) fig . add_trace ( go . Scatter ( visible = False , x = [ 0 , 0 ], y = [ 0 , E_t ], mode = 'lines' , line_color = 'blue' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the quadratic potential' )) fig . add_trace ( go . Scatter ( visible = False , x = [ avg , avg ], y = [ U_c ( avg ), E_t ], mode = 'lines' , line_color = 'red' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the cubic potential' )) # Initial starting image N_trace = int ( len ( fig . data ) / N_values ) # Number of traces added per step for j in range ( N_trace ): fig . data [ N_active * N_trace + j ] . visible = True # Creation of the aditional images steps = [] for i in range ( int ( len ( fig . data ) / N_trace )): step = dict ( method = \"restyle\" , args = [{ \"visible\" : [ False ] * len ( fig . data )}], value = str ( 0.1 * ( i + 1 )) ) for j in range ( N_trace ): step [ \"args\" ][ 0 ][ \"visible\" ][ N_trace * i + j ] = True # Toggle i'th trace to \"visible\" steps . append ( step ) # Creating the slider sliders = [ dict ( tickcolor = 'White' , font_color = 'White' , currentvalue_font_color = 'Black' , active = N_active , name = r 'Thermal Energy' , font_size = 16 , currentvalue = { \"prefix\" : r 'Thermal Energy k_B T: ' }, pad = { \"t\" : 50 }, steps = steps , )] # Updating the images for each step fig . update_layout ( sliders = sliders , showlegend = True , plot_bgcolor = 'rgb(254, 254, 254)' , width = 700 , height = 580 , xaxis = dict ( range = [ r_min , r_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), yaxis = dict ( range = [ U_min , U_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), title = { 'text' : r 'Thermal expansion of cubic potential' , 'y' : 0.9 , 'x' : 0.45 , 'xanchor' : 'center' , 'yanchor' : 'top' }, xaxis_title = r '$r$' , yaxis_title = r '$U [k_b T]$' , ) # Edit slider labels and adding text next to the horizontal bar indicating T_E for i in range ( N_values ): fig [ 'layout' ][ 'sliders' ][ 0 ][ 'steps' ][ i ][ 'label' ] = ' %.1f ' % (( E_t_max - E_t_min ) * i / ( N_values - 1 ) + E_t_min ) # Showing the figure plt . plot ( fig ) fig . show () The asymmetry due to nonzero \\kappa_3 slows the growth of the potential when the interatomic distance increases. On the other hand, when the interatomic distance decreases, the asymmetry accelerates the growth of the potential. Therefore, stretching the material is more energetically favorable than contracting it. As a result, thermal excitations increase the interatomic distance. This gives us a simple model of thermal expansion . Van der Waals bond \u00b6 While we focus on the mechanisms of covalent bonding, let us also review another bond type. A Van der Waals bond originates from an attraction between the dipole moments of two atoms. Suppose we have two atoms separated by an interatomic distance r . If one atom has a dipole moment \\mathbf{p_1} , it creates an electric field \\mathbf{E} = \\frac{\\mathbf{p_1}}{4\\pi \\varepsilon_0 r^3} at the position of the other atom. The other atom then develops a dipole moment \\mathbf{p_2} = \\chi \\mathbf{E} with \\chi the polarizability of the atom. The potential energy between between the two dipoles is \\begin{align} U(r) &= \\frac{-|\\mathbf{p_1}||\\mathbf{p_2}|}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}| \\chi \\mathbf{E}}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}|^2 \\chi}{(4\\pi\\varepsilon_0 r^3)^2}\\\\ &\\propto \\frac{1}{r^6}. \\end{align} The dipole attraction is much weaker than the covalent bonds but drops slower with increasing distance. How does the strength of a covalent bond scale with distance? The strength of the bond is determined by the interatomic hopping integral -t = \\langle 1 | H | 2 \\rangle . Since the wavefunction of a bound electron typically decays exponentially, so does the overlap integral. Although the Van der Waals force is weak, it is the only force when there are no chemically active electrons or when the atoms are too far apart to form covalent bonds. Therefore, there are materials where Van der Waals interactions are the dominant interactions. An example of such a material is graphite. The Van der Waals bonds in graphite hold layers of covalently bonded carbon atoms together: (image source: Wikipedia ) Looking ahead: multiple atoms \u00b6 So far we have only considered the interatomic interactions between diatomic systems. However, our aim is to understand electrons and phonons in solids containing N\\to\\infty atoms. Let us see what happens when we consider more than two atoms. Phonons \u00b6 In order to understand phonons better, we need to understand how a vibrational motion in a solid arises. To that end, we model an array of atoms that are connected by springs with a spring constant \\kappa . Our plan: Consider only a harmonic potential acting between the atoms ( \\kappa is constant) Write down equations of motion Compute normal modes For simplicity we consider 1D motion, and let us start with a chain of 3 atoms: # Defining constants y_max = 6 max_m = 3 # Off set for the new masses deviation_arr = [ + 2 , - 2 , - 3.5 ] def plot_spring ( x1 , x2 , y , annotation = r '$\\kappa$' ): L = ( x2 - x1 ) width , nturns = 1 , 10 N = 1000 pad = 200 # Make the spring, unit scale (0 to 1) x_spring = np . linspace ( 0 , L , N ) # distance along the spring y_spring = np . zeros ( N ) y_spring [ pad : - pad ] = width * np . sin ( 2 * np . pi * nturns * x_spring [ pad : - pad ] / L ) x_plot , y_plot = np . vstack (( x_spring , y_spring )) # And offset it x_plot += x1 y_plot += y ax . plot ( x_plot , y_plot , c = 'k' ) ax . annotate ( annotation , (( x2 + x1 ) / 2 , y + 2 ), fontsize = 40 ) def plot_mass ( x , y , annotation = r '$m$' ): mass = Circle (( x , y ), . 5 , fc = 'k' ) ax . add_patch ( mass ) ax . annotate ( annotation , ( x -. 5 , y + 2 ), fontsize = 30 ) def make_plot ( u , deviation_arr , max_masses = 3 ): # plotting initial masses plot_mass ( 0 , 0 ) plot_mass ( 0 + deviation_arr [ 0 ], u , annotation = '' ) # Plot initial dotted lines pyplot . plot (( 0 , 0 ), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( deviation_arr [ 0 ], deviation_arr [ 0 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((0, deviation_arr[0]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 0 , u - 2.5 , deviation_arr [ 0 ] -. 5 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotate the deviations annot_arr = [ r '$u_1$' , r '$u_2$' , r '$u_3$' ] off_set = - 1 ax . annotate ( annot_arr [ 0 ], ( deviation_arr [ 0 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Annotate large arrow containing the interatomic distance pyplot . arrow ( 0 , u / 3 , 9 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 5 , head_length = 1 ) ax . annotate ( r '$a$' , ( 5 , u / 3 + off_set ), fontsize = 30 ) # Plotting other masses and springs for j in range ( max_masses - 1 ): # Equilibrium plot plot_spring ( 10 * j , 10 * ( j + 1 ), 0 ) plot_mass ( 10 * ( j + 1 ), 0 ) # Plot containing deviations from it's equilibrium plot_spring ( 10 * j + deviation_arr [ j ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) plot_mass ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) # Plot dotted lines pyplot . plot (( 10 * ( j + 1 ), 10 * ( j + 1 )), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((10*(j+1), 10*(j+1)+deviation_arr[j+1]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 10 * ( j + 1 ), u - 2.5 , ( deviation_arr [ j + 1 ] +. 5 ), 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotations ax . annotate ( annot_arr [ j + 1 ], ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Initializing figure fig , ax = pyplot . subplots ( figsize = ( 10 , 7 )) pyplot . axis ( 'off' ) ax . set_xlim (( - 1 , 10 * ( max_m - 1 ) + 1 )) ax . set_ylim (( - y_max - 4.5 , y_max - 2.5 )) # Plottig system make_plot ( - y_max , deviation_arr , max_m ); fig . show () Let us denote the deviation of atom i from its equilibrium position by u_i . Newton's equations of motion for this system are then given by \\begin{aligned} m \\ddot{u}_1 &= - \\kappa (u_1 - u_2) \\\\ m \\ddot{u}_2 &= - \\kappa (u_2 - u_1) - \\kappa (u_2 - u_3) \\\\ m \\ddot{u}_3 &= - \\kappa (u_3 - u_2). \\end{aligned}, We write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} We are interested in phonons, patterns of motion that are periodic and have a fixed frequency \\omega . Hence we guess that the motion of the atoms is \\mathbf{u}(t) = \\mathbf{u}_0 e^{i\\omega t}. We substitute our guess into the equations of motion to yield an eigenvalue problem: \\omega^2 \\mathbf{u}_0 = \\frac{\\kappa}{m} \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u}_0. The solutions to the eigenvalue problem are phonon modes. Electrons \u00b6 We just looked at how a chain of atoms moves. Let us now look at how the electrons of those atoms behave. To that end, we consider a 3 atom chain without any motion. In order to understand how electrons behave, we use the LCAO model. The LCAO model generalizes in a very simple way. Let us consider the wavefunction: \\vert \\psi\\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. Because the three atoms are identical, the onsite energy is the same on all atoms \\langle 1|H|1 \\rangle = \\langle2|H|2 \\rangle = \\langle3|H|3 \\rangle = E_0 . Furthermore, we assume hopping only between the nearest neighbors and assume that it is real valued: \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t . We also assume that the orbitals are orthogonal to eachother. Just as we did in the previous lecture, we use the Schr\u00f6dinger equation H |\\psi\\rangle = E |\\psi\\rangle to set up a system of equations: \\begin{align} E \\varphi_1 &= E_0 \\varphi_1 - t \\varphi_2\\\\ E \\varphi_2 &= E_0 \\varphi_2 - t \\varphi_1 - t \\varphi_3\\\\ E \\varphi_3 &= E_0 \\varphi_3 -t \\varphi_2. \\end{align} Again, we write this in a matrix form: E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} E_0 & -t & 0 \\\\ -t & E_0 & -t \\\\ 0 & -t & E_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix} Numerical test \u00b6 Diagonalizing large matrices is unwieldy, but let's try and check it numerically to see if we notice a trend. Let us first model 3 atoms on a chain. The eigenfrequencies of the 3 atoms are: [0.0 1.0 1.732050] def DOS_finite_phonon_chain ( n ): rhs = 2 * np . eye ( n ) - np . eye ( n , k = 1 ) - np . eye ( n , k =- 1 ) rhs [ 0 , 0 ] -= 1 rhs [ - 1 , - 1 ] -= 1 pyplot . figure () pyplot . hist ( np . sqrt ( np . abs ( np . linalg . eigvalsh ( rhs ))), bins = 30 ) pyplot . xlabel ( \"$\\omega$\" ) pyplot . ylabel ( \"Number of eigenfrequencies\" ) DOS_finite_phonon_chain ( 3 ) The eigenenergies of the 3 orbitals are: [-1.41421356 0.0 1.41421356] def DOS_finite_electron_chain ( n ): rhs = 2 * np . eye ( n , k = 0 ) - np . eye ( n , k = 1 ) - np . eye ( n , k = - 1 ) pyplot . figure () pyplot . hist ( np . linalg . eigvalsh ( rhs ), bins = 30 ) pyplot . xlabel ( \"$E$\" ) pyplot . ylabel ( \"Number of eigenenergies\" ) DOS_finite_electron_chain ( 3 ) However, 3 atoms are far too few to model an actual solid. Hence, we need 'many more' atoms. From 3 atoms to 300 \u00b6 Phonon modes of the many atom chain are shown below. DOS_finite_phonon_chain ( 300 ) We observe that when \\omega is small, we have a constant DOS. This is in line with what we saw in the Debye model. There the DOS of a 1D system was constant! However, when the frequencies are higher, the DOS is not constant anymore. A plot of electron energies in the many atom chain is shown below. DOS_finite_electron_chain ( 300 ) The numerical results once again agree with the models we developed earlier. In the Sommerfeld free electron model, the DOS in 1D is proportional to \\frac{1}{\\sqrt{E}} . The above histogram also reflects this proportionality for small energies E . However, when E is higher, we observe a significant deviation from the \\frac{1}{\\sqrt{E}} behavior. In both cases, we find that our models agree with the numerical results whenever frequencies/energies are small. When the frequencies/energies are high, we find that there is a significant deviation from the Debye/Sommerfeld models. The nature of this deviation is the subject of the next lecture! Conclusions \u00b6 The DOS of phonons used in the Debye model is justified by modeling the atoms as particles on a chain connected by a spring in the small \\omega limit. The DOS of electrons in the Sommerfeld model is justified by modeling electrons as particles that can hop between atoms in the small E limit. Exercises \u00b6 Preliminary provocations \u00b6 What does the LCAO matrix in the lecture notes look like if we also consider a next nearest-neighbour hopping -\\tilde{t} ? What does the LCAO matrix look like if we consider six atoms instead of three? You may assume that each atom has a single orbital, onsite energy E_0 and the hopping between neighbouring atoms -t . How do you determine which part of the interatomic potential is attractive and which is repulsive? You may assume that the interatomic potential is only a function of the interatomic distance r . Exercise 1: Linear triatomic molecule \u00b6 Consider carbon dioxide (C0 _2 ) which is a linear triatomic molecule shown below ??? info \"source\" By Jasek FH. - Own work, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/3.0 \"Creative Commons Attribution-Share Alike 3.0\"), [Link](https://commons.wikimedia.org/w/index.php?curid=2875238) How many normal modes does this molecule have assuming motion in only 1D? How many normal modes does it have if the atoms can move in all three dimensions? For simplicity, we only consider 1D motion of the atoms. Write down Newton's equations of motion for the atoms, you may assume that the spring constant is the same for both bonds. Consider a symmetric mode, for which the displacements of the oxygen atoms are equal in magnitude and have an opposite direction. Find the eigenfrequency of this mode. Now consider the antisymmetric mode when both of the oxygen atoms move in phase and have the same displacement. Find the ratio between the displacements of the carbon and oxygen atoms. Make sure that the center of mass of the molecule is at rest. Compute the eigenfrequency of the antisymmetric mode. Hint Compare your answers with Wikipedia . From Diatomic solids Exercise 2: the Peierls transition \u00b6 In the previous lecture, we have derived the electronic band structure of an 1D, equally spaced atomic chain. Such chains, however, are in fact not stable and the equal spacing will be distorted. This is also known as the Peierls transition . The spacing of the distorted chain alternates between two different distances and this also causes the hopping energy to alternate between t_1 and t_2 . We further set the onsite energies of the atoms to \\epsilon . The situation is depicted in the figure below. Due to the alternating hopping energies, we must treat two consecutive atoms as two different orbitals ( |n,1\u27e9 and |n,2 \u27e9 in the figure) from the same unit cell. The corresponding LCAO of this chain is given by \\left|\\Psi \\right\\rangle = \\sum_n \\left(\\phi_n \\left| n,1 \\right\\rangle + \\psi_n \\left| n,2 \\right\\rangle\\right) As usual, we assume that all these atomic orbitals are orthogonal to each other. Indicate the length of the unit cell a in the figure. Using the Schr\u00f6dinger equation, write the equations of motion of the electrons. Hint To this end, find expressions for E \\left< n,1 \\vert \\Psi \\right> = \\left< n,1 \\right| H \\left|\\Psi \\right> and E \\left< n,2 \\vert \\Psi \\right> = \\left< n,2 \\right| H \\left|\\Psi \\right> . Using the trial solutions \\phi_n = \\phi_0 e^{ikna} and \\psi_n = \\psi_0 e^{ikna} , show that the Sch\u00f6dinger equation can be written in matrix form: \\begin{pmatrix} \\epsilon & t_1 + t_2 e^{-i k a} \\\\ t_1 + t_2 e^{i k a} & \\epsilon \\end{pmatrix} \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix} = E \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix}. Derive the dispersion relation of this Hamiltonian. Does it look like the figure of the band structure shown on the Wikipedia page ? Does it reduce to the 1D, equally spaced atomic chain if t_1 = t_2 ? Find an expression of the group velocity v(k) and effective mass m^*(k) of both bands. Derive an expression for the density of states g(E) of the entire band structure and make a plot of it. Does your result makes sense when considering the band structure? Tight binding \u00b6 Group velocity, effective mass, density of states \u00b6 (here we only discuss electrons; for phonons everything is the same except for replacing E = \\hbar \\omega ) Let us think what happens if we apply an external electric field to the crystal: x = np . linspace ( 0.001 , 3 , 200 ) fig , ax = pyplot . subplots ( 1 , 1 ) ax . plot ( x , 1.2 - 1 / np . abs ( np . sin ( np . pi * x )) ** ( 1 / 2 ) + . 2 * ( x - 1.5 )) ax . plot ( x , . 2 * ( x - 0.25 ), '--' ) ax . set_ylim ( -. 7 , . 5 ) ax . set_xlabel ( \"$x$\" ) ax . set_ylabel ( \"$U(x)$\" ) ax . set_xticks ([ -. 05 , 1 , 2 ]) ax . set_xticklabels ([ \"$0$\" , \"$a$\" , \"$2a$\" ]) draw_classic_axes ( ax ) The full Hamiltonian of the system is H = \\frac{p^2}{2m} + U_\\textrm{atomic}(x) + e \\mathcal{E} x, where U_\\textrm{atomic} is the potential created by the nuclei, and \\mathcal{E} the electric field. A typical electric field is much smaller than the interatomic potential, and therefore we can start by obtaining the dispersion relation E(k) without electric field (by applying the LCAO method), and then solve H = E(k) + e\\mathcal{E}x. To derive how particles with an arbitrary dispersion relation move, we recall the Hamilton's equations for particle velocity v and force F : \\begin{aligned} v \\equiv \\frac{dr}{dt} &= \\frac{\\partial H(p, r)}{\\partial p}\\\\ F \\equiv \\frac{dp}{dt} &= -\\frac{\\partial H(p, r)}{\\partial r}. \\end{aligned} Substituting p = \\hbar k into the first equation we arrive to the expression for the electron group velocity v \\equiv \\hbar^{-1}\\partial E/\\partial k . From the second equation we obtain that the force acting on electron in a band stays -e\\mathcal{E} , which in turn gives results in the acceleration \\frac{dv}{dt} = \\frac{\u2202v}{\u2202p}\\frac{dp}{dt} = F/m. Comparing this expression with dv/dt = F/m , we arrive to the effective mass : m^* \\equiv \\left(\\frac{\u2202v}{\u2202p}\\right)^{-1} = \\left(\\frac{\u2202\u00b2E}{\u2202p\u00b2}\\right)^{-1} = \u0127\u00b2\\left(\\frac{\u2202\u00b2E}{\u2202k\u00b2}\\right)^{-1}. The group velocity describes how quickly electrons with a certain k -vector move, while the effective mass describes how hard they are to accelerate by applying external force. By using the dispersion relation we derived earlier, we obtain the effective mass like this: pyplot . figure ( figsize = ( 8 , 5 )) k = np . linspace ( - pi , pi , 300 ) meff = 1 / np . cos ( k ) color = list ( matplotlib . rcParams [ 'axes.prop_cycle' ])[ 0 ][ 'color' ] pyplot . plot ( k [ meff > 0 ], meff [ meff > 0 ], c = color ) pyplot . plot ( k [ meff < 0 ], meff [ meff < 0 ], c = color ) pyplot . ylim ( - 5 , 5 ) pyplot . xlabel ( '$ka$' ); pyplot . ylabel ( '$m^*$' ) pyplot . xticks ([ - pi , 0 , pi ], [ r '$-\\pi$' , 0 , r '$\\pi$' ]); Notice that the effective mass can be negative, which implies the electrons accelerate in the direction opposite to the applied force. Density of states \u00b6 The DOS is the number of states per unit energy. In 1D we have g(E) = \\frac{L}{2\\pi}\\sum |dk/dE| = \\frac{L}{2\\pi}\\sum |v|^{-1} The sum goes over all possible values of k and spin which have the same energy E . If we are working in two or more dimensions, we must integrate over the values of k with the same energy. Also take note that for energies below E_0 - 2t or above E_0 + 2t , there are no values of k with that energy, so there is nothing to sum over. Once again, starting from E = E_0 - 2t \\cos(ka), we get ka = \\pm\\arccos[(E - E_0) / 2t], and |v| ^{-1} = \\left|\\frac{dk}{dE} \\right| = \\frac{1}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. You can get to this result immediately if you remember the derivative of arccosine. Otherwise you need to go the long way: compute dE/dk as a function of k , express k through E as we did above, and take the inverse. We now add together the contributions of the positive and the negative momenta as well both spin orientations, and arrive to the density of states g(E) = \\frac{L}{2\\pi}\\frac{4}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. A quick check: when the energy is close to the bottom of the band, E = E_0 - 2t + \\delta E , we get g(E) \\propto \\delta E^{-1/2} , as we expect in 1D. The process of calculating the DOS at a given energy E of a spin-independent Hamiltonian is done systematically with the following steps: At a given energy E , determine all of the values of k which correspond to that E using the dispersion relation. Compute \\rvert dk / dE \\rvert . Do this either by writing k as a (multi-valued) function of E and differentiating, or by computing (dE / dk)^{-1} . Sum or integrate dk / dE over the allowed values of k found in 1 and multiply by any degeneracies (spin/polarization). Multiply by spin degeneracy. If the Hamiltonian depends on spin, then there is no spin degeneracy and the spin number s must be treated in the same way as k .","title":"Placeholder"},{"location":"Lost/#placeholder","text":"Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon!","title":"Placeholder"},{"location":"Lost/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"Lost/#thermal-expansion","text":"While the quadratic approximation of the interatomic potential is certainly the most important, the anharmonic term \\kappa_3(r-a)^3/6 also has physical significance. Let us examine its role visually by comparing the harmonic and the third order approximations in the plot below. Notice that the second-order approximation is symmetric around the minimum while the third-order term is not. r = np . linspace ( - 2 , 2.5 , 750 ) a = 0 b = 0 c = 1 d = - 0.2 U_quadratic = a + b * r + c * r ** 2 U_cubic = a + b * r + c * r ** 2 + d * r ** 3 r_min = - 2 r_max = 2.5 U_min = - 0.2 U_max = 4 E_t_min = 0.4 E_t_max = 3.5 N_values = 20 l_width = 1.5 N_active = 0 # Create figure fig = go . Figure () def U_c ( r ): return a + b * r + c * r ** 2 + d * r ** 3 def line ( E_t ): right = min ( np . roots ([ c , b , a - E_t ])) roots = np . roots ([ d , c , b , a - E_t ]) roots = np . real ( roots [ np . isreal ( roots )]) roots . sort () left = roots [ 1 ] return [ left , right ] def avg_pos_cubic ( E_t ): Z = integrate . simps ( np . exp ( - U_cubic / E_t ), r ) r_avg = integrate . simps ( r * np . exp ( - U_cubic / E_t ), r ) x = r_avg / Z return x # Add traces, one for each slider step for E_t in np . linspace ( E_t_min , E_t_max , N_values ): avg = avg_pos_cubic ( E_t ) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_quadratic , mode = 'lines' , line_color = 'blue' , name = \"Quadratic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = r , y = U_cubic , mode = 'lines' , line_color = 'red' , name = \"Cubic potential\" , )) fig . add_trace ( go . Scatter ( visible = False , x = line ( E_t ), y = [ E_t , E_t ], mode = 'lines' , line_color = 'black' , name = r 'Thermal energy level' )) fig . add_trace ( go . Scatter ( visible = False , x = [ 0 , 0 ], y = [ 0 , E_t ], mode = 'lines' , line_color = 'blue' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the quadratic potential' )) fig . add_trace ( go . Scatter ( visible = False , x = [ avg , avg ], y = [ U_c ( avg ), E_t ], mode = 'lines' , line_color = 'red' , line_dash = 'dot' , name = r '$\\langle r \\rangle$ for the cubic potential' )) # Initial starting image N_trace = int ( len ( fig . data ) / N_values ) # Number of traces added per step for j in range ( N_trace ): fig . data [ N_active * N_trace + j ] . visible = True # Creation of the aditional images steps = [] for i in range ( int ( len ( fig . data ) / N_trace )): step = dict ( method = \"restyle\" , args = [{ \"visible\" : [ False ] * len ( fig . data )}], value = str ( 0.1 * ( i + 1 )) ) for j in range ( N_trace ): step [ \"args\" ][ 0 ][ \"visible\" ][ N_trace * i + j ] = True # Toggle i'th trace to \"visible\" steps . append ( step ) # Creating the slider sliders = [ dict ( tickcolor = 'White' , font_color = 'White' , currentvalue_font_color = 'Black' , active = N_active , name = r 'Thermal Energy' , font_size = 16 , currentvalue = { \"prefix\" : r 'Thermal Energy k_B T: ' }, pad = { \"t\" : 50 }, steps = steps , )] # Updating the images for each step fig . update_layout ( sliders = sliders , showlegend = True , plot_bgcolor = 'rgb(254, 254, 254)' , width = 700 , height = 580 , xaxis = dict ( range = [ r_min , r_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), yaxis = dict ( range = [ U_min , U_max ], visible = True , showticklabels = True , showline = True , linewidth = l_width , linecolor = 'black' , gridcolor = 'white' , tickfont = dict ( size = 16 )), title = { 'text' : r 'Thermal expansion of cubic potential' , 'y' : 0.9 , 'x' : 0.45 , 'xanchor' : 'center' , 'yanchor' : 'top' }, xaxis_title = r '$r$' , yaxis_title = r '$U [k_b T]$' , ) # Edit slider labels and adding text next to the horizontal bar indicating T_E for i in range ( N_values ): fig [ 'layout' ][ 'sliders' ][ 0 ][ 'steps' ][ i ][ 'label' ] = ' %.1f ' % (( E_t_max - E_t_min ) * i / ( N_values - 1 ) + E_t_min ) # Showing the figure plt . plot ( fig ) fig . show () The asymmetry due to nonzero \\kappa_3 slows the growth of the potential when the interatomic distance increases. On the other hand, when the interatomic distance decreases, the asymmetry accelerates the growth of the potential. Therefore, stretching the material is more energetically favorable than contracting it. As a result, thermal excitations increase the interatomic distance. This gives us a simple model of thermal expansion .","title":"Thermal expansion"},{"location":"Lost/#van-der-waals-bond","text":"While we focus on the mechanisms of covalent bonding, let us also review another bond type. A Van der Waals bond originates from an attraction between the dipole moments of two atoms. Suppose we have two atoms separated by an interatomic distance r . If one atom has a dipole moment \\mathbf{p_1} , it creates an electric field \\mathbf{E} = \\frac{\\mathbf{p_1}}{4\\pi \\varepsilon_0 r^3} at the position of the other atom. The other atom then develops a dipole moment \\mathbf{p_2} = \\chi \\mathbf{E} with \\chi the polarizability of the atom. The potential energy between between the two dipoles is \\begin{align} U(r) &= \\frac{-|\\mathbf{p_1}||\\mathbf{p_2}|}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}| \\chi \\mathbf{E}}{4\\pi\\varepsilon_0 r^3}\\\\ &= \\frac{-|\\mathbf{p_1}|^2 \\chi}{(4\\pi\\varepsilon_0 r^3)^2}\\\\ &\\propto \\frac{1}{r^6}. \\end{align} The dipole attraction is much weaker than the covalent bonds but drops slower with increasing distance. How does the strength of a covalent bond scale with distance? The strength of the bond is determined by the interatomic hopping integral -t = \\langle 1 | H | 2 \\rangle . Since the wavefunction of a bound electron typically decays exponentially, so does the overlap integral. Although the Van der Waals force is weak, it is the only force when there are no chemically active electrons or when the atoms are too far apart to form covalent bonds. Therefore, there are materials where Van der Waals interactions are the dominant interactions. An example of such a material is graphite. The Van der Waals bonds in graphite hold layers of covalently bonded carbon atoms together: (image source: Wikipedia )","title":"Van der Waals bond"},{"location":"Lost/#looking-ahead-multiple-atoms","text":"So far we have only considered the interatomic interactions between diatomic systems. However, our aim is to understand electrons and phonons in solids containing N\\to\\infty atoms. Let us see what happens when we consider more than two atoms.","title":"Looking ahead: multiple atoms"},{"location":"Lost/#phonons","text":"In order to understand phonons better, we need to understand how a vibrational motion in a solid arises. To that end, we model an array of atoms that are connected by springs with a spring constant \\kappa . Our plan: Consider only a harmonic potential acting between the atoms ( \\kappa is constant) Write down equations of motion Compute normal modes For simplicity we consider 1D motion, and let us start with a chain of 3 atoms: # Defining constants y_max = 6 max_m = 3 # Off set for the new masses deviation_arr = [ + 2 , - 2 , - 3.5 ] def plot_spring ( x1 , x2 , y , annotation = r '$\\kappa$' ): L = ( x2 - x1 ) width , nturns = 1 , 10 N = 1000 pad = 200 # Make the spring, unit scale (0 to 1) x_spring = np . linspace ( 0 , L , N ) # distance along the spring y_spring = np . zeros ( N ) y_spring [ pad : - pad ] = width * np . sin ( 2 * np . pi * nturns * x_spring [ pad : - pad ] / L ) x_plot , y_plot = np . vstack (( x_spring , y_spring )) # And offset it x_plot += x1 y_plot += y ax . plot ( x_plot , y_plot , c = 'k' ) ax . annotate ( annotation , (( x2 + x1 ) / 2 , y + 2 ), fontsize = 40 ) def plot_mass ( x , y , annotation = r '$m$' ): mass = Circle (( x , y ), . 5 , fc = 'k' ) ax . add_patch ( mass ) ax . annotate ( annotation , ( x -. 5 , y + 2 ), fontsize = 30 ) def make_plot ( u , deviation_arr , max_masses = 3 ): # plotting initial masses plot_mass ( 0 , 0 ) plot_mass ( 0 + deviation_arr [ 0 ], u , annotation = '' ) # Plot initial dotted lines pyplot . plot (( 0 , 0 ), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( deviation_arr [ 0 ], deviation_arr [ 0 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((0, deviation_arr[0]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 0 , u - 2.5 , deviation_arr [ 0 ] -. 5 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotate the deviations annot_arr = [ r '$u_1$' , r '$u_2$' , r '$u_3$' ] off_set = - 1 ax . annotate ( annot_arr [ 0 ], ( deviation_arr [ 0 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Annotate large arrow containing the interatomic distance pyplot . arrow ( 0 , u / 3 , 9 , 0 , fc = \"k\" , ec = \"k\" , head_width =. 5 , head_length = 1 ) ax . annotate ( r '$a$' , ( 5 , u / 3 + off_set ), fontsize = 30 ) # Plotting other masses and springs for j in range ( max_masses - 1 ): # Equilibrium plot plot_spring ( 10 * j , 10 * ( j + 1 ), 0 ) plot_mass ( 10 * ( j + 1 ), 0 ) # Plot containing deviations from it's equilibrium plot_spring ( 10 * j + deviation_arr [ j ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) plot_mass ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], u , annotation = '' ) # Plot dotted lines pyplot . plot (( 10 * ( j + 1 ), 10 * ( j + 1 )), ( 0 , u - 2.5 ), 'k--' ) pyplot . plot (( 10 * ( j + 1 ) + deviation_arr [ j + 1 ], 10 * ( j + 1 ) + deviation_arr [ j + 1 ]), ( u , u - 2.5 ), 'k--' ) # pyplot.plot((10*(j+1), 10*(j+1)+deviation_arr[j+1]), (u-2.5, u-2.5), 'k--') pyplot . arrow ( 10 * ( j + 1 ), u - 2.5 , ( deviation_arr [ j + 1 ] +. 5 ), 0 , fc = \"k\" , ec = \"k\" , head_width =. 25 , head_length =. 5 ) # Annotations ax . annotate ( annot_arr [ j + 1 ], ( 10 * ( j + 1 ) + deviation_arr [ j + 1 ] / 2 + off_set +. 4 , u - 3 + off_set ), fontsize = 30 ) # Initializing figure fig , ax = pyplot . subplots ( figsize = ( 10 , 7 )) pyplot . axis ( 'off' ) ax . set_xlim (( - 1 , 10 * ( max_m - 1 ) + 1 )) ax . set_ylim (( - y_max - 4.5 , y_max - 2.5 )) # Plottig system make_plot ( - y_max , deviation_arr , max_m ); fig . show () Let us denote the deviation of atom i from its equilibrium position by u_i . Newton's equations of motion for this system are then given by \\begin{aligned} m \\ddot{u}_1 &= - \\kappa (u_1 - u_2) \\\\ m \\ddot{u}_2 &= - \\kappa (u_2 - u_1) - \\kappa (u_2 - u_3) \\\\ m \\ddot{u}_3 &= - \\kappa (u_3 - u_2). \\end{aligned}, We write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} We are interested in phonons, patterns of motion that are periodic and have a fixed frequency \\omega . Hence we guess that the motion of the atoms is \\mathbf{u}(t) = \\mathbf{u}_0 e^{i\\omega t}. We substitute our guess into the equations of motion to yield an eigenvalue problem: \\omega^2 \\mathbf{u}_0 = \\frac{\\kappa}{m} \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u}_0. The solutions to the eigenvalue problem are phonon modes.","title":"Phonons"},{"location":"Lost/#electrons","text":"We just looked at how a chain of atoms moves. Let us now look at how the electrons of those atoms behave. To that end, we consider a 3 atom chain without any motion. In order to understand how electrons behave, we use the LCAO model. The LCAO model generalizes in a very simple way. Let us consider the wavefunction: \\vert \\psi\\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. Because the three atoms are identical, the onsite energy is the same on all atoms \\langle 1|H|1 \\rangle = \\langle2|H|2 \\rangle = \\langle3|H|3 \\rangle = E_0 . Furthermore, we assume hopping only between the nearest neighbors and assume that it is real valued: \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t . We also assume that the orbitals are orthogonal to eachother. Just as we did in the previous lecture, we use the Schr\u00f6dinger equation H |\\psi\\rangle = E |\\psi\\rangle to set up a system of equations: \\begin{align} E \\varphi_1 &= E_0 \\varphi_1 - t \\varphi_2\\\\ E \\varphi_2 &= E_0 \\varphi_2 - t \\varphi_1 - t \\varphi_3\\\\ E \\varphi_3 &= E_0 \\varphi_3 -t \\varphi_2. \\end{align} Again, we write this in a matrix form: E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} E_0 & -t & 0 \\\\ -t & E_0 & -t \\\\ 0 & -t & E_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix}","title":"Electrons"},{"location":"Lost/#numerical-test","text":"Diagonalizing large matrices is unwieldy, but let's try and check it numerically to see if we notice a trend. Let us first model 3 atoms on a chain. The eigenfrequencies of the 3 atoms are: [0.0 1.0 1.732050] def DOS_finite_phonon_chain ( n ): rhs = 2 * np . eye ( n ) - np . eye ( n , k = 1 ) - np . eye ( n , k =- 1 ) rhs [ 0 , 0 ] -= 1 rhs [ - 1 , - 1 ] -= 1 pyplot . figure () pyplot . hist ( np . sqrt ( np . abs ( np . linalg . eigvalsh ( rhs ))), bins = 30 ) pyplot . xlabel ( \"$\\omega$\" ) pyplot . ylabel ( \"Number of eigenfrequencies\" ) DOS_finite_phonon_chain ( 3 ) The eigenenergies of the 3 orbitals are: [-1.41421356 0.0 1.41421356] def DOS_finite_electron_chain ( n ): rhs = 2 * np . eye ( n , k = 0 ) - np . eye ( n , k = 1 ) - np . eye ( n , k = - 1 ) pyplot . figure () pyplot . hist ( np . linalg . eigvalsh ( rhs ), bins = 30 ) pyplot . xlabel ( \"$E$\" ) pyplot . ylabel ( \"Number of eigenenergies\" ) DOS_finite_electron_chain ( 3 ) However, 3 atoms are far too few to model an actual solid. Hence, we need 'many more' atoms.","title":"Numerical test"},{"location":"Lost/#from-3-atoms-to-300","text":"Phonon modes of the many atom chain are shown below. DOS_finite_phonon_chain ( 300 ) We observe that when \\omega is small, we have a constant DOS. This is in line with what we saw in the Debye model. There the DOS of a 1D system was constant! However, when the frequencies are higher, the DOS is not constant anymore. A plot of electron energies in the many atom chain is shown below. DOS_finite_electron_chain ( 300 ) The numerical results once again agree with the models we developed earlier. In the Sommerfeld free electron model, the DOS in 1D is proportional to \\frac{1}{\\sqrt{E}} . The above histogram also reflects this proportionality for small energies E . However, when E is higher, we observe a significant deviation from the \\frac{1}{\\sqrt{E}} behavior. In both cases, we find that our models agree with the numerical results whenever frequencies/energies are small. When the frequencies/energies are high, we find that there is a significant deviation from the Debye/Sommerfeld models. The nature of this deviation is the subject of the next lecture!","title":"From 3 atoms to 300"},{"location":"Lost/#conclusions","text":"The DOS of phonons used in the Debye model is justified by modeling the atoms as particles on a chain connected by a spring in the small \\omega limit. The DOS of electrons in the Sommerfeld model is justified by modeling electrons as particles that can hop between atoms in the small E limit.","title":"Conclusions"},{"location":"Lost/#exercises","text":"","title":"Exercises"},{"location":"Lost/#preliminary-provocations","text":"What does the LCAO matrix in the lecture notes look like if we also consider a next nearest-neighbour hopping -\\tilde{t} ? What does the LCAO matrix look like if we consider six atoms instead of three? You may assume that each atom has a single orbital, onsite energy E_0 and the hopping between neighbouring atoms -t . How do you determine which part of the interatomic potential is attractive and which is repulsive? You may assume that the interatomic potential is only a function of the interatomic distance r .","title":"Preliminary provocations"},{"location":"Lost/#exercise-1-linear-triatomic-molecule","text":"Consider carbon dioxide (C0 _2 ) which is a linear triatomic molecule shown below ??? info \"source\" By Jasek FH. - Own work, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/3.0 \"Creative Commons Attribution-Share Alike 3.0\"), [Link](https://commons.wikimedia.org/w/index.php?curid=2875238) How many normal modes does this molecule have assuming motion in only 1D? How many normal modes does it have if the atoms can move in all three dimensions? For simplicity, we only consider 1D motion of the atoms. Write down Newton's equations of motion for the atoms, you may assume that the spring constant is the same for both bonds. Consider a symmetric mode, for which the displacements of the oxygen atoms are equal in magnitude and have an opposite direction. Find the eigenfrequency of this mode. Now consider the antisymmetric mode when both of the oxygen atoms move in phase and have the same displacement. Find the ratio between the displacements of the carbon and oxygen atoms. Make sure that the center of mass of the molecule is at rest. Compute the eigenfrequency of the antisymmetric mode. Hint Compare your answers with Wikipedia . From Diatomic solids","title":"Exercise 1: Linear triatomic molecule"},{"location":"Lost/#exercise-2-the-peierls-transition","text":"In the previous lecture, we have derived the electronic band structure of an 1D, equally spaced atomic chain. Such chains, however, are in fact not stable and the equal spacing will be distorted. This is also known as the Peierls transition . The spacing of the distorted chain alternates between two different distances and this also causes the hopping energy to alternate between t_1 and t_2 . We further set the onsite energies of the atoms to \\epsilon . The situation is depicted in the figure below. Due to the alternating hopping energies, we must treat two consecutive atoms as two different orbitals ( |n,1\u27e9 and |n,2 \u27e9 in the figure) from the same unit cell. The corresponding LCAO of this chain is given by \\left|\\Psi \\right\\rangle = \\sum_n \\left(\\phi_n \\left| n,1 \\right\\rangle + \\psi_n \\left| n,2 \\right\\rangle\\right) As usual, we assume that all these atomic orbitals are orthogonal to each other. Indicate the length of the unit cell a in the figure. Using the Schr\u00f6dinger equation, write the equations of motion of the electrons. Hint To this end, find expressions for E \\left< n,1 \\vert \\Psi \\right> = \\left< n,1 \\right| H \\left|\\Psi \\right> and E \\left< n,2 \\vert \\Psi \\right> = \\left< n,2 \\right| H \\left|\\Psi \\right> . Using the trial solutions \\phi_n = \\phi_0 e^{ikna} and \\psi_n = \\psi_0 e^{ikna} , show that the Sch\u00f6dinger equation can be written in matrix form: \\begin{pmatrix} \\epsilon & t_1 + t_2 e^{-i k a} \\\\ t_1 + t_2 e^{i k a} & \\epsilon \\end{pmatrix} \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix} = E \\begin{pmatrix} \\phi_0 \\\\ \\psi_0 \\end{pmatrix}. Derive the dispersion relation of this Hamiltonian. Does it look like the figure of the band structure shown on the Wikipedia page ? Does it reduce to the 1D, equally spaced atomic chain if t_1 = t_2 ? Find an expression of the group velocity v(k) and effective mass m^*(k) of both bands. Derive an expression for the density of states g(E) of the entire band structure and make a plot of it. Does your result makes sense when considering the band structure?","title":"Exercise 2: the Peierls transition"},{"location":"Lost/#tight-binding","text":"","title":"Tight binding"},{"location":"Lost/#group-velocity-effective-mass-density-of-states","text":"(here we only discuss electrons; for phonons everything is the same except for replacing E = \\hbar \\omega ) Let us think what happens if we apply an external electric field to the crystal: x = np . linspace ( 0.001 , 3 , 200 ) fig , ax = pyplot . subplots ( 1 , 1 ) ax . plot ( x , 1.2 - 1 / np . abs ( np . sin ( np . pi * x )) ** ( 1 / 2 ) + . 2 * ( x - 1.5 )) ax . plot ( x , . 2 * ( x - 0.25 ), '--' ) ax . set_ylim ( -. 7 , . 5 ) ax . set_xlabel ( \"$x$\" ) ax . set_ylabel ( \"$U(x)$\" ) ax . set_xticks ([ -. 05 , 1 , 2 ]) ax . set_xticklabels ([ \"$0$\" , \"$a$\" , \"$2a$\" ]) draw_classic_axes ( ax ) The full Hamiltonian of the system is H = \\frac{p^2}{2m} + U_\\textrm{atomic}(x) + e \\mathcal{E} x, where U_\\textrm{atomic} is the potential created by the nuclei, and \\mathcal{E} the electric field. A typical electric field is much smaller than the interatomic potential, and therefore we can start by obtaining the dispersion relation E(k) without electric field (by applying the LCAO method), and then solve H = E(k) + e\\mathcal{E}x. To derive how particles with an arbitrary dispersion relation move, we recall the Hamilton's equations for particle velocity v and force F : \\begin{aligned} v \\equiv \\frac{dr}{dt} &= \\frac{\\partial H(p, r)}{\\partial p}\\\\ F \\equiv \\frac{dp}{dt} &= -\\frac{\\partial H(p, r)}{\\partial r}. \\end{aligned} Substituting p = \\hbar k into the first equation we arrive to the expression for the electron group velocity v \\equiv \\hbar^{-1}\\partial E/\\partial k . From the second equation we obtain that the force acting on electron in a band stays -e\\mathcal{E} , which in turn gives results in the acceleration \\frac{dv}{dt} = \\frac{\u2202v}{\u2202p}\\frac{dp}{dt} = F/m. Comparing this expression with dv/dt = F/m , we arrive to the effective mass : m^* \\equiv \\left(\\frac{\u2202v}{\u2202p}\\right)^{-1} = \\left(\\frac{\u2202\u00b2E}{\u2202p\u00b2}\\right)^{-1} = \u0127\u00b2\\left(\\frac{\u2202\u00b2E}{\u2202k\u00b2}\\right)^{-1}. The group velocity describes how quickly electrons with a certain k -vector move, while the effective mass describes how hard they are to accelerate by applying external force. By using the dispersion relation we derived earlier, we obtain the effective mass like this: pyplot . figure ( figsize = ( 8 , 5 )) k = np . linspace ( - pi , pi , 300 ) meff = 1 / np . cos ( k ) color = list ( matplotlib . rcParams [ 'axes.prop_cycle' ])[ 0 ][ 'color' ] pyplot . plot ( k [ meff > 0 ], meff [ meff > 0 ], c = color ) pyplot . plot ( k [ meff < 0 ], meff [ meff < 0 ], c = color ) pyplot . ylim ( - 5 , 5 ) pyplot . xlabel ( '$ka$' ); pyplot . ylabel ( '$m^*$' ) pyplot . xticks ([ - pi , 0 , pi ], [ r '$-\\pi$' , 0 , r '$\\pi$' ]); Notice that the effective mass can be negative, which implies the electrons accelerate in the direction opposite to the applied force.","title":"Group velocity, effective mass, density of states"},{"location":"Lost/#density-of-states","text":"The DOS is the number of states per unit energy. In 1D we have g(E) = \\frac{L}{2\\pi}\\sum |dk/dE| = \\frac{L}{2\\pi}\\sum |v|^{-1} The sum goes over all possible values of k and spin which have the same energy E . If we are working in two or more dimensions, we must integrate over the values of k with the same energy. Also take note that for energies below E_0 - 2t or above E_0 + 2t , there are no values of k with that energy, so there is nothing to sum over. Once again, starting from E = E_0 - 2t \\cos(ka), we get ka = \\pm\\arccos[(E - E_0) / 2t], and |v| ^{-1} = \\left|\\frac{dk}{dE} \\right| = \\frac{1}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. You can get to this result immediately if you remember the derivative of arccosine. Otherwise you need to go the long way: compute dE/dk as a function of k , express k through E as we did above, and take the inverse. We now add together the contributions of the positive and the negative momenta as well both spin orientations, and arrive to the density of states g(E) = \\frac{L}{2\\pi}\\frac{4}{a}\\frac{1}{\\sqrt{4t^2 - (E - E_0)^2}}. A quick check: when the energy is close to the bottom of the band, E = E_0 - 2t + \\delta E , we get g(E) \\propto \\delta E^{-1/2} , as we expect in 1D. The process of calculating the DOS at a given energy E of a spin-independent Hamiltonian is done systematically with the following steps: At a given energy E , determine all of the values of k which correspond to that E using the dispersion relation. Compute \\rvert dk / dE \\rvert . Do this either by writing k as a (multi-valued) function of E and differentiating, or by computing (dE / dk)^{-1} . Sum or integrate dk / dE over the allowed values of k found in 1 and multiply by any degeneracies (spin/polarization). Multiply by spin degeneracy. If the Hamiltonian depends on spin, then there is no spin degeneracy and the spin number s must be treated in the same way as k .","title":"Density of states"},{"location":"additional/","text":"Additional resources \u00b6 This page is the space for additional resources which may prove to be of some use and/or interest for curious individuals. Websites Open Solid State Notes from Delft University of Technology : a site after my own heart, given the usage of the same reference text and the same static site generator . Moreover, the open-source nature of their project ( CC BY-SA 4.0 ) is the reason much of the content here exists. 2021 is the first year that this course will run, and the dutiful preparation of content of a high calibre has allowed for reproduction of content, with edits running the gamut from a light to touch to utter destruction. Britney Spears' Guide to Semiconductor Physics : a relic of the time. A viral website before the phrase existed, the site does not hold up to modern standards - especially in light of the recent details of the singer's probate conservatorship - but sports surprisingly high-quality quality content relating to semiconductors, especially semiconductor lasers. Texts Quantum Mechanics and Quantum and Atom Optics by Daniel A. Steck from the University of Oregon : marvellous books that have been diligently curated and provide excellent reading for both revision and learning new content. Note that the Quantum and Atom Optics text contains relevant content and includes some of the most interesting material one is likely to encounter in the realm of quantum mechanics 1 , but it pitched at the graduate level and assumed knowledge of material not taught in the UTAS undergraduate program. I still think it worthwhile as an additional resource, but don't fret if it becomes a bit hard to follow once it gets into the weeds. and I am not just saying this because my research has been in this area, it is objectively true! \u21a9","title":"Additional resources"},{"location":"additional/#additional-resources","text":"This page is the space for additional resources which may prove to be of some use and/or interest for curious individuals. Websites Open Solid State Notes from Delft University of Technology : a site after my own heart, given the usage of the same reference text and the same static site generator . Moreover, the open-source nature of their project ( CC BY-SA 4.0 ) is the reason much of the content here exists. 2021 is the first year that this course will run, and the dutiful preparation of content of a high calibre has allowed for reproduction of content, with edits running the gamut from a light to touch to utter destruction. Britney Spears' Guide to Semiconductor Physics : a relic of the time. A viral website before the phrase existed, the site does not hold up to modern standards - especially in light of the recent details of the singer's probate conservatorship - but sports surprisingly high-quality quality content relating to semiconductors, especially semiconductor lasers. Texts Quantum Mechanics and Quantum and Atom Optics by Daniel A. Steck from the University of Oregon : marvellous books that have been diligently curated and provide excellent reading for both revision and learning new content. Note that the Quantum and Atom Optics text contains relevant content and includes some of the most interesting material one is likely to encounter in the realm of quantum mechanics 1 , but it pitched at the graduate level and assumed knowledge of material not taught in the UTAS undergraduate program. I still think it worthwhile as an additional resource, but don't fret if it becomes a bit hard to follow once it gets into the weeds. and I am not just saying this because my research has been in this area, it is objectively true! \u21a9","title":"Additional resources"},{"location":"particulars/","text":"Course information \u00b6 Administration \u00b6 The solid-state component of the course will run for seven weeks, beginning in week 7 and concluding at the end of semester. In previous years, the solid-state physics and semiconductor physics components of the course have been explicitly differentiated; however, in this iteration, the two will be more closely intertwined, with semiconductors being considered a flourish to the foundations that we shall construct during our adventures in describing matter. Prerequisite knowledge The content covered in this course is complicated, and without the frim bedrock of requisite knowledge and associated competencies, attempts to construct additional structures may be compromised. It is critical that one is comfortable with the following: The principles and machinery of quantum mechanics. Explicitly, an understanding of how physical systems and their evolution are modelled using the Schr\u00f6dinger equation, along with a fluency in common examples (e.g. particle in a box, harmonic oscillator, the hydrogen atom), and a vague familiarity with Dirac notation is assumed. Thermodynamic quantities and concepts abound, with statistical mechanics looming large in the background. Conveniently, you have just completed a course in statistical mechanics, but it will be assumed that you are comfortable with the content It is my intention that you will be required to call upon many of the other tools from the toolbox that you have been developing during your studies, with the explicit aim of further honing these tools, and maybe adding a few to the kit. Delivery of content \u00b6 The course will operate in a flipped-mode configuration, whereby the undergirding principle is that your out-of-class time is used to consume prepared content (e.g. lectures ) and scheduled times are used for discussions in a problem-based learning framework. I prefer to refer to the lecture-style material as the content download , and interactive, active-learning sessions as the content unpacking component. Subject matter \u00b6 The content for this course draws heavily from off the excellent text The Oxford Solid State Basics by Steven H. Simon , and the book is a prescribed text for the course, that is, it is assumed that you will access to this book. This particular text was chosen because of its concise discussion of the content, its accessibility, and the wry whit which permeates the content, in concert with the availability of freely distributed pre-print of the book . I will be working from the printed text, and I encourage you to do the same. Unsupported material Steven himself has said that the preprint is roughly 85% of the book; however, if you elect to work from the preprint, you do so at your own risk. Course outline \u00b6 Course summary This subject is designed to serve as an introduction into the field of solid-state physics. Solid-state physics is the largest field of condensed matter physics, which itself is the largest branch of physics, and so there is only so material we will cover. The trajectory we shall take begins with bulk descriptors of solids, into considering the fundamental nature of solids, collective behaviour within solids, and the place of these systems in the real world. A rough outline of the course is as follows: An introduction to solid-state physics The structure of materials Solids in one dimension The geometry of solids Electrons in solids Magnetism with approximately one week devoted to each topic, but with the natural ebb and flow ultimately dictating the timeline. The notes \u00b6 The notes are designed to be consumed in concert to the video content, with certain aspects highlighted differently in the different media, and in extreme cases with different paths used to arrive at the same destination. One of my aims is to expose you to different ways of thinking about problems, not just have the same method and then just \"press play\". Expected competencies Content has been constructed with the assumption that the material is consumed sequentially, with sections often explicitly relying on results from previous work. I have also to placed expected competency markers at the beginning of each section, outlining knowledge that I expect you to have seen in other areas of study, and importantly, these are cumulative from section to section. Text reference You will also encounter text references at the beginning of each section, relating to the relevant content in the course text The Oxford Solid State Basics . Computational content Computational content, which is normally just the jupyter notebook associated with the section, also appears at the top of the page, but may also contain links to other software or pertinent computational material. The videos \u00b6 The video content is hosted on Echo360 and is available only to those enrolled at the University of Tasmania, and is best accessed through the course MyLO page . Content download : table of contents Timestamps Timestamps detailing the topics discussed for each video are available in the video descriptions, but can be accessed in the table below A brief summary of the topics discussed in the content download sessions is shown below: Video Topic (click for details and timestamps) w0v01 An introduction to solid-state physics w1v01 The Einstein model of solids w1v02 The Debye model of solids w1v03 The Drude model of metals w2v01 The Sommerfeld free-electron model w2v02 Chemistry 101 w3v01 The 1D harmonic chain w3v02 The diatomic chain w3v03 The tight-binding model Support \u00b6 You are not on this journey alone: there are many avenues available to you to help you on the journey (depending on your inclination to play antiquated video games, you may recognise this as a scene from The Legend of Zelda ). \" We are all in this together \" \u00b6 The course materials as consumed through the content download components are necessarily an individual effort, but in all other facets I strongly encourage collaboration. The structure of content unpacking sessions is deliberately geared towards discussion, the exchange of ideas, and collective problem solving moreso than the execution of a solution finding program. Computational resources \u00b6 As part of the course, it will be expected that you perform calculation and computations. You are welcome to do this in which ever language you prefer, but it is strongly recommended that you use Python , and indeed, this is the only language that will be supported. In order to ensure equitable and easy access to Python computing resources, a Jupyter Notebook server has been established, which allows for one to write and execute code via a web browser. The server is named Jove 1 , and access is through the JupyterHub portal . You will need to create an account to start using the server, but beyond this is should be click and go. Should you have a machine upon which you already have, or you wish to deploy, your own instance of Python , this is perfectly acceptable, but note that you will have to manually import the distributed materials into Jupyter. This can be effectively accomplished using the clone functionality of GitHub as this is where the course content is hosted. Bug catcher \u00b6 Finally, this is more of a request than anything else, but should you find any errors in the content - this site, the distributed notebooks, the content download sessions, whatever - please let me know so I can correct the content, which is a major boon for everyone involved. Thanks! For those wondering, Jove is an alternate name for the Roman god Jupiter. \u21a9","title":"Course particulars"},{"location":"particulars/#course-information","text":"","title":"Course information"},{"location":"particulars/#administration","text":"The solid-state component of the course will run for seven weeks, beginning in week 7 and concluding at the end of semester. In previous years, the solid-state physics and semiconductor physics components of the course have been explicitly differentiated; however, in this iteration, the two will be more closely intertwined, with semiconductors being considered a flourish to the foundations that we shall construct during our adventures in describing matter. Prerequisite knowledge The content covered in this course is complicated, and without the frim bedrock of requisite knowledge and associated competencies, attempts to construct additional structures may be compromised. It is critical that one is comfortable with the following: The principles and machinery of quantum mechanics. Explicitly, an understanding of how physical systems and their evolution are modelled using the Schr\u00f6dinger equation, along with a fluency in common examples (e.g. particle in a box, harmonic oscillator, the hydrogen atom), and a vague familiarity with Dirac notation is assumed. Thermodynamic quantities and concepts abound, with statistical mechanics looming large in the background. Conveniently, you have just completed a course in statistical mechanics, but it will be assumed that you are comfortable with the content It is my intention that you will be required to call upon many of the other tools from the toolbox that you have been developing during your studies, with the explicit aim of further honing these tools, and maybe adding a few to the kit.","title":"Administration"},{"location":"particulars/#delivery-of-content","text":"The course will operate in a flipped-mode configuration, whereby the undergirding principle is that your out-of-class time is used to consume prepared content (e.g. lectures ) and scheduled times are used for discussions in a problem-based learning framework. I prefer to refer to the lecture-style material as the content download , and interactive, active-learning sessions as the content unpacking component.","title":"Delivery of content"},{"location":"particulars/#subject-matter","text":"The content for this course draws heavily from off the excellent text The Oxford Solid State Basics by Steven H. Simon , and the book is a prescribed text for the course, that is, it is assumed that you will access to this book. This particular text was chosen because of its concise discussion of the content, its accessibility, and the wry whit which permeates the content, in concert with the availability of freely distributed pre-print of the book . I will be working from the printed text, and I encourage you to do the same. Unsupported material Steven himself has said that the preprint is roughly 85% of the book; however, if you elect to work from the preprint, you do so at your own risk.","title":"Subject matter"},{"location":"particulars/#course-outline","text":"Course summary This subject is designed to serve as an introduction into the field of solid-state physics. Solid-state physics is the largest field of condensed matter physics, which itself is the largest branch of physics, and so there is only so material we will cover. The trajectory we shall take begins with bulk descriptors of solids, into considering the fundamental nature of solids, collective behaviour within solids, and the place of these systems in the real world. A rough outline of the course is as follows: An introduction to solid-state physics The structure of materials Solids in one dimension The geometry of solids Electrons in solids Magnetism with approximately one week devoted to each topic, but with the natural ebb and flow ultimately dictating the timeline.","title":"Course outline"},{"location":"particulars/#the-notes","text":"The notes are designed to be consumed in concert to the video content, with certain aspects highlighted differently in the different media, and in extreme cases with different paths used to arrive at the same destination. One of my aims is to expose you to different ways of thinking about problems, not just have the same method and then just \"press play\". Expected competencies Content has been constructed with the assumption that the material is consumed sequentially, with sections often explicitly relying on results from previous work. I have also to placed expected competency markers at the beginning of each section, outlining knowledge that I expect you to have seen in other areas of study, and importantly, these are cumulative from section to section. Text reference You will also encounter text references at the beginning of each section, relating to the relevant content in the course text The Oxford Solid State Basics . Computational content Computational content, which is normally just the jupyter notebook associated with the section, also appears at the top of the page, but may also contain links to other software or pertinent computational material.","title":"The notes"},{"location":"particulars/#the-videos","text":"The video content is hosted on Echo360 and is available only to those enrolled at the University of Tasmania, and is best accessed through the course MyLO page . Content download : table of contents Timestamps Timestamps detailing the topics discussed for each video are available in the video descriptions, but can be accessed in the table below A brief summary of the topics discussed in the content download sessions is shown below: Video Topic (click for details and timestamps) w0v01 An introduction to solid-state physics w1v01 The Einstein model of solids w1v02 The Debye model of solids w1v03 The Drude model of metals w2v01 The Sommerfeld free-electron model w2v02 Chemistry 101 w3v01 The 1D harmonic chain w3v02 The diatomic chain w3v03 The tight-binding model","title":"The videos"},{"location":"particulars/#support","text":"You are not on this journey alone: there are many avenues available to you to help you on the journey (depending on your inclination to play antiquated video games, you may recognise this as a scene from The Legend of Zelda ).","title":"Support"},{"location":"particulars/#we-are-all-in-this-together","text":"The course materials as consumed through the content download components are necessarily an individual effort, but in all other facets I strongly encourage collaboration. The structure of content unpacking sessions is deliberately geared towards discussion, the exchange of ideas, and collective problem solving moreso than the execution of a solution finding program.","title":"\"We are all in this together\""},{"location":"particulars/#computational-resources","text":"As part of the course, it will be expected that you perform calculation and computations. You are welcome to do this in which ever language you prefer, but it is strongly recommended that you use Python , and indeed, this is the only language that will be supported. In order to ensure equitable and easy access to Python computing resources, a Jupyter Notebook server has been established, which allows for one to write and execute code via a web browser. The server is named Jove 1 , and access is through the JupyterHub portal . You will need to create an account to start using the server, but beyond this is should be click and go. Should you have a machine upon which you already have, or you wish to deploy, your own instance of Python , this is perfectly acceptable, but note that you will have to manually import the distributed materials into Jupyter. This can be effectively accomplished using the clone functionality of GitHub as this is where the course content is hosted.","title":"Computational resources"},{"location":"particulars/#bug-catcher","text":"Finally, this is more of a request than anything else, but should you find any errors in the content - this site, the distributed notebooks, the content download sessions, whatever - please let me know so I can correct the content, which is a major boon for everyone involved. Thanks! For those wondering, Jove is an alternate name for the Roman god Jupiter. \u21a9","title":"Bug catcher"},{"location":"placeholder/","text":"Placeholder \u00b6 Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon! Introduction \u00b6 Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Content Conclusions \u00b6 Exercises \u00b6 Preliminary provocations \u00b6","title":"Quantum simulators"},{"location":"placeholder/#placeholder","text":"Empty page This page is a placeholder: the treasure you seek is in another castle Unbaked The content here is still very much under development. Please come back soon!","title":"Placeholder"},{"location":"placeholder/#introduction","text":"Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Text reference The material covered here is discussed in section(s) \\S of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Content","title":"Introduction"},{"location":"placeholder/#conclusions","text":"","title":"Conclusions"},{"location":"placeholder/#exercises","text":"","title":"Exercises"},{"location":"placeholder/#preliminary-provocations","text":"","title":"Preliminary provocations"},{"location":"1-intoduction/1-1-specificheatI/","text":"The specific heat of solids I \u00b6 Introduction \u00b6 We embark on our journey by starting at the nexus of the known and unknown, namely around the turn of the nineteenth and twentieth centuries, where the development of \"modern\" physics was being applied to systems which had hitherto be poorly understood. Now this is not to say that nothing was known about the systems, on the contrary: empirical laws had been used to great effect to describe the observable world, but with the increasing sophistication of experimental technique and apparatus, the cracks in certain rules started to appear. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Thermal physics: heat capacity Quantum mechanics: energy spectrum of the harmonic oscillator Statistical physics: the partition function, equipartition theorem Text reference The material covered here is discussed in section(s) \\S 2.1 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The Dulong\u2013Petit law \u00b6 Consider the heat capacity of a solid. 1.1 Explain the concept of heat capacity in a manner understandable to someone without a science background. The heat capacity is the a measure of how much heat, or how much energy transfer, is required to change the temperature of a material. By measuring the heat capacity per weight, that is the mass-specific heat capacity , of a range of different elements, the two chemists Pierre Dulong and Alexis Petit observed the value was approximately constant when multiplied by the atomic weight of the element, stating that: c \\times M = constant where c is the specific heat capacity and M is the molar mass of the material. More commonly, one will see the law expressed in terms of the heat capacity C and number of moles n : C/n=\\frac{\\partial Q}{\\partial T} = 3R where R \\approx 8.314~\\mathrm{J K^{-1} mol^{-1}} is the ideal gas constant 1 . In the physics context, it is much more common do talk about the number of atoms N , which transforms the above equation into the Dulong-Peteit Law : C/N= 3 k_\\mathrm{B} But is this an accurate description? Let's have a look. Shown below is a plot of the heat capacity C 2 (in units of R ) as a function of atomic number: Heat capacity of the elements at room temperature as sourced from the CRC handbook of chemistry of physics which is pretty incredible. But a natural question arises: why is this the case? The Boltzmann model of a solid \u00b6 It was exactly the question of \"why does the Dulong-Petit law seem to work?\" that motivated Ludwig Boltzmann to use his novel - and at the time completely unaccepted - ideas, notably the existence of atoms and molecules and the mechanics that arises from statistically significant numbers of these atoms and molecules, to model unexplained systems. The insight of Boltzmann was to consider a solid as a collection of constituent particles, but unlike gasses, these particles would be strongly interacting. Explicitly, the idea of atoms interacting with their nearest neighbours through an elastic spring-like potential - an harmonic potential - would allow the system to be modelled with statistical mechanics. Like the case of a gas, energy can be stored in the system in the form of atomic motion, but unlike a gas, the motion of the atoms is constrained. A schematic of the atomic-scale model constructed by Boltzmann in an attempt to explain the Dulong-Petit law Whilst this may seem like a major leap forward, it is worth considering what the explaination for this behaviour had been prior to this proposal: nothing . Then, using the recently minted ideas such as the equipartition theorem , it was clear why the C/N= 3 k_\\mathrm{B} . To see this, recall that for a gas in thermal equilibrium, we have C_V/N = f/2 k_\\mathrm{B} where f is the number of thermodynamic degrees of freedom, or stated another way: each degree of freedom contributes k_\\mathrm{B}/2 to the heat capacity. Immediately, we can see that the Dulong\u2013Petit law is of this form, but suggests that the number of DoF is six, i.e. twice that of an ideal monatomic gas. 1.2 What is the difference between the heat capacities C_V and C_P ? What is the relationship between the two quantities, and what is the implication for the heat capacity of solids? As the heat capacity of an object is defined through \\frac{\\partial Q}{\\partial T} , one must consider the different thermodynamic processes (e.g. isochoric versus isobraic) as the heat supplied to the system will be different (e.g. dQ = dU versus dQ = dU + PdV ). It then follows that we define the heat capacity at constant volume C_V and the heat capacity at constant pressure C_P . Linking the two quantities is Mayer's relation, which states that for an ideal gas: C_P - C_V = nR and more generally C_P - C_V = \\frac{VT\\alpha^2}{\\beta} where \\alpha is the thermal expansion coefficient and \\beta is the isothermal compressibility. This should immediately point to the implication for solids: solids tend to be rather incompressible, which manifests in small values of \\alpha and \\beta , and especially small values of \\alpha^2 , and a negligible difference between C_P and C_V . Hence the usage of C ! The thermal expansion coefficients of various elements, noting that \\beta is of order 10^{-6} \\mathrm{m}^2 \\mathrm{N}^{-1} for squishy things (e.g. soft clay) down to 10^{-10} \\mathrm{m}^2 \\mathrm{N}^{-1} for things that a not squishy (e.g. rocks) Hopefully it not a mystery why there are additional degrees of freedom in a solid as compared to a monatomic gas: remember that Boltzmann's model of a solid is effectively a collection of harmonic oscillators, so not only could energy be stored in the motion of the motion of the constituent atoms, but also their position, storing energy in the \"bonds\" between atoms. Monatomic gas The degrees of freedom for a monatomic gas are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z The 4 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B}/2 . Diatomic gas The degrees of freedom for a diatomic gas (at room temperature) are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Rotation Axial and end-over-end The 5 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=5k_\\mathrm{B}/2 . Solid The degrees of freedom for a solid are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Position x , y , and z The 6 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B} . This result stood as one of the great achievements of statistical mechanics at the time, as up to (and indeed past this point) it was still considered a fringe theory. So, what happened? Diamond is the worst \u00b6 but also, diamond is remarkable Whilst most people will be familiar with the cool properties of diamond, some things are best seen. For example, diamond is often quoted as having the greatest thermal conductivity of any material, but what does that actually look like? Well, take a look : If one inspects the plot of heat capacities for the different elements, one can see that there are a few outliers, but it should be made clear that these measurements were taken at room temperature. Above room temperature, there is widespread agreement - even better than room temperature - but if one makes the same measurements at low temperatures, the Dulong-Petit law completely falls apart, with C \\rightarrow 0 as T \\rightarrow 0 . And even at room temperature, some materials do not behave as expected, and in particular: diamond. At room temperature, diamond has a value of C/R \\approx 0.74 , which is much less than 3! The remarkable properties of diamond had long been known, and consequently any theory its salt had to explain why diamond was special. Shown below is a plot of the heat capacity of diamond versus temperature: The heat capacity of diamond as a function of temperature. Data has been sourced from Einstein's original paper of $T>230~\\mathrm{K}$ and from J. E. Desnoyehs & J. A. Morrison for $T < 230~\\mathrm{K}$. Immediately one can notice: C is not a constant Things are good at high temperature Things are bad at low temperature and Boltzmann's theory does not do anything to explain any of this. The Einstein model of a solid \u00b6 It is perhaps unsurprising that a both difficult and well-known problem became the focus of attention for Einstein, someone who even at the very beginning of his career showed remarkable insight into physical systems, often reasoning from observations what must be going on, and constructing a theory to make it all work. Following his work on the photoelectric effect and Brownian motion, he was well placed to tackle the problem of the unexpected behaviour of heat capacity at low-temperatures. The model \u00b6 Like Boltzmann's model, Einstein's model was based around atoms in an harmonic potential, but they key - and highly consequential - difference being that each atom is an identical potential, and that oscillation in said potential occurs at a frequency \\omega , later dubbed the Einstein frequency . Basically, he took Boltzmann's model, injected quantum mechanics and asked: what will be the result. It is worth pausing to point out that this was done prior to quantum mechanics having been developed: Einstein's explanation of the photoelectric effect is widely heralded as the starting point of quantum, but it was not until roughly 20 years later that the Schr\u00f6dinger equation was published! So this was a pretty wild assertion. To see the implications, we can make use of our knowledge of statistical physics and the quantum harmonic oscillator: using the energy eigenstates of the system E_n , we can calculate the partition function Z , then the expectation value for the energy \\langle E \\rangle , and ultimately the heat cavity C . 1.3 Beginning with the energy eigenstates of a single one-dimensional harmonic oscillator, show that the heat capacity for a single oscillator is C = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} In one dimension, the energy eigenstates E_n of a single harmonic oscillator are given by: E_n = \\hbar\\omega(n+1/2) where \\omega is the frequency of the harmonic oscillator. The partition function is then given by: \\begin{aligned} Z = & \\sum_{n\\ge0} \\exp\\left[-\\beta\\hbar\\omega(n+1/2)\\right] \\\\ = & \\frac{\\exp(-\\beta\\hbar\\omega/2)}{1-\\exp(-\\beta\\hbar\\omega)} = \\frac{1}{2\\sinh(\\beta\\hbar\\omega/2)} \\end{aligned} We can then compute the expectation value of the energy \\langle E \\rangle via \\begin{aligned} \\langle E \\rangle = -\\frac{1}{Z}\\frac{\\partial Z}{\\partial \\beta} & = \\frac{\\hbar \\omega}{2}\\coth\\left(\\beta\\hbar\\omega/2\\right) \\\\ & = \\hbar \\omega \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + 1/2\\right) \\end{aligned} where n_\\mathrm{B} is the Bose occupation factor, defined as n_\\mathrm{B}(x) = \\frac{1}{\\exp(x)-1} It then straightforward to extract the heat capacity for a single oscillator through C = \\frac{\\partial \\langle E \\rangle}{\\partial T} = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} The above result is stated for a one-dimensional harmonic oscillator, but to expend the system three dimensions we need to multiply this result by three 3 which gives the final result C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} 1.4 Produce a plot the specific heat C versus temperature for realistic values of \\omega , providing your code. Code the produce the plot as requested in shown below, along with the output of said code. Note that # Import all the goodies required for running code in this unit from ssp import * # Define a function to calculate the heat capacity def c_einstein ( T , w ): \"\"\" Calculate the specific heat capacity according to the Einstein model of a solid Input: --- T: Temperature [K] w: Einstein frequency \\omega [rad.s^-1] Returns: --- The heat capacity in units of k_B \"\"\" x = ( hbar * w ) / ( T * kb ) # scale the variable return 3 * x ** 2 * np . exp ( x ) / ( np . exp ( x ) - 1 ) ** 2 # compute the heat capacity # The range of temperatures over which the heat capity will be calculated # Note: overflow errors will occur is the x_min value is too small temp = np . linspace ( 10 , 1000 , 200 ) # The range of Einstein freqeuncies to be computed. For reference, diamond has \\omega \\approx 170 w = np . linspace ( 20 , 200 , 5 ) # Create the plot instance fig , ax = plt . subplots () # Plot and label each frequency for f in w : ax . plot ( temp , c_einstein ( temp , f * 1e12 ), label = f '$\\omega= { f : .0f } $ THz' ) # Make the plot readable ax . set_xlabel ( '$T [K]$' ) ax . set_ylabel ( r '$C/k_B$' ); ax . set_title ( r 'The Einstein model of heat capacity' ) ax . legend () # Save the figure plt . savefig ( '01_Einstein_c.svg' , facecolor = 'white' , transparent = False ) plt . show () # Show the plot A plot of the specific heat as computed from the Einstein model. Note that diamond has a an Einstein frequency of approximately \\omega = 170~\\mathrm{Hz} Looking at the from of C , it is clear: In the high-temperature limit ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ) we recover the Dulong-Petit law In the low-temperature limit, the heat capacity is exponentially small But what is the physical interpretation of this behaviour? For higher temperatures ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ), the ability to store energy in harmonic motion is unencumbered, with a decrease in temperature, this ceases to be the case of these degrees of freedom are \"frozen out\". Once the temperature is sufficiently low ( k_{\\mathrm{B}} T/\\hbar\\omega < 1 ), atoms are necessarily in the ground states of the harmonic oscillator; only with sufficient energy ( E = \\hbar\\omega ) can an atom be excited, and with a temperature much less than the energy level spacing, atoms are stuck and thus cannot absorb any energy. It is incredible that Einstein reasoned that this process must be occurring, which prompted him to describe the theory, essentially leading to him inventing the quantisation of energy levels. Coming up diamonds \u00b6 Attempting to explain the heat capacity of diamond had proven the death knell of all theories up to this point, and so it is unsurprising that in Einstein's original paper on the topic cantered around measurements of the heat capacity of diamond, which is shown below: Plot A plot of the molar heat capacity of diamond as a function of temperature. The plot is somewhat diabolical in its omission of labels and units, which should read k_\\mathrm{B}T/\\hbar\\omega and C~[\\mathrm{cal}~\\mathrm{K}^{-1}~\\mathrm{mol}^{-1}] for the x and y axes respectively Data The raw data used to produce the figure of the heat capacity of diamond There are obviously a few discrepancies, but on the whole it looks much better than the Boltzmann model and rightly was seen as a major triumph. But again, the question is why does this happen physically? What is it about diamond that makes is act so strangely? An energy-level diagram for the quantum harmonic oscillator, showing the wavefunctions for the three lowest-energy eigenstates Well if we consider the energy spacing of the harmonic oscillator, \\hbar\\omega , it is related to both the mass ( m ) and the spring constant ( \\kappa ) of the oscillator. For most materials, the Einstein frequency is such that C/N \\approx 3 k_\\mathrm{B} , but diamond has an especially low value of \\omega = \\sqrt{\\kappa/m} , which perhaps is unsurprising given that carbon is light (low m ) and diamond is incredibly hard (large \\kappa ). Conclusions \u00b6 The law of Dulong\u2013Petit is an observation that all materials have C \\approx 3k_B per atom. The Einstein model describes each atom in a solid as an independent quantum harmonic oscillator with the same eigenfrequency \\omega_0 . At sufficiently low T , the thermal excitations freeze out, resulting in \\langle E \\rangle = \\hbar \\omega_0/2 . The Einstein model correctly predicts that the heat capacity drops to 0 as T\\rightarrow 0 . Exercises \u00b6 Preliminary provocations \u00b6 What is the high-temperature heat capacity of an atom in a solid with two momentum and two spatial coordinate degrees of freedom? Sketch the Bose Einstein distribution as a function of \\omega for two different values of T Exercise 1: Total heat capacity of a diatomic material \u00b6 One of the assumptions of the Einstein model states that every atom in a solid oscillates with the same frequency \\omega_0 . However, if the solid contains different types of atoms, it is unreasonable to assume that the atoms oscillate with the same frequency. One example of such a solid is a lithium crystal, which consists of the two stable isotopes ^6 Li (7.5%) and ^7 Li (92.5%) in their natural abundance. Let us extend the Einstein model to take into account the different masses of these different isotopes. Assume that the solid is 1D (1D quantum harmonic oscillator). Assume that the strength of the returning force k experienced by each atom is the same. What is the difference in the oscillation frequencies of the two different isotopes in the lithium crystal? Write down the total energy stored in the vibrations of each atom of the lithium crystal, assuming that all ^6 Li atoms are in n=2 vibrational mode and all ^7 Li atoms are in n=4 vibrational mode. In the case where the oscilators can occupy any vibrational mode, write down the total energy stored in the vibrations of each atom in the lithium crystal at a temperature T by modifying the Einstein model. Compute the heat capacity of the lithium crystal as a function of T . The exact value can be found on the NIST database \u21a9 Data is collated in the Heat Capacity of the Elements at 25 ^{\\circ} C as published in the CRC handbook of chemistry of physics , but was sourced from wikipedia \u21a9 Verify this explicitly if it is not obvious: noting the result Z_{3D} = Z_{1D}^3 is a key observation \u21a9","title":"1.1: The specific heat of solids I"},{"location":"1-intoduction/1-1-specificheatI/#the-specific-heat-of-solids-i","text":"","title":"The specific heat of solids I"},{"location":"1-intoduction/1-1-specificheatI/#introduction","text":"We embark on our journey by starting at the nexus of the known and unknown, namely around the turn of the nineteenth and twentieth centuries, where the development of \"modern\" physics was being applied to systems which had hitherto be poorly understood. Now this is not to say that nothing was known about the systems, on the contrary: empirical laws had been used to great effect to describe the observable world, but with the increasing sophistication of experimental technique and apparatus, the cracks in certain rules started to appear. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Thermal physics: heat capacity Quantum mechanics: energy spectrum of the harmonic oscillator Statistical physics: the partition function, equipartition theorem Text reference The material covered here is discussed in section(s) \\S 2.1 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-1-specificheatI/#the-dulongpetit-law","text":"Consider the heat capacity of a solid. 1.1 Explain the concept of heat capacity in a manner understandable to someone without a science background. The heat capacity is the a measure of how much heat, or how much energy transfer, is required to change the temperature of a material. By measuring the heat capacity per weight, that is the mass-specific heat capacity , of a range of different elements, the two chemists Pierre Dulong and Alexis Petit observed the value was approximately constant when multiplied by the atomic weight of the element, stating that: c \\times M = constant where c is the specific heat capacity and M is the molar mass of the material. More commonly, one will see the law expressed in terms of the heat capacity C and number of moles n : C/n=\\frac{\\partial Q}{\\partial T} = 3R where R \\approx 8.314~\\mathrm{J K^{-1} mol^{-1}} is the ideal gas constant 1 . In the physics context, it is much more common do talk about the number of atoms N , which transforms the above equation into the Dulong-Peteit Law : C/N= 3 k_\\mathrm{B} But is this an accurate description? Let's have a look. Shown below is a plot of the heat capacity C 2 (in units of R ) as a function of atomic number: Heat capacity of the elements at room temperature as sourced from the CRC handbook of chemistry of physics which is pretty incredible. But a natural question arises: why is this the case?","title":"The Dulong\u2013Petit law"},{"location":"1-intoduction/1-1-specificheatI/#the-boltzmann-model-of-a-solid","text":"It was exactly the question of \"why does the Dulong-Petit law seem to work?\" that motivated Ludwig Boltzmann to use his novel - and at the time completely unaccepted - ideas, notably the existence of atoms and molecules and the mechanics that arises from statistically significant numbers of these atoms and molecules, to model unexplained systems. The insight of Boltzmann was to consider a solid as a collection of constituent particles, but unlike gasses, these particles would be strongly interacting. Explicitly, the idea of atoms interacting with their nearest neighbours through an elastic spring-like potential - an harmonic potential - would allow the system to be modelled with statistical mechanics. Like the case of a gas, energy can be stored in the system in the form of atomic motion, but unlike a gas, the motion of the atoms is constrained. A schematic of the atomic-scale model constructed by Boltzmann in an attempt to explain the Dulong-Petit law Whilst this may seem like a major leap forward, it is worth considering what the explaination for this behaviour had been prior to this proposal: nothing . Then, using the recently minted ideas such as the equipartition theorem , it was clear why the C/N= 3 k_\\mathrm{B} . To see this, recall that for a gas in thermal equilibrium, we have C_V/N = f/2 k_\\mathrm{B} where f is the number of thermodynamic degrees of freedom, or stated another way: each degree of freedom contributes k_\\mathrm{B}/2 to the heat capacity. Immediately, we can see that the Dulong\u2013Petit law is of this form, but suggests that the number of DoF is six, i.e. twice that of an ideal monatomic gas. 1.2 What is the difference between the heat capacities C_V and C_P ? What is the relationship between the two quantities, and what is the implication for the heat capacity of solids? As the heat capacity of an object is defined through \\frac{\\partial Q}{\\partial T} , one must consider the different thermodynamic processes (e.g. isochoric versus isobraic) as the heat supplied to the system will be different (e.g. dQ = dU versus dQ = dU + PdV ). It then follows that we define the heat capacity at constant volume C_V and the heat capacity at constant pressure C_P . Linking the two quantities is Mayer's relation, which states that for an ideal gas: C_P - C_V = nR and more generally C_P - C_V = \\frac{VT\\alpha^2}{\\beta} where \\alpha is the thermal expansion coefficient and \\beta is the isothermal compressibility. This should immediately point to the implication for solids: solids tend to be rather incompressible, which manifests in small values of \\alpha and \\beta , and especially small values of \\alpha^2 , and a negligible difference between C_P and C_V . Hence the usage of C ! The thermal expansion coefficients of various elements, noting that \\beta is of order 10^{-6} \\mathrm{m}^2 \\mathrm{N}^{-1} for squishy things (e.g. soft clay) down to 10^{-10} \\mathrm{m}^2 \\mathrm{N}^{-1} for things that a not squishy (e.g. rocks) Hopefully it not a mystery why there are additional degrees of freedom in a solid as compared to a monatomic gas: remember that Boltzmann's model of a solid is effectively a collection of harmonic oscillators, so not only could energy be stored in the motion of the motion of the constituent atoms, but also their position, storing energy in the \"bonds\" between atoms. Monatomic gas The degrees of freedom for a monatomic gas are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z The 4 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B}/2 . Diatomic gas The degrees of freedom for a diatomic gas (at room temperature) are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Rotation Axial and end-over-end The 5 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=5k_\\mathrm{B}/2 . Solid The degrees of freedom for a solid are listed below: Property Degrees of freedom Momentum p_x , p_y , and p_z Position x , y , and z The 6 degrees of freedom each contribute k_\\mathrm{B}/2 to the specific heat such that C=3k_\\mathrm{B} . This result stood as one of the great achievements of statistical mechanics at the time, as up to (and indeed past this point) it was still considered a fringe theory. So, what happened?","title":"The Boltzmann model of a solid"},{"location":"1-intoduction/1-1-specificheatI/#diamond-is-the-worst","text":"but also, diamond is remarkable Whilst most people will be familiar with the cool properties of diamond, some things are best seen. For example, diamond is often quoted as having the greatest thermal conductivity of any material, but what does that actually look like? Well, take a look : If one inspects the plot of heat capacities for the different elements, one can see that there are a few outliers, but it should be made clear that these measurements were taken at room temperature. Above room temperature, there is widespread agreement - even better than room temperature - but if one makes the same measurements at low temperatures, the Dulong-Petit law completely falls apart, with C \\rightarrow 0 as T \\rightarrow 0 . And even at room temperature, some materials do not behave as expected, and in particular: diamond. At room temperature, diamond has a value of C/R \\approx 0.74 , which is much less than 3! The remarkable properties of diamond had long been known, and consequently any theory its salt had to explain why diamond was special. Shown below is a plot of the heat capacity of diamond versus temperature: The heat capacity of diamond as a function of temperature. Data has been sourced from Einstein's original paper of $T>230~\\mathrm{K}$ and from J. E. Desnoyehs & J. A. Morrison for $T < 230~\\mathrm{K}$. Immediately one can notice: C is not a constant Things are good at high temperature Things are bad at low temperature and Boltzmann's theory does not do anything to explain any of this.","title":"Diamond is the worst"},{"location":"1-intoduction/1-1-specificheatI/#the-einstein-model-of-a-solid","text":"It is perhaps unsurprising that a both difficult and well-known problem became the focus of attention for Einstein, someone who even at the very beginning of his career showed remarkable insight into physical systems, often reasoning from observations what must be going on, and constructing a theory to make it all work. Following his work on the photoelectric effect and Brownian motion, he was well placed to tackle the problem of the unexpected behaviour of heat capacity at low-temperatures.","title":"The Einstein model of a solid"},{"location":"1-intoduction/1-1-specificheatI/#the-model","text":"Like Boltzmann's model, Einstein's model was based around atoms in an harmonic potential, but they key - and highly consequential - difference being that each atom is an identical potential, and that oscillation in said potential occurs at a frequency \\omega , later dubbed the Einstein frequency . Basically, he took Boltzmann's model, injected quantum mechanics and asked: what will be the result. It is worth pausing to point out that this was done prior to quantum mechanics having been developed: Einstein's explanation of the photoelectric effect is widely heralded as the starting point of quantum, but it was not until roughly 20 years later that the Schr\u00f6dinger equation was published! So this was a pretty wild assertion. To see the implications, we can make use of our knowledge of statistical physics and the quantum harmonic oscillator: using the energy eigenstates of the system E_n , we can calculate the partition function Z , then the expectation value for the energy \\langle E \\rangle , and ultimately the heat cavity C . 1.3 Beginning with the energy eigenstates of a single one-dimensional harmonic oscillator, show that the heat capacity for a single oscillator is C = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} In one dimension, the energy eigenstates E_n of a single harmonic oscillator are given by: E_n = \\hbar\\omega(n+1/2) where \\omega is the frequency of the harmonic oscillator. The partition function is then given by: \\begin{aligned} Z = & \\sum_{n\\ge0} \\exp\\left[-\\beta\\hbar\\omega(n+1/2)\\right] \\\\ = & \\frac{\\exp(-\\beta\\hbar\\omega/2)}{1-\\exp(-\\beta\\hbar\\omega)} = \\frac{1}{2\\sinh(\\beta\\hbar\\omega/2)} \\end{aligned} We can then compute the expectation value of the energy \\langle E \\rangle via \\begin{aligned} \\langle E \\rangle = -\\frac{1}{Z}\\frac{\\partial Z}{\\partial \\beta} & = \\frac{\\hbar \\omega}{2}\\coth\\left(\\beta\\hbar\\omega/2\\right) \\\\ & = \\hbar \\omega \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + 1/2\\right) \\end{aligned} where n_\\mathrm{B} is the Bose occupation factor, defined as n_\\mathrm{B}(x) = \\frac{1}{\\exp(x)-1} It then straightforward to extract the heat capacity for a single oscillator through C = \\frac{\\partial \\langle E \\rangle}{\\partial T} = k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} The above result is stated for a one-dimensional harmonic oscillator, but to expend the system three dimensions we need to multiply this result by three 3 which gives the final result C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} 1.4 Produce a plot the specific heat C versus temperature for realistic values of \\omega , providing your code. Code the produce the plot as requested in shown below, along with the output of said code. Note that # Import all the goodies required for running code in this unit from ssp import * # Define a function to calculate the heat capacity def c_einstein ( T , w ): \"\"\" Calculate the specific heat capacity according to the Einstein model of a solid Input: --- T: Temperature [K] w: Einstein frequency \\omega [rad.s^-1] Returns: --- The heat capacity in units of k_B \"\"\" x = ( hbar * w ) / ( T * kb ) # scale the variable return 3 * x ** 2 * np . exp ( x ) / ( np . exp ( x ) - 1 ) ** 2 # compute the heat capacity # The range of temperatures over which the heat capity will be calculated # Note: overflow errors will occur is the x_min value is too small temp = np . linspace ( 10 , 1000 , 200 ) # The range of Einstein freqeuncies to be computed. For reference, diamond has \\omega \\approx 170 w = np . linspace ( 20 , 200 , 5 ) # Create the plot instance fig , ax = plt . subplots () # Plot and label each frequency for f in w : ax . plot ( temp , c_einstein ( temp , f * 1e12 ), label = f '$\\omega= { f : .0f } $ THz' ) # Make the plot readable ax . set_xlabel ( '$T [K]$' ) ax . set_ylabel ( r '$C/k_B$' ); ax . set_title ( r 'The Einstein model of heat capacity' ) ax . legend () # Save the figure plt . savefig ( '01_Einstein_c.svg' , facecolor = 'white' , transparent = False ) plt . show () # Show the plot A plot of the specific heat as computed from the Einstein model. Note that diamond has a an Einstein frequency of approximately \\omega = 170~\\mathrm{Hz} Looking at the from of C , it is clear: In the high-temperature limit ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ) we recover the Dulong-Petit law In the low-temperature limit, the heat capacity is exponentially small But what is the physical interpretation of this behaviour? For higher temperatures ( k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ), the ability to store energy in harmonic motion is unencumbered, with a decrease in temperature, this ceases to be the case of these degrees of freedom are \"frozen out\". Once the temperature is sufficiently low ( k_{\\mathrm{B}} T/\\hbar\\omega < 1 ), atoms are necessarily in the ground states of the harmonic oscillator; only with sufficient energy ( E = \\hbar\\omega ) can an atom be excited, and with a temperature much less than the energy level spacing, atoms are stuck and thus cannot absorb any energy. It is incredible that Einstein reasoned that this process must be occurring, which prompted him to describe the theory, essentially leading to him inventing the quantisation of energy levels.","title":"The model"},{"location":"1-intoduction/1-1-specificheatI/#coming-up-diamonds","text":"Attempting to explain the heat capacity of diamond had proven the death knell of all theories up to this point, and so it is unsurprising that in Einstein's original paper on the topic cantered around measurements of the heat capacity of diamond, which is shown below: Plot A plot of the molar heat capacity of diamond as a function of temperature. The plot is somewhat diabolical in its omission of labels and units, which should read k_\\mathrm{B}T/\\hbar\\omega and C~[\\mathrm{cal}~\\mathrm{K}^{-1}~\\mathrm{mol}^{-1}] for the x and y axes respectively Data The raw data used to produce the figure of the heat capacity of diamond There are obviously a few discrepancies, but on the whole it looks much better than the Boltzmann model and rightly was seen as a major triumph. But again, the question is why does this happen physically? What is it about diamond that makes is act so strangely? An energy-level diagram for the quantum harmonic oscillator, showing the wavefunctions for the three lowest-energy eigenstates Well if we consider the energy spacing of the harmonic oscillator, \\hbar\\omega , it is related to both the mass ( m ) and the spring constant ( \\kappa ) of the oscillator. For most materials, the Einstein frequency is such that C/N \\approx 3 k_\\mathrm{B} , but diamond has an especially low value of \\omega = \\sqrt{\\kappa/m} , which perhaps is unsurprising given that carbon is light (low m ) and diamond is incredibly hard (large \\kappa ).","title":"Coming up diamonds"},{"location":"1-intoduction/1-1-specificheatI/#conclusions","text":"The law of Dulong\u2013Petit is an observation that all materials have C \\approx 3k_B per atom. The Einstein model describes each atom in a solid as an independent quantum harmonic oscillator with the same eigenfrequency \\omega_0 . At sufficiently low T , the thermal excitations freeze out, resulting in \\langle E \\rangle = \\hbar \\omega_0/2 . The Einstein model correctly predicts that the heat capacity drops to 0 as T\\rightarrow 0 .","title":"Conclusions"},{"location":"1-intoduction/1-1-specificheatI/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-1-specificheatI/#preliminary-provocations","text":"What is the high-temperature heat capacity of an atom in a solid with two momentum and two spatial coordinate degrees of freedom? Sketch the Bose Einstein distribution as a function of \\omega for two different values of T","title":"Preliminary provocations"},{"location":"1-intoduction/1-1-specificheatI/#exercise-1-total-heat-capacity-of-a-diatomic-material","text":"One of the assumptions of the Einstein model states that every atom in a solid oscillates with the same frequency \\omega_0 . However, if the solid contains different types of atoms, it is unreasonable to assume that the atoms oscillate with the same frequency. One example of such a solid is a lithium crystal, which consists of the two stable isotopes ^6 Li (7.5%) and ^7 Li (92.5%) in their natural abundance. Let us extend the Einstein model to take into account the different masses of these different isotopes. Assume that the solid is 1D (1D quantum harmonic oscillator). Assume that the strength of the returning force k experienced by each atom is the same. What is the difference in the oscillation frequencies of the two different isotopes in the lithium crystal? Write down the total energy stored in the vibrations of each atom of the lithium crystal, assuming that all ^6 Li atoms are in n=2 vibrational mode and all ^7 Li atoms are in n=4 vibrational mode. In the case where the oscilators can occupy any vibrational mode, write down the total energy stored in the vibrations of each atom in the lithium crystal at a temperature T by modifying the Einstein model. Compute the heat capacity of the lithium crystal as a function of T . The exact value can be found on the NIST database \u21a9 Data is collated in the Heat Capacity of the Elements at 25 ^{\\circ} C as published in the CRC handbook of chemistry of physics , but was sourced from wikipedia \u21a9 Verify this explicitly if it is not obvious: noting the result Z_{3D} = Z_{1D}^3 is a key observation \u21a9","title":"Exercise 1: Total heat capacity of a diatomic material"},{"location":"1-intoduction/1-2-specificheatII/","text":"The specific heat of solids II \u00b6 Introduction \u00b6 Previously, we saw how an empirical observation describing the behaviour of the specific heat of solids motivated the development of atomic-scale models of solids in order to understand and predict their behaviour. This marked the beginning of using theories which involved the quantisation of certain observables to accurately predict previously unexplained behaviour, but as we shall see, one must continue down the rabbit hole of quantisation in order to better model physical systems. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Wave mechanics: acoustic waves in solids (sound) Mathematics: periodic boundary conditions, spherical coordinates Text reference The material covered here is discussed in section(s) \\S 2.2 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Shortcomings of the Einstein model \u00b6 The Einstein model did much better than the Boltzmann model at explaining the behaviour of solids outside the regime of k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ; however, it would turn out that the model routinely underpredicts the heat capacity as T \\rightarrow 0 . This can be seen in Einstein's plot of diamond , but also in other, better behaved materials. For example shown below is a plot of the heat capacity of silver (and diamond), along with a fit of the data using the Einstein model: The heat capacity of silver and diamond as a function of temperature, with the data fitted to the Einstein model with fitting parameter $\\omega$. Indeed, it was known that at low temperatures, the heat capacity displayed cubic behaviour, that is C \\propto T^3 . 2.1 Using the provided data for the heat capacity of silver, verify the cubic behaviour of the heat capacity at low temperatures. Ensure to include you code. # Import the data from the supplied .csv file data = pd . read_csv ( 'Heat_capacity_Ag.csv' ) data = data [ data [ 'T' ] < 25 ] # take only the low-termperature data # Define the function to fit (a cubic) def cubic ( x , a ): return a * x ** 3 # The range of temperatures over which the fit capity will be calculated temp = np . linspace ( 0 , 25 , 100 ) fit = curve_fit ( cubic , data [ 'T' ], data [ 'C' ]) # perform the fit a_ag = fit [ 0 ][ 0 ] # extract the fit parameter # Make the plot fig , ax = plt . subplots () ax . scatter ( data [ 'T' ], data [ 'C' ], label = 'Silver' , color = 'C1' ) ax . plot ( temp , cubic ( temp , a_ag ), label = f 'Cubic fit' , color = 'C1' ) ax . set_xlabel ( '$T$ [K]' ) ax . set_ylabel ( '$C/k_\\mathrm {B} $' ) ax . set_ylim (( 0 , . 3 )) ax . set_xlim (( 0 , 25 )) ax . set_title ( 'Heat capacity of silver at low temperature' ); ax . legend () plt . savefig ( '02_heat_capacity_cubic.svg' , facecolor = 'white' , transparent = False ) plt . show () The heat capacity of silver at low temperature, T<25~\\mathrm{K} , follows well a cubic relationship in temperature. 2.2 How does C predicted by the Einstein model behave at low T ? From the Einstein model, we have C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} which means that as T \\rightarrow 0 , \\beta \\rightarrow \\infty and thus C \\propto \\beta^2/\\exp(\\beta\\hbar\\omega) , which is exponentially small - and definitely not cubic! The Debye model \u00b6 In the years following the development of Einstein's model, with more people delving into the world of quantised oscillators, people were grappling with the links to other areas of physics. A key insight of Peter Debye was that oscillators in a crystal cannot be thought of as isolated identical systems, but rather as a coupled network of oscillators: recognising that oscillations in solids gives rise to sound waves, these waves should be quantised in the same way that Max Plank had done previously for light. Sound wave refresher A sound wave is a collective motion of atoms through a solid. The displacement \\mathbf{\\delta r} of an atom at position \\mathbf{r} and time t is described by \\mathbf{\\delta r} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)}, where \\mathbf{\\delta r}_0 is the amplitude of the wave and \\mathbf{k} = (k_x, k_y, k_z) the wave vector . The wavelength \\lambda is related to the wavevector \\mathbf{k} though \\lambda = 2\\pi/|\\mathbf{k}| . The wave depends on time only through the factor e^{-i\\omega t} . Therefore these waves are normal modes : oscillations of a system in which all parts of the system oscillate with the same frequency and fixed phase relation. In addition to direction of the wave k , each sound wave has another degree of freedom: the direction in which the atoms themselves move or the wave polarization . Per wavevector \\mathbf{k} there are three modes in a 3D solid: two transverse (perpendicular to \\mathbf{k} ) and one longitudinal mode (parallel to \\mathbf{k} ). The space containing all possible values of \\mathbf{k} is called the k -space (also named the reciprocal space ). Debye modelled the oscillation modes of a solid as waves with frequency \\omega , which through the dispersion relation is related to the wavevector k , explicitly \\omega(\\mathbf{k}) = v|\\mathbf{k}| where v is the speed of sound in the material. If we then construct a partition function and from this, compute the expectation value of the energy, we arrive at \\langle E \\rangle = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) which is very similar to the equivalent expression from Einstein's treatment, other than the sum over wavevectors. It is also interesting to note that the oscillation modes also obey Bose statistics - we shall discuss this more later. 2.3 From Where does the factor of 3 in the above expression originate? The factor 3 comes from the three possible normal modes of polarization for each wavevector \\mathbf{k} . To reiterate where we are and where we are going: instead of having 3N oscillators with the same frequency \\omega_0 , we now have 3N possible vibrational modes with frequencies depending on \\textbf{k} through the dispersion relation \\omega(\\mathbf{k}) = v_s|\\mathbf{k}| . But the question is now, how do we evaluate the above expression? And there are a few natural questions that arise from what we have done thus far: Don't normal modes depend on the material's shape. What impact does this have on the heat capacity? Which values of \\mathbf{k} are possible? and if all \\mathbf{k} are possible, won't E be infinite? Detour: periodic boundary conditions \u00b6 It is expected that you will have seen periodic boundary conditions in various contexts, mostly likely in studies of differential equations of electromagnetism, but during this course we shall use them regularly. Importantly, we must first establish why we would impose periodic boundary conditions on a system which is not periodic: solids have boundaries! An intuition can be cultivated by considering that C is a macroscopic property : it should not depend on the material's shape and should only be proportional to its volume. Therefore, we can consider making measurements of quantities of interest far from any boundary, and with this, we are free to choose the geometry of material and of course, we pick things that make our life easier! Consider a box of dimension L \\times L \\times L , which then has volume V = L^3 with periodic boundary conditions. These conditions enforce that the atomic displacement \\mathbf{\\delta r} is periodic inside the material. If we consider a translation by L in the x -direction \\mathbf{\\delta r}(\\mathbf{r} + L\\mathbf{\\hat{x}}) = \\mathbf{\\delta r}(\\mathbf{r}) and a wave in this sample \\delta \\mathbf{r_0} e^{i(\\mathbf{k}\\cdot\\mathbf{r}-\\omega t)} must satisfy the above equation, implying \\delta\\mathbf{r_0} e^{i(\\mathbf{k} \\cdot \\mathbf{r}+k_xL-\\omega t)} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)} and ultimately e^{i k_x L} = e^{i 0} = 1. This then restricts the possible values of k_x = n_x \\frac{2 \\pi}{L} , for n_x \\in \\mathbb{Z} . Given the same condition holds for the y - and z -direction, the allowed values for \\mathbf{k} are given by \\mathbf{k} = \\frac{2\\pi}{L}(n_x, n_y, n_z), \\quad \\{n_x, n_y, n_z\\} \\in \\mathbb{Z}. A key observation here is that the imposition of periodic boundary conditions results in a discretisation of k -space, where the allowed values of \\mathbf{k} form a regular grid in k -space, and moreover, per volume \\left(\\frac{2\\pi}{L}\\right)^3 in k -space there is exactly one allowed \\mathbf{k} . In the standard way, if we are required to sum over all possible values of \\mathbf{k} and the volume of L is sufficiently large - that is, the volume per allowed mode becomes smaller - we can replace the sum over \\mathbf{k} with an integral \\sum_\\mathbf{k} \\approx \\frac{L^3}{(2\\pi)^3}\\int \\textrm{d} \\textbf{k} Integral over k -space We shall use this result very regularly. The conversion from a sum over the discrete grid of k -space states to a volume integral provides an effective way to count all the possible waves. The density of states \u00b6 Armed with a shiny new tool (wrapping our sample into a hypertorus to make the maths nicer), we can return to evaluating the expectation value \\langle E \\rangle : \\begin{aligned} \\langle E \\rangle & = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\\\ & = 3 \\frac{L^3}{(2\\pi)^3}\\int \\mathrm{d} \\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\end{aligned} Note that in the above expression, the integrand depends only on \\omega(\\mathbf{k}) \\propto |\\mathbf{k}| , and it is therefore natural to move to spherical coordinates, where the 3-dimensional integral can be collapsed to one dimension since: \\int \\mathrm{d} \\mathbf{k} \\rightarrow 4\\pi\\int_0^\\infty k^2 \\mathrm{d} \\mathbf{k} Spherical coordinate transformation As a refresher, using the coordinate system defined by x = r \\sin(\\theta)\\cos(\\varphi) , y = r \\sin(\\theta)\\sin(\\varphi) , and z = r\\cos(\\theta) , the transformation of the integral can be performed via \\int f(\\mathbf{r}) \\textrm{d} \\textbf{r} \\to \\int\\limits_0^{2\\pi}\\int\\limits_0^{\\pi} \\int\\limits_0^\\infty f(r, \\theta, \\varphi) ~ r^2 \\sin(\\theta) \\textrm{d}r \\textrm{d}\\theta \\textrm{d}\\varphi Performing the change of variables, we obtain the expression for the total energy in spherical coordinates: \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) where we have introduced g(\\omega) , the density of states g(\\omega) = L^3\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 v_s^3} \\right] = n\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 n v_s^3} \\right] = N \\frac{9\\omega^2}{\\omega_d^3} and \\omega_d^3 is the Debye frequency \\omega_d^3 = 6\\pi^2 n v_s^3 The (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) term describes the average energy of an oscillation with frequency \\omega , and g(\\omega) describes the number of modes at the frequency \\omega The density of states Technically, the density of states total is related to number of oscillation modes between frequencies \\omega and \\omega + \\mathrm{d} \\omega via g(\\omega)\\mathrm{d} \\omega It can be convenient to express the density of states in the form g(\\omega) = 3 \\left(\\frac{L}{2\\pi}\\right)^3 \\frac{4\\pi \\omega^2}{v_s^3} which allows for us to see g(\\omega) in terms of its constituent components: 3 comes from the number of possible polarizations in 3D (two transversal, one longitudinal). (\\frac{L}{2\\pi})^3 is the density of \\textbf{k} points in k -space. 4\\pi is the area of a unit sphere. \\omega^2 is due to the area of a sphere in k -space being proportional to its squared radius k^2 and by having a linear dispersion relation \\omega = v_sk . v_s^{-3} is from the linear dispersion relation \\omega = v_sk . So in our case, due to the spherical symmetry, g(\\omega)\\textrm{d} \\omega can be obtained by calculating the density of states of a volume element dV = 4\\pi k^2 dk in k -space and substituting the dispersion relation \\omega(k) . In terms of the calculation of \\langle E \\rangle , we multiply the number of modes g(\\omega) by the average energy of a single mode at a given frequency \\omega and integrate over all frequencies. Low-temperature behaviour \u00b6 In order to calculate a value of C , we must compute C = \\partial \\langle E \\rangle/\\partial T , which means actually computing \\langle E \\rangle . From above we have \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) which we are going to separate into two components: \\begin{aligned} \\langle E \\rangle & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{2} \\\\ & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\textrm{ something independent of } T \\end{aligned} The independent component is the zero-point energy E_{ZP} of the vibrational modes, which despite diverging towards infinity, does not contribute to C . One can then make the substitution x =\\beta\\hbar\\omega to transform the above equation into something a little more palatable: \\langle E \\rangle = \\frac{9N\\hbar}{\\omega_d^3 (\\beta\\hbar)^4} \\int\\limits_0^{\\infty}\\mathrm{d} x ~ \\frac{x^3}{\\exp(x)-1} + E_{ZP} which evaluates to \\langle E \\rangle = 9N\\frac{\\left(k_\\mathrm{B} T\\right)^4}{(\\hbar \\omega_d)^3}\\frac{\\pi^4}{15} + E_{ZP} It should not be conspicuous that this result looks similar to Plank's result that E \\propto T^4 for photons. The above result also yields the result that C = \\frac{\\partial \\langle E\\rangle}{\\partial T} = N k_\\mathrm{B} \\frac{(k_\\mathrm{B} T)}{(\\hbar\\omega_d)^3}\\frac{12\\pi^4}{5} \\propto T^3 which produces the desired T^3 dependence, but critically, and unlike the result of Einstein, does not have any free parameters rather only requiring knowledge material density and the sound velocity. 2.4 What is the physical reason for the difference in the low-temperature behaviour of C for the Einstein and Debye models? In the Einstein model, once the temperature was below the energy associated with the oscillator, there was no capacity for the heat to absorb heat. In contrast, in the Debye model, there exist vibrational modes of the solid do have the capacity to absorb heat. Debye's interpolation (the duct-tape solution) \u00b6 Debye successfully constructed a model which quantised the vibrational modes of solids to accurately predict the low-temperature behaviour of the heat capacity. Unfortunately, the model produces a T^3 dependence for all T , not just low temperature and thus does not recover the Dulong-Petit law at high temperature. Debye understood the problem arose from allowing an infinite number of modes, with an implication that there are more modes of oscillation than atoms in the system. The \"quick fix\" for this - which will be revisited with rigour later in the course - to ensure that there are only as many oscillation modes as there are degrees of freedom is to introduce a cutoff frequency \\omega_{\\textrm{cutoff}} such that the total number of oscillation modes equates to the 3N , the number of normal modes in an 3-dimension material. This manifests in the following way: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which leads to an altered expression for \\langle E \\rangle : \\langle E \\rangle = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega)~\\hbar\\omega ~ n_{\\textrm{B}}(\\beta\\hbar\\omega) + E_{ZP} 2.5 Verify that the expected high-temperature behaviour for C is recovered when using Debye's interpolation For high temperature: n_{\\textrm{B}}(\\beta\\hbar\\omega) = \\frac{1}{\\exp(\\beta\\hbar\\omega)-1} \\to \\frac{k_{\\textrm{B}}T}{\\hbar\\omega} which when put into the expression above (ignoring the zero-point energy) \\langle E \\rangle = k_{\\textrm{B}}T \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which by design returns 3 k_{\\textrm{B}} T N , which is exactly the Dulong-Petit law. 2.6 Explicitly evaluate the cutoff frequency, and express the solution in terms of the Debye frequency An educated guess would probably land you in the correct spot: given the arbitrary definition of the Debye frequency, it is likely that \\omega_{\\textrm{cutoff}} = \\omega_d . Let's have a look: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) = 9N \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ \\frac{\\omega^2}{\\omega_d^3} = 3N \\frac{\\omega_{\\textrm{cutoff}}^3}{\\omega_d^3} That is, \\omega_{\\textrm{cutoff}} = \\omega_d . The question that one must ask: was it worth it? Well, let's look at the Debye model applied to the silver data as shown at the beginning of this section: The heat capacity of silver with fits to both the Einstein and Debye models of solids. Conclusions \u00b6 The Debye model assumes that atoms in materials move in a collective fashion, described by quantized normal modes with a dispersion relation \\omega = v_s|\\mathbf{k}| . The oscillation modes have a constant density of (L/2\\pi)^3 in the reciprocal / k -space. The total energy and heat capacity are obtained by integrating the contribution of the individual modes over k -space. The density of states g(\\omega) is the number of states per frequency. With a dispersion relation \u03c9 = v_s|\\mathbf{k}| , g(\\omega) is proportional to \\omega^2 for a 3D bosonic system. At low temperatures the heat capacity due to oscillations is proportional to T^3 . Modes of oscillation exist only up until the Debye frequency \\omega_D , after which there are no modes in the system. Exercises \u00b6 Preliminary provocations \u00b6 Why are there only 3 polarizations when there are 6 degrees of freedom in three-dimensions for an oscillator? Express the two-dimensional integral \\int\\mathrm{d}k_x\\mathrm{d}k_y in terms of polar coordinates. You can assume rotational symmetry. The Einstein model has a material-dependent frequency \\omega_0 = k_\\mathrm{B} T_E/\\hbar of the quantum harmonic oscillators as a free fitting parameter. What is the material-dependent parameter that plays a similar role in the Debye model? Derive an expression for the shortest possible wavelength in the Debye model it in terms of the interatomic distance a . Hint: assume that the number of atoms is given by N=V/a^3 . Discuss if the answer is reasonable. Exercise 1: Debye model - concepts \u00b6 Consider the probability to find an atom of a 1D solid that originally had a position x at a displacement \\delta x shown below: Describe which k -states are occupied. Explain your answer. Describe the concept of k -space. What momenta are allowed in a 2D system with dimensions L\\times L ? Explain the concept of density of states. Calculate the density of states g(\\omega) for oscillations of a 3D, 2D and 1D solid with linear dispersion \\omega=v_s|\\mathbf{k}| . Exercise 2: Debye model in 2D \u00b6 State the assumptions of the Debye model. Determine the energy of a two-dimensional solid as a function of T using the Debye approximation. You do not have to solve the integral. Calculate the heat capacity in the high T limit. At low T , show that C_V=KT^{n} . Find n . Express K as an indefinite integral (similarly to what done during the lecture). Exercise 3: Different oscillation modes \u00b6 (adapted from exercise 2.6a of The Oxford Solid State Basics ) During the lecture we derived the low-temperature heat capacity assuming that all the modes of oscillation have the same sound velocity v . In reality, the longitudinal and transverse modes have different sound velocities (see Wikipedia for an illustration of different sound wave types). Assume that there are two types of excitations: One longitudinal mode with \\omega = v_\\parallel |k| Two transverse modes with \\omega = v_\\bot |k| Write down the total energy of oscillations in this material. Verify that at high T you reproduce the Dulong-Petit law. Compute the behaviour of heat capacity at low T . Exercise 4: Anisotropic sound velocities \u00b6 (adapted from exercise 2.6b of The Oxford Solid State Basics ) Suppose now that the velocity is anisotropic ( v_x \\neq v_y \\neq v_z ) and \\omega = \\sqrt{v_x^2 k_x^2 + v_y^2 k_y^2 + v_z^2 k_z^2} . How does this change the Debye result for the heat capacity? Hint Write down the total energy as an integral over k , then change the integration variables so that the spherical symmetry of the integrand is restored.","title":"1.2: The specific heat of solids II"},{"location":"1-intoduction/1-2-specificheatII/#the-specific-heat-of-solids-ii","text":"","title":"The specific heat of solids II"},{"location":"1-intoduction/1-2-specificheatII/#introduction","text":"Previously, we saw how an empirical observation describing the behaviour of the specific heat of solids motivated the development of atomic-scale models of solids in order to understand and predict their behaviour. This marked the beginning of using theories which involved the quantisation of certain observables to accurately predict previously unexplained behaviour, but as we shall see, one must continue down the rabbit hole of quantisation in order to better model physical systems. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Wave mechanics: acoustic waves in solids (sound) Mathematics: periodic boundary conditions, spherical coordinates Text reference The material covered here is discussed in section(s) \\S 2.2 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-2-specificheatII/#shortcomings-of-the-einstein-model","text":"The Einstein model did much better than the Boltzmann model at explaining the behaviour of solids outside the regime of k_{\\mathrm{B}} T/\\hbar\\omega \\gg 1 ; however, it would turn out that the model routinely underpredicts the heat capacity as T \\rightarrow 0 . This can be seen in Einstein's plot of diamond , but also in other, better behaved materials. For example shown below is a plot of the heat capacity of silver (and diamond), along with a fit of the data using the Einstein model: The heat capacity of silver and diamond as a function of temperature, with the data fitted to the Einstein model with fitting parameter $\\omega$. Indeed, it was known that at low temperatures, the heat capacity displayed cubic behaviour, that is C \\propto T^3 . 2.1 Using the provided data for the heat capacity of silver, verify the cubic behaviour of the heat capacity at low temperatures. Ensure to include you code. # Import the data from the supplied .csv file data = pd . read_csv ( 'Heat_capacity_Ag.csv' ) data = data [ data [ 'T' ] < 25 ] # take only the low-termperature data # Define the function to fit (a cubic) def cubic ( x , a ): return a * x ** 3 # The range of temperatures over which the fit capity will be calculated temp = np . linspace ( 0 , 25 , 100 ) fit = curve_fit ( cubic , data [ 'T' ], data [ 'C' ]) # perform the fit a_ag = fit [ 0 ][ 0 ] # extract the fit parameter # Make the plot fig , ax = plt . subplots () ax . scatter ( data [ 'T' ], data [ 'C' ], label = 'Silver' , color = 'C1' ) ax . plot ( temp , cubic ( temp , a_ag ), label = f 'Cubic fit' , color = 'C1' ) ax . set_xlabel ( '$T$ [K]' ) ax . set_ylabel ( '$C/k_\\mathrm {B} $' ) ax . set_ylim (( 0 , . 3 )) ax . set_xlim (( 0 , 25 )) ax . set_title ( 'Heat capacity of silver at low temperature' ); ax . legend () plt . savefig ( '02_heat_capacity_cubic.svg' , facecolor = 'white' , transparent = False ) plt . show () The heat capacity of silver at low temperature, T<25~\\mathrm{K} , follows well a cubic relationship in temperature. 2.2 How does C predicted by the Einstein model behave at low T ? From the Einstein model, we have C = 3k_{\\mathrm{B}}(\\beta\\hbar\\omega)^2\\frac{\\exp(\\beta\\hbar\\omega)}{(\\exp(\\beta\\hbar\\omega)-1)^2} which means that as T \\rightarrow 0 , \\beta \\rightarrow \\infty and thus C \\propto \\beta^2/\\exp(\\beta\\hbar\\omega) , which is exponentially small - and definitely not cubic!","title":"Shortcomings of the Einstein model"},{"location":"1-intoduction/1-2-specificheatII/#the-debye-model","text":"In the years following the development of Einstein's model, with more people delving into the world of quantised oscillators, people were grappling with the links to other areas of physics. A key insight of Peter Debye was that oscillators in a crystal cannot be thought of as isolated identical systems, but rather as a coupled network of oscillators: recognising that oscillations in solids gives rise to sound waves, these waves should be quantised in the same way that Max Plank had done previously for light. Sound wave refresher A sound wave is a collective motion of atoms through a solid. The displacement \\mathbf{\\delta r} of an atom at position \\mathbf{r} and time t is described by \\mathbf{\\delta r} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)}, where \\mathbf{\\delta r}_0 is the amplitude of the wave and \\mathbf{k} = (k_x, k_y, k_z) the wave vector . The wavelength \\lambda is related to the wavevector \\mathbf{k} though \\lambda = 2\\pi/|\\mathbf{k}| . The wave depends on time only through the factor e^{-i\\omega t} . Therefore these waves are normal modes : oscillations of a system in which all parts of the system oscillate with the same frequency and fixed phase relation. In addition to direction of the wave k , each sound wave has another degree of freedom: the direction in which the atoms themselves move or the wave polarization . Per wavevector \\mathbf{k} there are three modes in a 3D solid: two transverse (perpendicular to \\mathbf{k} ) and one longitudinal mode (parallel to \\mathbf{k} ). The space containing all possible values of \\mathbf{k} is called the k -space (also named the reciprocal space ). Debye modelled the oscillation modes of a solid as waves with frequency \\omega , which through the dispersion relation is related to the wavevector k , explicitly \\omega(\\mathbf{k}) = v|\\mathbf{k}| where v is the speed of sound in the material. If we then construct a partition function and from this, compute the expectation value of the energy, we arrive at \\langle E \\rangle = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) which is very similar to the equivalent expression from Einstein's treatment, other than the sum over wavevectors. It is also interesting to note that the oscillation modes also obey Bose statistics - we shall discuss this more later. 2.3 From Where does the factor of 3 in the above expression originate? The factor 3 comes from the three possible normal modes of polarization for each wavevector \\mathbf{k} . To reiterate where we are and where we are going: instead of having 3N oscillators with the same frequency \\omega_0 , we now have 3N possible vibrational modes with frequencies depending on \\textbf{k} through the dispersion relation \\omega(\\mathbf{k}) = v_s|\\mathbf{k}| . But the question is now, how do we evaluate the above expression? And there are a few natural questions that arise from what we have done thus far: Don't normal modes depend on the material's shape. What impact does this have on the heat capacity? Which values of \\mathbf{k} are possible? and if all \\mathbf{k} are possible, won't E be infinite?","title":"The Debye model"},{"location":"1-intoduction/1-2-specificheatII/#detour-periodic-boundary-conditions","text":"It is expected that you will have seen periodic boundary conditions in various contexts, mostly likely in studies of differential equations of electromagnetism, but during this course we shall use them regularly. Importantly, we must first establish why we would impose periodic boundary conditions on a system which is not periodic: solids have boundaries! An intuition can be cultivated by considering that C is a macroscopic property : it should not depend on the material's shape and should only be proportional to its volume. Therefore, we can consider making measurements of quantities of interest far from any boundary, and with this, we are free to choose the geometry of material and of course, we pick things that make our life easier! Consider a box of dimension L \\times L \\times L , which then has volume V = L^3 with periodic boundary conditions. These conditions enforce that the atomic displacement \\mathbf{\\delta r} is periodic inside the material. If we consider a translation by L in the x -direction \\mathbf{\\delta r}(\\mathbf{r} + L\\mathbf{\\hat{x}}) = \\mathbf{\\delta r}(\\mathbf{r}) and a wave in this sample \\delta \\mathbf{r_0} e^{i(\\mathbf{k}\\cdot\\mathbf{r}-\\omega t)} must satisfy the above equation, implying \\delta\\mathbf{r_0} e^{i(\\mathbf{k} \\cdot \\mathbf{r}+k_xL-\\omega t)} = \\mathbf{\\delta r}_0 e^{i(\\mathbf{k} \\cdot \\mathbf{r}-\\omega t)} and ultimately e^{i k_x L} = e^{i 0} = 1. This then restricts the possible values of k_x = n_x \\frac{2 \\pi}{L} , for n_x \\in \\mathbb{Z} . Given the same condition holds for the y - and z -direction, the allowed values for \\mathbf{k} are given by \\mathbf{k} = \\frac{2\\pi}{L}(n_x, n_y, n_z), \\quad \\{n_x, n_y, n_z\\} \\in \\mathbb{Z}. A key observation here is that the imposition of periodic boundary conditions results in a discretisation of k -space, where the allowed values of \\mathbf{k} form a regular grid in k -space, and moreover, per volume \\left(\\frac{2\\pi}{L}\\right)^3 in k -space there is exactly one allowed \\mathbf{k} . In the standard way, if we are required to sum over all possible values of \\mathbf{k} and the volume of L is sufficiently large - that is, the volume per allowed mode becomes smaller - we can replace the sum over \\mathbf{k} with an integral \\sum_\\mathbf{k} \\approx \\frac{L^3}{(2\\pi)^3}\\int \\textrm{d} \\textbf{k} Integral over k -space We shall use this result very regularly. The conversion from a sum over the discrete grid of k -space states to a volume integral provides an effective way to count all the possible waves.","title":"Detour: periodic boundary conditions"},{"location":"1-intoduction/1-2-specificheatII/#the-density-of-states","text":"Armed with a shiny new tool (wrapping our sample into a hypertorus to make the maths nicer), we can return to evaluating the expectation value \\langle E \\rangle : \\begin{aligned} \\langle E \\rangle & = 3 \\sum_\\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\\\ & = 3 \\frac{L^3}{(2\\pi)^3}\\int \\mathrm{d} \\mathbf{k} \\hbar\\omega(\\mathbf{k}) \\left( n_\\mathrm{B}(\\beta\\hbar\\omega(\\mathbf{k}))+\\frac{1}{2} \\right) \\end{aligned} Note that in the above expression, the integrand depends only on \\omega(\\mathbf{k}) \\propto |\\mathbf{k}| , and it is therefore natural to move to spherical coordinates, where the 3-dimensional integral can be collapsed to one dimension since: \\int \\mathrm{d} \\mathbf{k} \\rightarrow 4\\pi\\int_0^\\infty k^2 \\mathrm{d} \\mathbf{k} Spherical coordinate transformation As a refresher, using the coordinate system defined by x = r \\sin(\\theta)\\cos(\\varphi) , y = r \\sin(\\theta)\\sin(\\varphi) , and z = r\\cos(\\theta) , the transformation of the integral can be performed via \\int f(\\mathbf{r}) \\textrm{d} \\textbf{r} \\to \\int\\limits_0^{2\\pi}\\int\\limits_0^{\\pi} \\int\\limits_0^\\infty f(r, \\theta, \\varphi) ~ r^2 \\sin(\\theta) \\textrm{d}r \\textrm{d}\\theta \\textrm{d}\\varphi Performing the change of variables, we obtain the expression for the total energy in spherical coordinates: \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) where we have introduced g(\\omega) , the density of states g(\\omega) = L^3\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 v_s^3} \\right] = n\\left[\\frac{12\\pi\\omega^2}{(2\\pi)^3 n v_s^3} \\right] = N \\frac{9\\omega^2}{\\omega_d^3} and \\omega_d^3 is the Debye frequency \\omega_d^3 = 6\\pi^2 n v_s^3 The (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) term describes the average energy of an oscillation with frequency \\omega , and g(\\omega) describes the number of modes at the frequency \\omega The density of states Technically, the density of states total is related to number of oscillation modes between frequencies \\omega and \\omega + \\mathrm{d} \\omega via g(\\omega)\\mathrm{d} \\omega It can be convenient to express the density of states in the form g(\\omega) = 3 \\left(\\frac{L}{2\\pi}\\right)^3 \\frac{4\\pi \\omega^2}{v_s^3} which allows for us to see g(\\omega) in terms of its constituent components: 3 comes from the number of possible polarizations in 3D (two transversal, one longitudinal). (\\frac{L}{2\\pi})^3 is the density of \\textbf{k} points in k -space. 4\\pi is the area of a unit sphere. \\omega^2 is due to the area of a sphere in k -space being proportional to its squared radius k^2 and by having a linear dispersion relation \\omega = v_sk . v_s^{-3} is from the linear dispersion relation \\omega = v_sk . So in our case, due to the spherical symmetry, g(\\omega)\\textrm{d} \\omega can be obtained by calculating the density of states of a volume element dV = 4\\pi k^2 dk in k -space and substituting the dispersion relation \\omega(k) . In terms of the calculation of \\langle E \\rangle , we multiply the number of modes g(\\omega) by the average energy of a single mode at a given frequency \\omega and integrate over all frequencies.","title":"The density of states"},{"location":"1-intoduction/1-2-specificheatII/#low-temperature-behaviour","text":"In order to calculate a value of C , we must compute C = \\partial \\langle E \\rangle/\\partial T , which means actually computing \\langle E \\rangle . From above we have \\langle E \\rangle = \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ g(\\omega) (\\hbar\\omega) \\left(n_\\mathrm{B}(\\beta\\hbar\\omega) + \\frac{1}{2} \\right) which we are going to separate into two components: \\begin{aligned} \\langle E \\rangle & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{2} \\\\ & = \\frac{9N\\hbar}{\\omega_d^3} \\int\\limits_0^{\\infty}\\textrm{d} \\omega ~ \\frac{\\omega^3}{\\exp(\\beta\\hbar\\omega)-1} + \\textrm{ something independent of } T \\end{aligned} The independent component is the zero-point energy E_{ZP} of the vibrational modes, which despite diverging towards infinity, does not contribute to C . One can then make the substitution x =\\beta\\hbar\\omega to transform the above equation into something a little more palatable: \\langle E \\rangle = \\frac{9N\\hbar}{\\omega_d^3 (\\beta\\hbar)^4} \\int\\limits_0^{\\infty}\\mathrm{d} x ~ \\frac{x^3}{\\exp(x)-1} + E_{ZP} which evaluates to \\langle E \\rangle = 9N\\frac{\\left(k_\\mathrm{B} T\\right)^4}{(\\hbar \\omega_d)^3}\\frac{\\pi^4}{15} + E_{ZP} It should not be conspicuous that this result looks similar to Plank's result that E \\propto T^4 for photons. The above result also yields the result that C = \\frac{\\partial \\langle E\\rangle}{\\partial T} = N k_\\mathrm{B} \\frac{(k_\\mathrm{B} T)}{(\\hbar\\omega_d)^3}\\frac{12\\pi^4}{5} \\propto T^3 which produces the desired T^3 dependence, but critically, and unlike the result of Einstein, does not have any free parameters rather only requiring knowledge material density and the sound velocity. 2.4 What is the physical reason for the difference in the low-temperature behaviour of C for the Einstein and Debye models? In the Einstein model, once the temperature was below the energy associated with the oscillator, there was no capacity for the heat to absorb heat. In contrast, in the Debye model, there exist vibrational modes of the solid do have the capacity to absorb heat.","title":"Low-temperature behaviour"},{"location":"1-intoduction/1-2-specificheatII/#debyes-interpolation-the-duct-tape-solution","text":"Debye successfully constructed a model which quantised the vibrational modes of solids to accurately predict the low-temperature behaviour of the heat capacity. Unfortunately, the model produces a T^3 dependence for all T , not just low temperature and thus does not recover the Dulong-Petit law at high temperature. Debye understood the problem arose from allowing an infinite number of modes, with an implication that there are more modes of oscillation than atoms in the system. The \"quick fix\" for this - which will be revisited with rigour later in the course - to ensure that there are only as many oscillation modes as there are degrees of freedom is to introduce a cutoff frequency \\omega_{\\textrm{cutoff}} such that the total number of oscillation modes equates to the 3N , the number of normal modes in an 3-dimension material. This manifests in the following way: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which leads to an altered expression for \\langle E \\rangle : \\langle E \\rangle = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega)~\\hbar\\omega ~ n_{\\textrm{B}}(\\beta\\hbar\\omega) + E_{ZP} 2.5 Verify that the expected high-temperature behaviour for C is recovered when using Debye's interpolation For high temperature: n_{\\textrm{B}}(\\beta\\hbar\\omega) = \\frac{1}{\\exp(\\beta\\hbar\\omega)-1} \\to \\frac{k_{\\textrm{B}}T}{\\hbar\\omega} which when put into the expression above (ignoring the zero-point energy) \\langle E \\rangle = k_{\\textrm{B}}T \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) which by design returns 3 k_{\\textrm{B}} T N , which is exactly the Dulong-Petit law. 2.6 Explicitly evaluate the cutoff frequency, and express the solution in terms of the Debye frequency An educated guess would probably land you in the correct spot: given the arbitrary definition of the Debye frequency, it is likely that \\omega_{\\textrm{cutoff}} = \\omega_d . Let's have a look: 3N = \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ g(\\omega) = 9N \\int_0^{\\omega_{\\textrm{cutoff}}} \\textrm{d} \\omega ~ \\frac{\\omega^2}{\\omega_d^3} = 3N \\frac{\\omega_{\\textrm{cutoff}}^3}{\\omega_d^3} That is, \\omega_{\\textrm{cutoff}} = \\omega_d . The question that one must ask: was it worth it? Well, let's look at the Debye model applied to the silver data as shown at the beginning of this section: The heat capacity of silver with fits to both the Einstein and Debye models of solids.","title":"Debye's interpolation (the duct-tape solution)"},{"location":"1-intoduction/1-2-specificheatII/#conclusions","text":"The Debye model assumes that atoms in materials move in a collective fashion, described by quantized normal modes with a dispersion relation \\omega = v_s|\\mathbf{k}| . The oscillation modes have a constant density of (L/2\\pi)^3 in the reciprocal / k -space. The total energy and heat capacity are obtained by integrating the contribution of the individual modes over k -space. The density of states g(\\omega) is the number of states per frequency. With a dispersion relation \u03c9 = v_s|\\mathbf{k}| , g(\\omega) is proportional to \\omega^2 for a 3D bosonic system. At low temperatures the heat capacity due to oscillations is proportional to T^3 . Modes of oscillation exist only up until the Debye frequency \\omega_D , after which there are no modes in the system.","title":"Conclusions"},{"location":"1-intoduction/1-2-specificheatII/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-2-specificheatII/#preliminary-provocations","text":"Why are there only 3 polarizations when there are 6 degrees of freedom in three-dimensions for an oscillator? Express the two-dimensional integral \\int\\mathrm{d}k_x\\mathrm{d}k_y in terms of polar coordinates. You can assume rotational symmetry. The Einstein model has a material-dependent frequency \\omega_0 = k_\\mathrm{B} T_E/\\hbar of the quantum harmonic oscillators as a free fitting parameter. What is the material-dependent parameter that plays a similar role in the Debye model? Derive an expression for the shortest possible wavelength in the Debye model it in terms of the interatomic distance a . Hint: assume that the number of atoms is given by N=V/a^3 . Discuss if the answer is reasonable.","title":"Preliminary provocations"},{"location":"1-intoduction/1-2-specificheatII/#exercise-1-debye-model-concepts","text":"Consider the probability to find an atom of a 1D solid that originally had a position x at a displacement \\delta x shown below: Describe which k -states are occupied. Explain your answer. Describe the concept of k -space. What momenta are allowed in a 2D system with dimensions L\\times L ? Explain the concept of density of states. Calculate the density of states g(\\omega) for oscillations of a 3D, 2D and 1D solid with linear dispersion \\omega=v_s|\\mathbf{k}| .","title":"Exercise 1: Debye model - concepts"},{"location":"1-intoduction/1-2-specificheatII/#exercise-2-debye-model-in-2d","text":"State the assumptions of the Debye model. Determine the energy of a two-dimensional solid as a function of T using the Debye approximation. You do not have to solve the integral. Calculate the heat capacity in the high T limit. At low T , show that C_V=KT^{n} . Find n . Express K as an indefinite integral (similarly to what done during the lecture).","title":"Exercise 2: Debye model in 2D"},{"location":"1-intoduction/1-2-specificheatII/#exercise-3-different-oscillation-modes","text":"(adapted from exercise 2.6a of The Oxford Solid State Basics ) During the lecture we derived the low-temperature heat capacity assuming that all the modes of oscillation have the same sound velocity v . In reality, the longitudinal and transverse modes have different sound velocities (see Wikipedia for an illustration of different sound wave types). Assume that there are two types of excitations: One longitudinal mode with \\omega = v_\\parallel |k| Two transverse modes with \\omega = v_\\bot |k| Write down the total energy of oscillations in this material. Verify that at high T you reproduce the Dulong-Petit law. Compute the behaviour of heat capacity at low T .","title":"Exercise 3: Different oscillation modes"},{"location":"1-intoduction/1-2-specificheatII/#exercise-4-anisotropic-sound-velocities","text":"(adapted from exercise 2.6b of The Oxford Solid State Basics ) Suppose now that the velocity is anisotropic ( v_x \\neq v_y \\neq v_z ) and \\omega = \\sqrt{v_x^2 k_x^2 + v_y^2 k_y^2 + v_z^2 k_z^2} . How does this change the Debye result for the heat capacity? Hint Write down the total energy as an integral over k , then change the integration variables so that the spherical symmetry of the integrand is restored.","title":"Exercise 4: Anisotropic sound velocities"},{"location":"1-intoduction/1-3-emetalsI/","text":"Electrons in metals I \u00b6 Introduction \u00b6 Metals are awesome, and this has long been known. People have been drawn to their shininess since the first chunks of the stuff were found, and it mustn't have been long before it was realised that they were different to other materials. To nail down exactly what is the difference would take some time (of order 10,000 years!) but it boils down to metals being conductive, and this comes from the ability of electrons in the material to move more freely. It turns out that one can apply a somewhat crude kinetic theory to understand the transport of electrons in metals, in the same way kinetic theory can be used to understand the transport of gasses. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Electromagnetism: The influence of fields of charged particles Mechanics: Construct and solve equations of motion Thermal physics: Kinetic theory of gases Text reference The material covered here is discussed in section(s) \\S 3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The kinetic theory of gases electrons \u00b6 Ohm's law is an empirical observation of conductors, which states that voltage is proportional to current V=IR . Since we are interested in the properties of materials, we would like to express this in a relation that does not depend on the conductor geometry. We achieve this by expressing the terms in Ohm's law with their microscopic equivalents. Consider a conducting wire with cross-sectional area A and length l . Such a wire has resistance R = \\rho l / A where \\rho is the material-dependent resistivity. An applied voltage V in the wire creates an electric field E = V/l . The resulting current I in the wire is described by the current density j \\equiv I / A . Returning these relations to Ohm's law, we get: V = I \u03c1 \\frac{l}{A} \u21d2 E = \u03c1 j, which relates the local quantities E and j . Our mission is to understand how this relation arises by considering motion of individual electrons in metals. This path was first trodden by Drude, who applied Boltzmann's kinetic theory of gases to a \"gas\" of electrons. The assumptions of Boltzmann's kinetic theory are that: Electrons scatter randomly at uncorrelated times. The average time between scattering is \\tau . Therefore, the probability of scattering in a time interval dt is dt / \\tau After each scattering event, the electron's momentum randomizes with a zero average \u27e8\\mathbf{p}\u27e9=0 It is also important to bake in the difference between neutral atoms or molecules in a gas and electrons, namely: Electrons are charged particles with charge -e , and consequently they respond to electric and magnetic fields through the Lorentz force ( \\mathbf{F}=-e\\left(\\mathbf{E}+\\mathbf{v}\u00d7\\mathbf{B}\\right) ) Even under these simplistic assumptions, the trajectory of the electrons is hard to calculate. As you will have seen elsewhere, with random scattering events, each trajectory is very different. A simple animation below shows several example trajectories, which are markedly different: Equations of motion \u00b6 Keep in your mind that our goal is to find electric current density j . Each electron with charge -e and velocity \\mathbf{v} carries current -e\\mathbf{v} . Therefore, if the electron density is n , the average current they carry is -ne\u27e8\\mathbf{v}\u27e9 . Thus, our goal shifts to compute the average velocity. Underpinning this calculation is idea that although it is difficult to calculate the motion of an individual electron, computing the average motion of the electrons is a much more tractable task. For convenience from now on, we will omit the average brackets, and write \\mathbf{v} instead of \u27e8\\mathbf{v}\u27e9 . This also applies to F . We derive an equation of motion for the \"average\" electron in the following way: Consider everything that happens in an (infinitesimal) time interval dt . A fraction dt/\u03c4 of the electrons scatters, and their average velocity becomes zero. m\\mathbf{v}(t + dt) = 0 The rest of the electrons (1 - dt/\u03c4) are accelerated by the Lorentz force F , so their velocity becomes m\\mathbf{v}(t + dt) = m\\mathbf{v}(t) + \\mathbf{F}\u22c5dt. To find the average velocity, we take a weighted average of these two groups of particles: \\begin{aligned} m\\mathbf{v}(t+dt) &= [m\\mathbf{v}(t) + F dt]\\left(1 - \\frac{dt}{\\tau}\\right) + 0\u22c5\\frac{dt}{\\tau} \\\\ &= [m\\mathbf{v}(t) + \\mathbf{F} dt] \\left(1 - \\frac{dt}{\\tau}\\right) \\\\ &= m\\mathbf{v}(t) + dt \\left[\\mathbf{F} - m\\frac{\\mathbf{v(t)}}{\\tau}\\right] - \\frac{\\mathbf{F}}{\\tau} dt^2 \\end{aligned} We now neglect the term proportional to dt\u00b2 (it vanishes when dt \u2192 0 ). Finally, we recognize that \\left[\\mathbf{v}(t+dt) - \\mathbf{v}(t)\\right]/dt = d\\mathbf{v}(t)/dt , which results in m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} + \\mathbf{F}. Observe that the first term on the right-hand side has the same form as a drag force: it always decelerates the electrons. This equation equation of motion of the average electron is our main result: now we only need to apply it. Consequences of the Drude model \u00b6 Let us first consider the case without magnetic fields, \\mathbf{B} = 0 , and with constant electric field \\mathbf{E} . For many properties of interest, we are interested in the steady-state behaviour of the system, i.e. d\\mathbf{v}/dt = 0 , which upon solving the equation of motion yields: \\mathbf{v}=\\frac{-e\u03c4}{m}\\mathbf{E}=-|\u03bc|\\mathbf{E}. In the above equation, we define the mobility \u03bc\\equiv |e|\u03c4/m to be the ratio between the electron drift velocity and the electric field. The mobility is an extremely important quantity: it describes the response of the electron to an electric field. We can then substitute the steady-state velocity into the definition of current density: \\mathbf{j}=-en\\mathbf{v}=\\frac{n e^2\u03c4}{m}\\mathbf{E}=\\sigma\\mathbf{E} where \\sigma is the conductivity \\sigma=\\frac{ne^2\u03c4}{m}=ne\\mu such that \u03c1=1/\\sigma . The Hall effect \u00b6 Let us now consider the case where both and electric field \\mathbf{E} and magnetic field \\mathbf{B} are non-zero. Imagine a conductor with a current I flowing perpendicular to an external magnetic field \\mathbf{B} as shown below: Looking at the equations of motion: m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} - e(\\mathbf{E} + \\mathbf{v}\\times\\mathbf{B}) we once again consider the steady state d\\mathbf{v}/dt = 0 . After substituting \\mathbf{v} = -\\mathbf{j}/ne , we arrive at \\mathbf{E}=\\frac{m}{ne^2\u03c4}\\mathbf{j} + \\frac{1}{ne}\\mathbf{j}\\times\\mathbf{B}. The first term is the same as before and describes the electric field parallel to the current, while the second is the electric field perpendicular to the current flow. In other words, if we send a current through a sample and apply a magnetic field, a voltage develops in the direction perpendicular to the current. This phenomenon is known as the Hall effect , with the perpendicular voltage called the Hall voltage , and the proportionality coefficient B/ne the Hall resistivity . Because of the Lorentz force, the electrons are deflected in a direction perpendicular to \\mathbf{B} and \\mathbf{j} . The deflection creates a charge imbalance, which in turn creates the electric field \\mathbf{E}_\\mathrm{H} compensating the Lorentz force. The above relation between the electric field and the current is linear, which allows us to write it in matrix form \\mathbf{E} = \\bar{\\rho} \\mathbf{j} where \\bar{\\rho} the resistivity matrix . Its diagonal elements are \\rho_{xx}=\\rho_{yy}=\\rho_{zz}=m/ne^2\\tau , which is the same as without magnetic field. The only nonzero off-diagonal elements when \\mathbf{B} points in the z -direction are \u03c1_{xy}=-\u03c1_{yx}=\\frac{B}{ne}\\equiv -R_\\mathrm{H}B, where R_H=-1/ne is the Hall coefficient . So by measuring the Hall voltage and knowing the electron charge, we can determine the density of free electrons in a material. While most materials have R_\\mathrm{H}<0 , interestingly some materials are found to have R_\\mathrm{H}>0 . 3.1 What would be the implications of a negative hall coefficient? With the density is negative, or that the charge carrier has a positive charge and thus not an electron. Thermal transport \u00b6 Given that Drude had cobbled together a kinetic theory for electrons, he decided that it was worthwhile to keep going, and make predictions about the properties of the \"gas\" in the same way as Boltzmann. Thermal conductivity \u00b6 Explicitly, Boltzmann derived the thermal conductivity \\kappa : \\kappa = \\frac{1}{3}n c_v \\langle v \\rangle \\lambda where c_v is the heat capacity per particle, \\langle v \\rangle is the average thermal velocity and \\lambda = \\langle v \\rangle \\tau is the scattering length. For a monatomic gas c_v = \\frac{3}{2} k_{\\mathrm{B}} and \\langle v \\rangle = \\sqrt{\\frac{8 k_{\\mathrm{B}} T}{\\pi m}} Without any grounds for justification, we can just give these a whirl and see how they go for electrons: \\kappa = \\frac{4}{\\pi}\\frac{n \\tau k_{\\mathrm{B}} T}{m} which still holds the phenomenological scattering rate \\tau , meaning makes independent measurement impossible. However, the conductivity \\sigma also contains \\tau , so by measuring their ratio we can eliminate the dependence and define the Lorenz number: L = \\frac{\\kappa}{T\\sigma} = \\frac{4}{\\pi}\\left(\\frac{k_{\\mathrm{B}}}{e}\\right) \\approx 1 \\times 10^{-8}~\\mathrm{W\\Omega~K^{-2}} which was close to the measured values of the Lorenz number (within a factor of 2 or so), and given there had never been any explanation at all for this behaviour, this result served was hailed as a major accomplishment. The Peltier effect \u00b6 A stack of peltier (thermoelectric cooling) devices can be used for highly-effective heat transfer Aside from being amazingly cool, we look at the Peltier effect to see that whilst the model well predicts the Lorenz number, this is actually somewhat of a fluke. The Peltier effect arises when a current flows through a material, as that current necessarily transports heat. The Peltier coefficient is defined by \\mathbf{j}^q = \\Pi \\mathbf{j} Where \\mathbf{j}^q is the heat current density and \\mathbf{j} is the electrical current density. In kinetic theory, the thermal current is \\mathbf{j}^q = \\frac{1}{3} (c_v T) n \\mathbf{v} where c_v T is the heat carried by one particle and c_v = 3 k_{\\mathrm{B}}/2 the heat capacity per particle, n the density of particles, and the factor of 1/3 from geometry. The electric current is \\mathbf{j} = -en\\mathbf{v} and thus \\Pi = \\frac{-c_v T}{3 e} = \\frac{-k_\\mathrm{B} T}{2 e} which yield the temperature independent ratio S = \\frac{\\Pi}{T} = \\frac{-k_{\\textrm{B}}}{2e} = -4.3 \\times 10^{-4} \\mathrm{V/K} where S is Seebeck coefficient (also the thermopower). And how well does this agree? Well, unlike \\kappa which was only a factor of 2 from the measure values, most values of S are on the order of 10^{-6} \\mathrm{V/K} . So unfortunately, whilst the Drude model gives us a \"broad brushstrokes\" picture of what is happening in a metal, it fails to accurately predict the specific behaviour and the quantities about which we care. Conclusions \u00b6 Drude theory is a kinetic theory and microscopic justification of the Ohm's law. We can calculate the resistivity from the characteristic scattering time \\tau . The Lorentz force leads to the Hall voltage that is perpendicular to the direction of both the electric current and the magnetic field. Drude theory does some things well, but ultimately falls short. Exercises \u00b6 Preliminary provocations \u00b6 How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way. Exercise 1: Extracting quantities from basic Hall measurements \u00b6 We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample? Exercise 2: Motion of an electron in a magnetic and an electric field. \u00b6 Consider an electron in free space experiencing a magnetic field \\mathbf{B} along the z -direction. Assume that the electron starts at the origin with a velocity v_0 along the x -direction. Write down the Newton's equation of motion for the electron, compute \\frac{d\\mathbf{v}}{{dt}} . What is the shape of the motion of the electron? Calculate the characteristic frequency and time-period T_c of this motion for B=1 Tesla. Now we accelerate the electron by adding an electric field \\mathbf{E} = E \\hat{x} . Adjust the differential equation for \\frac{d\\mathbf{v}}{{dt}} found in (1) to include \\mathbf{E} . Sketch the motion of the electron. Exercise 3: Temperature dependence of resistance in the Drude model \u00b6 We consider copper, which has a density of 8960 kg/m ^3 , an atomic weight of 63.55 g/mol, and a room-temperature resistivity of \u03c1=1.68\\cdot 10^{-8} \\Omega m. Each copper atom provides one free electron. Calculate the Drude scattering time \u03c4 at room temperature. Assuming that electrons move with the thermal velocity \\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}} , calculate the electron mean free path \\lambda , defined as the average distance an electron travels in between scattering events. The Drude model assumes that \\lambda is independent of temperature. How does the electrical resistivity \u03c1 depend on temperature under this assumption? Sketch \u03c1(T) . The empirical observation known as Matthiessen's Rule states that \u03c1(T) \\propto T . Discuss this result with reference to your answer above. Exercise 4: The Hall conductivity matrix and the Hall coefficient \u00b6 We apply a magnetic field \\bf B along the z -direction to a current carrying 2D sample in the xy plane. In this situation, the electric field \\mathbf{E} is related to the current density \\mathbf{j} by the resistivity matrix: \\mathbf{E} = \\begin{pmatrix} \u03c1_{xx} & \u03c1_{xy} \\\\ \u03c1_{yx} & \u03c1_{yy} \\end{pmatrix} \\mathbf{j} Sketch the expressions for \u03c1_{xx} and \u03c1_{xy} derived in the lecture notes as a function of the magnetic field \\bf{B} . Invert the resistivity matrix to obtain the conductivity matrix, \\begin{pmatrix} \\sigma_{xx} & \\sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_{yy} \\end{pmatrix} allowing you to express \\mathbf{j} as a function of \\mathbf{E} .","title":"1.3: Electrons in metals I"},{"location":"1-intoduction/1-3-emetalsI/#electrons-in-metals-i","text":"","title":"Electrons in metals I"},{"location":"1-intoduction/1-3-emetalsI/#introduction","text":"Metals are awesome, and this has long been known. People have been drawn to their shininess since the first chunks of the stuff were found, and it mustn't have been long before it was realised that they were different to other materials. To nail down exactly what is the difference would take some time (of order 10,000 years!) but it boils down to metals being conductive, and this comes from the ability of electrons in the material to move more freely. It turns out that one can apply a somewhat crude kinetic theory to understand the transport of electrons in metals, in the same way kinetic theory can be used to understand the transport of gasses. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Electromagnetism: The influence of fields of charged particles Mechanics: Construct and solve equations of motion Thermal physics: Kinetic theory of gases Text reference The material covered here is discussed in section(s) \\S 3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-3-emetalsI/#the-kinetic-theory-of-gases-electrons","text":"Ohm's law is an empirical observation of conductors, which states that voltage is proportional to current V=IR . Since we are interested in the properties of materials, we would like to express this in a relation that does not depend on the conductor geometry. We achieve this by expressing the terms in Ohm's law with their microscopic equivalents. Consider a conducting wire with cross-sectional area A and length l . Such a wire has resistance R = \\rho l / A where \\rho is the material-dependent resistivity. An applied voltage V in the wire creates an electric field E = V/l . The resulting current I in the wire is described by the current density j \\equiv I / A . Returning these relations to Ohm's law, we get: V = I \u03c1 \\frac{l}{A} \u21d2 E = \u03c1 j, which relates the local quantities E and j . Our mission is to understand how this relation arises by considering motion of individual electrons in metals. This path was first trodden by Drude, who applied Boltzmann's kinetic theory of gases to a \"gas\" of electrons. The assumptions of Boltzmann's kinetic theory are that: Electrons scatter randomly at uncorrelated times. The average time between scattering is \\tau . Therefore, the probability of scattering in a time interval dt is dt / \\tau After each scattering event, the electron's momentum randomizes with a zero average \u27e8\\mathbf{p}\u27e9=0 It is also important to bake in the difference between neutral atoms or molecules in a gas and electrons, namely: Electrons are charged particles with charge -e , and consequently they respond to electric and magnetic fields through the Lorentz force ( \\mathbf{F}=-e\\left(\\mathbf{E}+\\mathbf{v}\u00d7\\mathbf{B}\\right) ) Even under these simplistic assumptions, the trajectory of the electrons is hard to calculate. As you will have seen elsewhere, with random scattering events, each trajectory is very different. A simple animation below shows several example trajectories, which are markedly different:","title":"The kinetic theory of gases electrons"},{"location":"1-intoduction/1-3-emetalsI/#equations-of-motion","text":"Keep in your mind that our goal is to find electric current density j . Each electron with charge -e and velocity \\mathbf{v} carries current -e\\mathbf{v} . Therefore, if the electron density is n , the average current they carry is -ne\u27e8\\mathbf{v}\u27e9 . Thus, our goal shifts to compute the average velocity. Underpinning this calculation is idea that although it is difficult to calculate the motion of an individual electron, computing the average motion of the electrons is a much more tractable task. For convenience from now on, we will omit the average brackets, and write \\mathbf{v} instead of \u27e8\\mathbf{v}\u27e9 . This also applies to F . We derive an equation of motion for the \"average\" electron in the following way: Consider everything that happens in an (infinitesimal) time interval dt . A fraction dt/\u03c4 of the electrons scatters, and their average velocity becomes zero. m\\mathbf{v}(t + dt) = 0 The rest of the electrons (1 - dt/\u03c4) are accelerated by the Lorentz force F , so their velocity becomes m\\mathbf{v}(t + dt) = m\\mathbf{v}(t) + \\mathbf{F}\u22c5dt. To find the average velocity, we take a weighted average of these two groups of particles: \\begin{aligned} m\\mathbf{v}(t+dt) &= [m\\mathbf{v}(t) + F dt]\\left(1 - \\frac{dt}{\\tau}\\right) + 0\u22c5\\frac{dt}{\\tau} \\\\ &= [m\\mathbf{v}(t) + \\mathbf{F} dt] \\left(1 - \\frac{dt}{\\tau}\\right) \\\\ &= m\\mathbf{v}(t) + dt \\left[\\mathbf{F} - m\\frac{\\mathbf{v(t)}}{\\tau}\\right] - \\frac{\\mathbf{F}}{\\tau} dt^2 \\end{aligned} We now neglect the term proportional to dt\u00b2 (it vanishes when dt \u2192 0 ). Finally, we recognize that \\left[\\mathbf{v}(t+dt) - \\mathbf{v}(t)\\right]/dt = d\\mathbf{v}(t)/dt , which results in m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} + \\mathbf{F}. Observe that the first term on the right-hand side has the same form as a drag force: it always decelerates the electrons. This equation equation of motion of the average electron is our main result: now we only need to apply it.","title":"Equations of motion"},{"location":"1-intoduction/1-3-emetalsI/#consequences-of-the-drude-model","text":"Let us first consider the case without magnetic fields, \\mathbf{B} = 0 , and with constant electric field \\mathbf{E} . For many properties of interest, we are interested in the steady-state behaviour of the system, i.e. d\\mathbf{v}/dt = 0 , which upon solving the equation of motion yields: \\mathbf{v}=\\frac{-e\u03c4}{m}\\mathbf{E}=-|\u03bc|\\mathbf{E}. In the above equation, we define the mobility \u03bc\\equiv |e|\u03c4/m to be the ratio between the electron drift velocity and the electric field. The mobility is an extremely important quantity: it describes the response of the electron to an electric field. We can then substitute the steady-state velocity into the definition of current density: \\mathbf{j}=-en\\mathbf{v}=\\frac{n e^2\u03c4}{m}\\mathbf{E}=\\sigma\\mathbf{E} where \\sigma is the conductivity \\sigma=\\frac{ne^2\u03c4}{m}=ne\\mu such that \u03c1=1/\\sigma .","title":"Consequences of the Drude model"},{"location":"1-intoduction/1-3-emetalsI/#the-hall-effect","text":"Let us now consider the case where both and electric field \\mathbf{E} and magnetic field \\mathbf{B} are non-zero. Imagine a conductor with a current I flowing perpendicular to an external magnetic field \\mathbf{B} as shown below: Looking at the equations of motion: m\\frac{d\\mathbf{v}}{dt} = -m\\frac{\\mathbf{v}}{\u03c4} - e(\\mathbf{E} + \\mathbf{v}\\times\\mathbf{B}) we once again consider the steady state d\\mathbf{v}/dt = 0 . After substituting \\mathbf{v} = -\\mathbf{j}/ne , we arrive at \\mathbf{E}=\\frac{m}{ne^2\u03c4}\\mathbf{j} + \\frac{1}{ne}\\mathbf{j}\\times\\mathbf{B}. The first term is the same as before and describes the electric field parallel to the current, while the second is the electric field perpendicular to the current flow. In other words, if we send a current through a sample and apply a magnetic field, a voltage develops in the direction perpendicular to the current. This phenomenon is known as the Hall effect , with the perpendicular voltage called the Hall voltage , and the proportionality coefficient B/ne the Hall resistivity . Because of the Lorentz force, the electrons are deflected in a direction perpendicular to \\mathbf{B} and \\mathbf{j} . The deflection creates a charge imbalance, which in turn creates the electric field \\mathbf{E}_\\mathrm{H} compensating the Lorentz force. The above relation between the electric field and the current is linear, which allows us to write it in matrix form \\mathbf{E} = \\bar{\\rho} \\mathbf{j} where \\bar{\\rho} the resistivity matrix . Its diagonal elements are \\rho_{xx}=\\rho_{yy}=\\rho_{zz}=m/ne^2\\tau , which is the same as without magnetic field. The only nonzero off-diagonal elements when \\mathbf{B} points in the z -direction are \u03c1_{xy}=-\u03c1_{yx}=\\frac{B}{ne}\\equiv -R_\\mathrm{H}B, where R_H=-1/ne is the Hall coefficient . So by measuring the Hall voltage and knowing the electron charge, we can determine the density of free electrons in a material. While most materials have R_\\mathrm{H}<0 , interestingly some materials are found to have R_\\mathrm{H}>0 . 3.1 What would be the implications of a negative hall coefficient? With the density is negative, or that the charge carrier has a positive charge and thus not an electron.","title":"The Hall effect"},{"location":"1-intoduction/1-3-emetalsI/#thermal-transport","text":"Given that Drude had cobbled together a kinetic theory for electrons, he decided that it was worthwhile to keep going, and make predictions about the properties of the \"gas\" in the same way as Boltzmann.","title":"Thermal transport"},{"location":"1-intoduction/1-3-emetalsI/#thermal-conductivity","text":"Explicitly, Boltzmann derived the thermal conductivity \\kappa : \\kappa = \\frac{1}{3}n c_v \\langle v \\rangle \\lambda where c_v is the heat capacity per particle, \\langle v \\rangle is the average thermal velocity and \\lambda = \\langle v \\rangle \\tau is the scattering length. For a monatomic gas c_v = \\frac{3}{2} k_{\\mathrm{B}} and \\langle v \\rangle = \\sqrt{\\frac{8 k_{\\mathrm{B}} T}{\\pi m}} Without any grounds for justification, we can just give these a whirl and see how they go for electrons: \\kappa = \\frac{4}{\\pi}\\frac{n \\tau k_{\\mathrm{B}} T}{m} which still holds the phenomenological scattering rate \\tau , meaning makes independent measurement impossible. However, the conductivity \\sigma also contains \\tau , so by measuring their ratio we can eliminate the dependence and define the Lorenz number: L = \\frac{\\kappa}{T\\sigma} = \\frac{4}{\\pi}\\left(\\frac{k_{\\mathrm{B}}}{e}\\right) \\approx 1 \\times 10^{-8}~\\mathrm{W\\Omega~K^{-2}} which was close to the measured values of the Lorenz number (within a factor of 2 or so), and given there had never been any explanation at all for this behaviour, this result served was hailed as a major accomplishment.","title":"Thermal conductivity"},{"location":"1-intoduction/1-3-emetalsI/#the-peltier-effect","text":"A stack of peltier (thermoelectric cooling) devices can be used for highly-effective heat transfer Aside from being amazingly cool, we look at the Peltier effect to see that whilst the model well predicts the Lorenz number, this is actually somewhat of a fluke. The Peltier effect arises when a current flows through a material, as that current necessarily transports heat. The Peltier coefficient is defined by \\mathbf{j}^q = \\Pi \\mathbf{j} Where \\mathbf{j}^q is the heat current density and \\mathbf{j} is the electrical current density. In kinetic theory, the thermal current is \\mathbf{j}^q = \\frac{1}{3} (c_v T) n \\mathbf{v} where c_v T is the heat carried by one particle and c_v = 3 k_{\\mathrm{B}}/2 the heat capacity per particle, n the density of particles, and the factor of 1/3 from geometry. The electric current is \\mathbf{j} = -en\\mathbf{v} and thus \\Pi = \\frac{-c_v T}{3 e} = \\frac{-k_\\mathrm{B} T}{2 e} which yield the temperature independent ratio S = \\frac{\\Pi}{T} = \\frac{-k_{\\textrm{B}}}{2e} = -4.3 \\times 10^{-4} \\mathrm{V/K} where S is Seebeck coefficient (also the thermopower). And how well does this agree? Well, unlike \\kappa which was only a factor of 2 from the measure values, most values of S are on the order of 10^{-6} \\mathrm{V/K} . So unfortunately, whilst the Drude model gives us a \"broad brushstrokes\" picture of what is happening in a metal, it fails to accurately predict the specific behaviour and the quantities about which we care.","title":"The Peltier effect"},{"location":"1-intoduction/1-3-emetalsI/#conclusions","text":"Drude theory is a kinetic theory and microscopic justification of the Ohm's law. We can calculate the resistivity from the characteristic scattering time \\tau . The Lorentz force leads to the Hall voltage that is perpendicular to the direction of both the electric current and the magnetic field. Drude theory does some things well, but ultimately falls short.","title":"Conclusions"},{"location":"1-intoduction/1-3-emetalsI/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-3-emetalsI/#preliminary-provocations","text":"How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way.","title":"Preliminary provocations"},{"location":"1-intoduction/1-3-emetalsI/#exercise-1-extracting-quantities-from-basic-hall-measurements","text":"We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample?","title":"Exercise 1: Extracting quantities from basic Hall measurements"},{"location":"1-intoduction/1-3-emetalsI/#exercise-2-motion-of-an-electron-in-a-magnetic-and-an-electric-field","text":"Consider an electron in free space experiencing a magnetic field \\mathbf{B} along the z -direction. Assume that the electron starts at the origin with a velocity v_0 along the x -direction. Write down the Newton's equation of motion for the electron, compute \\frac{d\\mathbf{v}}{{dt}} . What is the shape of the motion of the electron? Calculate the characteristic frequency and time-period T_c of this motion for B=1 Tesla. Now we accelerate the electron by adding an electric field \\mathbf{E} = E \\hat{x} . Adjust the differential equation for \\frac{d\\mathbf{v}}{{dt}} found in (1) to include \\mathbf{E} . Sketch the motion of the electron.","title":"Exercise 2: Motion of an electron in a magnetic and an electric field."},{"location":"1-intoduction/1-3-emetalsI/#exercise-3-temperature-dependence-of-resistance-in-the-drude-model","text":"We consider copper, which has a density of 8960 kg/m ^3 , an atomic weight of 63.55 g/mol, and a room-temperature resistivity of \u03c1=1.68\\cdot 10^{-8} \\Omega m. Each copper atom provides one free electron. Calculate the Drude scattering time \u03c4 at room temperature. Assuming that electrons move with the thermal velocity \\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}} , calculate the electron mean free path \\lambda , defined as the average distance an electron travels in between scattering events. The Drude model assumes that \\lambda is independent of temperature. How does the electrical resistivity \u03c1 depend on temperature under this assumption? Sketch \u03c1(T) . The empirical observation known as Matthiessen's Rule states that \u03c1(T) \\propto T . Discuss this result with reference to your answer above.","title":"Exercise 3: Temperature dependence of resistance in the Drude model"},{"location":"1-intoduction/1-3-emetalsI/#exercise-4-the-hall-conductivity-matrix-and-the-hall-coefficient","text":"We apply a magnetic field \\bf B along the z -direction to a current carrying 2D sample in the xy plane. In this situation, the electric field \\mathbf{E} is related to the current density \\mathbf{j} by the resistivity matrix: \\mathbf{E} = \\begin{pmatrix} \u03c1_{xx} & \u03c1_{xy} \\\\ \u03c1_{yx} & \u03c1_{yy} \\end{pmatrix} \\mathbf{j} Sketch the expressions for \u03c1_{xx} and \u03c1_{xy} derived in the lecture notes as a function of the magnetic field \\bf{B} . Invert the resistivity matrix to obtain the conductivity matrix, \\begin{pmatrix} \\sigma_{xx} & \\sigma_{xy} \\\\ \\sigma_{yx} & \\sigma_{yy} \\end{pmatrix} allowing you to express \\mathbf{j} as a function of \\mathbf{E} .","title":"Exercise 4: The Hall conductivity matrix and the Hall coefficient"},{"location":"1-intoduction/1-4-emetalsII/","text":"Electrons in metals II \u00b6 Introduction \u00b6 The kinetic theory of Drude was a great first step in trying to answer the question \"metals, how do they work?\", but it was clear from the outset that there were problems with the theory. But with the discovery of the Pauli exclusion principle, it seemed only logical that with the inclusion of the fundamental property of the electrons, life would improve. Indeed it does... Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Statistical mechanics: familiarity with Fermi-Dirac statistics Quantum mechanics: Wavefunction of a free (unbound) electron Text reference The material covered here is discussed in section(s) \\S 4 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: The free electron model \u00b6 Fermi statistics \u00b6 In studying the Debye model , we saw the properties and physical behavior which arise from considering modes of oscillation in a material. The model we look at there, the Sommerfeld model, applies the same conceptual approach to electrons in metals. Sommerfeld considered the electrons as free particles that are not interacting with atomic nuclei, which is why the model is also called the free electron model . Similar to the Debye model, we consider a cubic box of size L \\times L \\times L with periodic boundary conditions. The solutions to the Schr\u00f6dinger equation of a free particle are plane waves: \\psi \\propto \\exp(i\\mathbf{k} \\cdot \\mathbf{r}), where \\mathbf{k} is the electron wave vector. Because we impose periodic boundary conditions, \\mathbf{k} must take discrete values \\frac{2\\pi}{L} (n_x, n_y, n_z) . The plane waves have eigenenergies given by the dispersion relation \\varepsilon(\\mathbf{k}) = \\frac{\\hbar^2 \\mathbf{k}^2}{2m}, with m being the mass of the electron. Let us plot \\varepsilon(k) as a function of k for a 1D system: A plot of the energy $\\varepsilon$ as a function of $k$, where black dot represent possible electron states In is worth highlighting the differences between the modes of oscillation in a solid we have considered previously, and the states of electrons which we now consider: electrons have a quadratic dispersion dispersion relation, and critically, electrons obey fermionic statistics. Conseqeuntly, the occupation of electron states is described by the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} where \\beta = 1/k_{\\textrm{B}} T , \\varepsilon is the energy, and \\mu the chemical potential of an electron. The Fermi-Dirac distribution defines the number of electrons in the system: \\begin{align} N &= 2 \\sum_{\\mathbf{k}} n_{F}(\\beta(\\varepsilon-\\mu))\\\\ &= 2 \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)). \\end{align} Where we have again replaced a discrete sum over k with a volume integral. From where does the factor of 2 come from in the above equation? Contrast this to the Debye model. The factor 2 accounts for the spin degeneracy, whereas in the Debye model we had a factor of 3 to account for the distinct polarisations. To keep track of the origin of this term we will denote the spin degeneracy as 2_s . In the same way that we compute the number of electrons, we can compute the total energy of the electrons via E = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)) . To cement the differences between Debye theory and Sommerfeld theory, different parameters are listed below: Crystal oscillations Electrons Dispersion relation \\omega = v_s \\lvert\\mathbf{k}\\rvert \\varepsilon = \\frac{\\hbar^2\\mathbf{k}^2}{2m} Statistics Bose-Einstein Fermi-Dirac n(\\varepsilon) = 1/(e^{\\beta \\varepsilon} - 1) 1/(e^{\\beta(\\varepsilon - \\mu)} + 1) Degeneracy per \\mathbf{k} 3 (polarization) 2 (spin) Total particle number temperature-dependent constant Note that the last element is important: in the case of oscillations within a material, warming said material creates more more oscillations. In contrast, the number of electrons stays generally remains the same. The Fermi sea \u00b6 To determine the chemical potential \\mu let us consider a 2D system with zero temperature and a finite number of electrons. At T=0 , the Fermi-Dirac distribution is step function n_{F}(\\beta(\\varepsilon-\\mu)) = \\Theta(-(\\varepsilon-\\varepsilon_F)). The chemical potential at T=0 is called the Fermi energy \\varepsilon_F and in this scenario, all electronic states with lower energies are occupied and all the states with higher energies are empty. In the reciprocal space, the occupied \\mathbf{k} -states form a circle (in 1D it is a line and in 3D a sphere). Reciprical space in two dimensions at $T=0$: states within the circle are occuplied and those outside are not A all-pervasive metaphor for describing this state of many electrons is the idea of the Fermi sea : electrons occupy a finite area in reciprocal space, starting from the \"deepest\" points with the lowest energy all the way up to the chemical potential. The border of the Fermi sea is called the Fermi surface , and in the free electron model it is a sphere with the radius equal to the Fermi wave vector . Can you identify the pattern in the nomencalture of important concepts? Pick an object or concept x, and name it the Fermi x. In an attempt to clarify the relationship between these concepts, let us take a look at the dispersion relation in 1D: The dispersion relation for electrons in one dimension By using the dispersion relation, we arrive to the relation \\varepsilon_F = \\frac{\\hbar^2 \\mathbf{k}_F^2}{2m}. The Fermi wavevector \\mathbf{k}_F also defines the Fermi momentum \\mathbf{p}_F = \\hbar \\mathbf{k}_F and the Fermi velocity : \\mathbf{v}_F = \\frac{\\mathbf{p}_F}{m} = \\frac{\\hbar \\mathbf{k}_F}{m}. The Fermi energy of copper is ~7 eV. What is the corresponding Fermi velocity? The Fermi velocity v_F\\approx 1700 km/s or 0.3% of the speed of light! Heat capacity \u00b6 Density of states \u00b6 As were have done previously, we want to compute the heat capacity, and to do this, we need to find the density of states: the number of states per energy interval. We have expression for both the number N and the energy E from above : \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} which we seek to evaluate. Using the same tricks as last time, we move to spherical coordinates to reduce the inegral over three dimension to an intergral over one dimension, we can arrive at the expressions \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ \\varepsilon(k) ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\end{aligned} We rewrite the expression above by substituting k=\\sqrt{2m\\varepsilon/\\hbar^2} and \\mathrm{d}k=\\sqrt{m/2\\varepsilon\\hbar^2} d\\varepsilon : \\begin{aligned} N & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} where the density of states per unit volume is given by \\begin{aligned} g(\\varepsilon) \\mathrm{d} \\varepsilon & = \\frac{2}{(2\\pi)^3} 4 \\pi k^2 \\mathrm{d} k \\\\ & = \\frac{(2m)^{3/2}}{2 \\pi^2 \\hbar^3} \\varepsilon^{1/2} \\mathrm{d} \\varepsilon \\end{aligned} and quantifies the number of energy eigenstates between \\varepsilon and \\varepsilon + \\mathrm{d} \\varepsilon . This expression can be more cleanly written as g(\\varepsilon) \\mathrm{d} \\varepsilon = \\frac{3n}{2\\varepsilon_F}\\left(\\frac{\\varepsilon}{\\varepsilon_F}\\right)^{1/2} Verify the above expression. A good starting point would be to find a value for the number of electrons inside the sphere defined by k_F for T=0 Do the maths , integral of the Heaviside just means intergral is the volume of a sphere. We observe that the density of states of a 3D solid is proportional to a square root of energy: g(\\varepsilon) \\mathrm{d} \\varepsilon \\propto\\sqrt{\\varepsilon} Repeating the similar derivations, we find the density of states of 1D and 2D systems: 1D: g(\\varepsilon) = \\frac{2 L}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto 1/\\sqrt{\\varepsilon} 2D: g(\\varepsilon) = \\frac{k L^2}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto \\text{constant} which we can plot for a comparison of the behaviour of the system with different dimensionalities: Crank the handle \u00b6 Given we have an expression for E , we can set about computing the heat capacity. Sommerfeld expansion To explicitly calculate the heat capacity is a lot of work, and is the definition of a mathematical persuit with little reward. With a bit of hand waving, we can arrive at the same point, so that is what we are going to do. Some will think this lazy, and by all means, feel free to pursue the full calcuation - the Sommerfeld expansion may be of use. To effectively hand wave, let us begin by taking a closer look at the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} which is plotted below for T = 0 and T > 0 with the same chemical potential \\mu = \\varepsilon_F . The Fermi-Dirac distribution at both $T = 0$ and $T > 0$ with the same chemical potential With a finite temperature T>0 , thermal excitations smear out the sharp change in the number of occupied electrons near \\varepsilon_F . Because the Fermi energy is typically in the range of electronvolts, the temperature of \\sim 10 000 ~\\mathrm{K} would be required in order for thermal excitations to give an electron a similar amount of energy! Therefore at room temperature T = 300~\\mathrm{K} the electron distribution over energies is very similar to that at T=0 . Below we compare the number of occupied electron states at each energy g(\\varepsilon) n_{F}(\\beta(\\varepsilon-\\mu)) at T = 0 (blue shaded area) with T > 0 (orange shaded area). In order to estimate the electron energy increase, we approximate difference between the blue and orange areas by triangles, as shown in the figure. This approximation is appropriate because the thermal smearing happens at the energies E \u223c k_B T , and it is much smaller than the Fermi energy \\varepsilon_{F} . At a finite temperature, the electrons occupying the top triangle (blue) are thermally excited to occupy the bottom triangle (orange). The base of the triangle is proportional to k_\\mathrm{B}T and the height is \\sim g(\\varepsilon_F) . Hence the number of excited electrons is N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT (neglecting constants not depending on \\varepsilon_{F} ). These electrons gain k_BT of thermal energy, such that the total extra energy is \\begin{align} E(T) &= E(T = 0) + N_\\mathrm{exc}k_BT\\\\ &\\approx E(T = 0) + g(\\varepsilon_F)k_B^2T^2. \\end{align} Therefore, the electron heat capacity C_e is \\begin{align} C_e &= \\frac{ \\mathrm{d}E}{ \\mathrm{d}T}\\\\ &\\approx 2 g(\u03b5_F)k_B^2T\\\\ &\\overset{\\mathrm{3D}}{=} 3 Nk_B\\frac{T}{T_F}\\\\ &\\propto T, \\end{align} where we used N=\\frac{2}{3}\\varepsilon_Fg(\\varepsilon_F) and defined the Fermi temperature T_F \\equiv \\varepsilon_F/k_B . So what does all of this mean? Well, our algebraic journey has left us with a heat capacity that is linear in T , which to contrast to the model of Debye, had dependence on T^3 , which fitted the measurements pretty well. Does this mean that we have made things worse? Well let's look closely: The heat capacity of Silver divided by temperature versus the square of the temperature as taken from Atomic Heats of Copper, Silver, and Gold from 1\u00b0K to 5\u00b0K What is the expected behaviour of the plot above as predicted by the Debye model and the Sommerfeld model? We now have two contributions to the heat capacity C_{\\textrm{Sommerfeld}} = \\gamma T and C_{\\textrm{Debye}} = \\alpha T^3 , which should manifest as C = \\gamma T + \\alpha T^3. In the plot above, if we plot C/T , any offest at T=0 is indicative of free-electron behaviour, which we indeed observe. It is worth noting that At room temperature C_{\\textrm{Debye}}\\approx 3Nk_B\\gg C_{\\textrm{Sommerfeld}} \\propto k_B T / T_F , because T \\ll T_F . Near T=0 , the heat capacity due to ocillations C_{\\textrm{Debye}} \\propto k_B (T/T_D)^3 , which becomes smaller than the electron heat capacity at T \\lesssim \\sqrt{T_D^3/T_F} The scaling of C \u00b6 The behavior of contribution to C from the free-electron component at low temperature can be intuited via particles within an energy range of \\sim k_{B}T to the Fermi energy \\varepsilon_F become thermally excited, and each carries an extra energy k_{B}T : N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT \\\\ E \\sim N_\\mathrm{exc} k_\\mathrm{B} T Example 1: 3D free electrons \u00b6 In 3D, g(\\varepsilon_F) is roughly constant. Thus the total energy obtained through thermal excitation is proportional to T \\times \\left( T\\times g(\\varepsilon_F) \\right) , from which it follows that C_e \\propto T . Example 2: graphene \u00b6 Graphene has a Fermi energy \\varepsilon_F = 0 and a density of states g(\\varepsilon) \\propto \\varepsilon . Therefore, within the energy range of k_BT , g(\\varepsilon) \\propto k_BT . Thus the total energy is proportional to T \\times T^2 and the heat capacity C_e \\propto T^2 . Conclusions \u00b6 The Sommerfeld free-electron model treats electrons as free particles with energy dispersion \\varepsilon = \\frac{\\hbar^2k^2}{2m} . The Fermi-Dirac distribution gives the probability of an electron state to be occupied. The electron contribution to the heat capacity is proportional to T . It is much lower than the heat capacity due to oscillations at high temperatures, and much higher at low temperatures. The scaling of heat capacity with T can be quickly estimated by estimating the number of particles in an energy range k_\\mathrm{B}T from the Fermi energy. Exercises \u00b6 Preliminary provocations \u00b6 Write down the expression for the total energy of particles with the density of states g(\\varepsilon) and the occupation number n_{F}(\\beta(\\varepsilon - \\mu)) . Explain what happens if a material is heated up to its Fermi temperature (assuming that material where this is possible exists). Why can we not use the Sommerfeld expansion with a Fermi energy of the order of the thermal energy? Is the heat capacity of a solid at temperatures near T=0 dominated by electrons or vibrations? Exercise 1: potassium \u00b6 The Sommerfeld model provides a good description of free electrons in alkali metals such as potassium (element K), which has a Fermi energy of \\varepsilon_{F} = 2.12 eV (data from Ashcroft, N. W. and Mermin, N. D., Solid State Physics, Saunders, 1976.). Check the Fermi surface database . Explain why potassium and (most) other alkali metals can be described well with the Sommerfeld model. Calculate the Fermi temperature, Fermi wave vector and Fermi velocity for potassium. Why is the Fermi temperature much higher than room temperature? Calculate the free electron density n in potassium. Compare this with the actual electron density of potassium, which can be calculated by using the density, atomic mass and atomic number of potassium. What can you conclude from this? Exercise 2: a hypothetical material \u00b6 A hypothetical metal has a Fermi energy \\varepsilon_F = 5.2 \\, \\mathrm{eV} and a density of states g(\\varepsilon) = 2 \\times 10^{10} \\, \\mathrm{eV}^{-\\frac{3}{2}} \\sqrt{\\varepsilon} . Give an integral expression for the total energy of the electrons in this hypothetical material in terms of the density of states g(\\varepsilon) , the temperature T and the chemical potential \\mu = \\varepsilon_F . Find the ground state energy at T = 0 . In order to obtain a good approximation of the integral for non-zero T , one can make use of the Sommerfeld expansion (the first equation is all you need and you can neglect the O\\left(\\frac{1}{\\beta \\mu}\\right)^{4} term). Using this expansion, find the difference between the total energy of the electrons for T = 1000 \\, \\mathrm{K} with that of the ground state. Now, find this difference in energy by calculating the integral found in 1 numerically. Compare your result with 3. Hint You can do numerical integration in python with scipy.integrate.quad(func, xmin, xmax) Calculate the heat capacity for T = 1000 \\, \\mathrm{K} in eV/K. Numerically compute the heat capacity by approximating the derivative of energy difference found in 4 with respect to T . To this end, make use of the fact that \\frac{dy}{dx}=\\lim_{\\Delta x \\to 0} \\frac{y(x + \\Delta x) - y(x - \\Delta x)}{2 \\Delta x}. Compare your result with 5. Exercise 3: graphene \u00b6 One of the most famous recently discovered materials is graphene . It consists of carbon atoms arranged in a 2D honeycomb structure. In this exercise, we will focus on the electrons in bulk graphene. Unlike in metals, electrons in graphene cannot be treated as 'free'. However, close to the Fermi level, the dispersion relation can be approximated by a linear relation: \\varepsilon(\\mathbf{k}) = \\pm c|\\mathbf{k}|. Note that the \\pm here means that there are two energy levels at a specified \\mathbf{k} . The Fermi level is set at \\varepsilon_F = 0 . Make a sketch of the dispersion relation. What other well-known particles have a linear dispersion relation? Using the dispersion relation and assuming periodic boundary conditions, derive an expression for the density of states of graphene. Do not forget spin degeneracy, and take into account that graphene has an additional two-fold 'valley degeneracy' (hence there is a total of a fourfold degeneracy instead of two). Your result should be linear with |\\varepsilon| . Hint It is convenient to first start by only considering the positive energy contributions \\varepsilon(\\mathbf{k}) = + c|\\mathbf{k}| and calculate the density of states for it. Then account for the negative energy contributions \\varepsilon(\\mathbf{k}) = - c|\\mathbf{k}| by adding it to the density of states for the positive energies. You can also make use of \\frac{\\rm{d} |k|}{\\rm{d}k} = \\frac{k}{|k|} . At finite temperatures, assume that electrons close to the Fermi level (i.e. not more than k_B T below the Fermi level) will get thermally excited, thereby increasing their energy by k_B T . Calculate the difference between the energy of the thermally excited state and that of the ground state E(T)-E_0 . To do so, show first that the number of electrons that will get excited is given by n_{ex} = \\frac{1}{2} g(-k_B T) k_B T. Calculate the heat capacity C_e as a function of the temperature T .","title":"1.4: Electrons in metals II"},{"location":"1-intoduction/1-4-emetalsII/#electrons-in-metals-ii","text":"","title":"Electrons in metals II"},{"location":"1-intoduction/1-4-emetalsII/#introduction","text":"The kinetic theory of Drude was a great first step in trying to answer the question \"metals, how do they work?\", but it was clear from the outset that there were problems with the theory. But with the discovery of the Pauli exclusion principle, it seemed only logical that with the inclusion of the fundamental property of the electrons, life would improve. Indeed it does... Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Statistical mechanics: familiarity with Fermi-Dirac statistics Quantum mechanics: Wavefunction of a free (unbound) electron Text reference The material covered here is discussed in section(s) \\S 4 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"1-intoduction/1-4-emetalsII/#the-free-electron-model","text":"","title":"The free electron model"},{"location":"1-intoduction/1-4-emetalsII/#fermi-statistics","text":"In studying the Debye model , we saw the properties and physical behavior which arise from considering modes of oscillation in a material. The model we look at there, the Sommerfeld model, applies the same conceptual approach to electrons in metals. Sommerfeld considered the electrons as free particles that are not interacting with atomic nuclei, which is why the model is also called the free electron model . Similar to the Debye model, we consider a cubic box of size L \\times L \\times L with periodic boundary conditions. The solutions to the Schr\u00f6dinger equation of a free particle are plane waves: \\psi \\propto \\exp(i\\mathbf{k} \\cdot \\mathbf{r}), where \\mathbf{k} is the electron wave vector. Because we impose periodic boundary conditions, \\mathbf{k} must take discrete values \\frac{2\\pi}{L} (n_x, n_y, n_z) . The plane waves have eigenenergies given by the dispersion relation \\varepsilon(\\mathbf{k}) = \\frac{\\hbar^2 \\mathbf{k}^2}{2m}, with m being the mass of the electron. Let us plot \\varepsilon(k) as a function of k for a 1D system: A plot of the energy $\\varepsilon$ as a function of $k$, where black dot represent possible electron states In is worth highlighting the differences between the modes of oscillation in a solid we have considered previously, and the states of electrons which we now consider: electrons have a quadratic dispersion dispersion relation, and critically, electrons obey fermionic statistics. Conseqeuntly, the occupation of electron states is described by the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} where \\beta = 1/k_{\\textrm{B}} T , \\varepsilon is the energy, and \\mu the chemical potential of an electron. The Fermi-Dirac distribution defines the number of electrons in the system: \\begin{align} N &= 2 \\sum_{\\mathbf{k}} n_{F}(\\beta(\\varepsilon-\\mu))\\\\ &= 2 \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)). \\end{align} Where we have again replaced a discrete sum over k with a volume integral. From where does the factor of 2 come from in the above equation? Contrast this to the Debye model. The factor 2 accounts for the spin degeneracy, whereas in the Debye model we had a factor of 3 to account for the distinct polarisations. To keep track of the origin of this term we will denote the spin degeneracy as 2_s . In the same way that we compute the number of electrons, we can compute the total energy of the electrons via E = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon(\\mathbf{k})-\\mu)) . To cement the differences between Debye theory and Sommerfeld theory, different parameters are listed below: Crystal oscillations Electrons Dispersion relation \\omega = v_s \\lvert\\mathbf{k}\\rvert \\varepsilon = \\frac{\\hbar^2\\mathbf{k}^2}{2m} Statistics Bose-Einstein Fermi-Dirac n(\\varepsilon) = 1/(e^{\\beta \\varepsilon} - 1) 1/(e^{\\beta(\\varepsilon - \\mu)} + 1) Degeneracy per \\mathbf{k} 3 (polarization) 2 (spin) Total particle number temperature-dependent constant Note that the last element is important: in the case of oscillations within a material, warming said material creates more more oscillations. In contrast, the number of electrons stays generally remains the same.","title":"Fermi statistics"},{"location":"1-intoduction/1-4-emetalsII/#the-fermi-sea","text":"To determine the chemical potential \\mu let us consider a 2D system with zero temperature and a finite number of electrons. At T=0 , the Fermi-Dirac distribution is step function n_{F}(\\beta(\\varepsilon-\\mu)) = \\Theta(-(\\varepsilon-\\varepsilon_F)). The chemical potential at T=0 is called the Fermi energy \\varepsilon_F and in this scenario, all electronic states with lower energies are occupied and all the states with higher energies are empty. In the reciprocal space, the occupied \\mathbf{k} -states form a circle (in 1D it is a line and in 3D a sphere). Reciprical space in two dimensions at $T=0$: states within the circle are occuplied and those outside are not A all-pervasive metaphor for describing this state of many electrons is the idea of the Fermi sea : electrons occupy a finite area in reciprocal space, starting from the \"deepest\" points with the lowest energy all the way up to the chemical potential. The border of the Fermi sea is called the Fermi surface , and in the free electron model it is a sphere with the radius equal to the Fermi wave vector . Can you identify the pattern in the nomencalture of important concepts? Pick an object or concept x, and name it the Fermi x. In an attempt to clarify the relationship between these concepts, let us take a look at the dispersion relation in 1D: The dispersion relation for electrons in one dimension By using the dispersion relation, we arrive to the relation \\varepsilon_F = \\frac{\\hbar^2 \\mathbf{k}_F^2}{2m}. The Fermi wavevector \\mathbf{k}_F also defines the Fermi momentum \\mathbf{p}_F = \\hbar \\mathbf{k}_F and the Fermi velocity : \\mathbf{v}_F = \\frac{\\mathbf{p}_F}{m} = \\frac{\\hbar \\mathbf{k}_F}{m}. The Fermi energy of copper is ~7 eV. What is the corresponding Fermi velocity? The Fermi velocity v_F\\approx 1700 km/s or 0.3% of the speed of light!","title":"The Fermi sea"},{"location":"1-intoduction/1-4-emetalsII/#heat-capacity","text":"","title":"Heat capacity"},{"location":"1-intoduction/1-4-emetalsII/#density-of-states","text":"As were have done previously, we want to compute the heat capacity, and to do this, we need to find the density of states: the number of states per energy interval. We have expression for both the number N and the energy E from above : \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int \\mathrm{d} \\mathbf{k} ~ \\varepsilon(\\mathbf{k}) n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} which we seek to evaluate. Using the same tricks as last time, we move to spherical coordinates to reduce the inegral over three dimension to an intergral over one dimension, we can arrive at the expressions \\begin{aligned} N & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\\\ E & = 2_s \\frac{V}{(2 \\pi)^3} \\int_0^\\infty \\mathrm{d} k ~ 4\\pi k^2 ~ \\varepsilon(k) ~ n_{F}(\\beta(\\varepsilon(k)-\\mu)) \\end{aligned} We rewrite the expression above by substituting k=\\sqrt{2m\\varepsilon/\\hbar^2} and \\mathrm{d}k=\\sqrt{m/2\\varepsilon\\hbar^2} d\\varepsilon : \\begin{aligned} N & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\\\ E & = V \\int_0^\\infty \\mathrm{d} \\varepsilon ~ \\varepsilon ~ g(\\varepsilon) ~ n_{F}(\\beta(\\varepsilon-\\mu)) \\end{aligned} where the density of states per unit volume is given by \\begin{aligned} g(\\varepsilon) \\mathrm{d} \\varepsilon & = \\frac{2}{(2\\pi)^3} 4 \\pi k^2 \\mathrm{d} k \\\\ & = \\frac{(2m)^{3/2}}{2 \\pi^2 \\hbar^3} \\varepsilon^{1/2} \\mathrm{d} \\varepsilon \\end{aligned} and quantifies the number of energy eigenstates between \\varepsilon and \\varepsilon + \\mathrm{d} \\varepsilon . This expression can be more cleanly written as g(\\varepsilon) \\mathrm{d} \\varepsilon = \\frac{3n}{2\\varepsilon_F}\\left(\\frac{\\varepsilon}{\\varepsilon_F}\\right)^{1/2} Verify the above expression. A good starting point would be to find a value for the number of electrons inside the sphere defined by k_F for T=0 Do the maths , integral of the Heaviside just means intergral is the volume of a sphere. We observe that the density of states of a 3D solid is proportional to a square root of energy: g(\\varepsilon) \\mathrm{d} \\varepsilon \\propto\\sqrt{\\varepsilon} Repeating the similar derivations, we find the density of states of 1D and 2D systems: 1D: g(\\varepsilon) = \\frac{2 L}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto 1/\\sqrt{\\varepsilon} 2D: g(\\varepsilon) = \\frac{k L^2}{\\pi} \\frac{ \\mathrm{d}k}{ \\mathrm{d}\\varepsilon} \\propto \\text{constant} which we can plot for a comparison of the behaviour of the system with different dimensionalities:","title":"Density of states"},{"location":"1-intoduction/1-4-emetalsII/#crank-the-handle","text":"Given we have an expression for E , we can set about computing the heat capacity. Sommerfeld expansion To explicitly calculate the heat capacity is a lot of work, and is the definition of a mathematical persuit with little reward. With a bit of hand waving, we can arrive at the same point, so that is what we are going to do. Some will think this lazy, and by all means, feel free to pursue the full calcuation - the Sommerfeld expansion may be of use. To effectively hand wave, let us begin by taking a closer look at the Fermi-Dirac distribution n_{F}(\\beta(\\varepsilon-\\mu)) = \\frac{1}{e^{\\beta(\\varepsilon-\\mu)}+1} which is plotted below for T = 0 and T > 0 with the same chemical potential \\mu = \\varepsilon_F . The Fermi-Dirac distribution at both $T = 0$ and $T > 0$ with the same chemical potential With a finite temperature T>0 , thermal excitations smear out the sharp change in the number of occupied electrons near \\varepsilon_F . Because the Fermi energy is typically in the range of electronvolts, the temperature of \\sim 10 000 ~\\mathrm{K} would be required in order for thermal excitations to give an electron a similar amount of energy! Therefore at room temperature T = 300~\\mathrm{K} the electron distribution over energies is very similar to that at T=0 . Below we compare the number of occupied electron states at each energy g(\\varepsilon) n_{F}(\\beta(\\varepsilon-\\mu)) at T = 0 (blue shaded area) with T > 0 (orange shaded area). In order to estimate the electron energy increase, we approximate difference between the blue and orange areas by triangles, as shown in the figure. This approximation is appropriate because the thermal smearing happens at the energies E \u223c k_B T , and it is much smaller than the Fermi energy \\varepsilon_{F} . At a finite temperature, the electrons occupying the top triangle (blue) are thermally excited to occupy the bottom triangle (orange). The base of the triangle is proportional to k_\\mathrm{B}T and the height is \\sim g(\\varepsilon_F) . Hence the number of excited electrons is N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT (neglecting constants not depending on \\varepsilon_{F} ). These electrons gain k_BT of thermal energy, such that the total extra energy is \\begin{align} E(T) &= E(T = 0) + N_\\mathrm{exc}k_BT\\\\ &\\approx E(T = 0) + g(\\varepsilon_F)k_B^2T^2. \\end{align} Therefore, the electron heat capacity C_e is \\begin{align} C_e &= \\frac{ \\mathrm{d}E}{ \\mathrm{d}T}\\\\ &\\approx 2 g(\u03b5_F)k_B^2T\\\\ &\\overset{\\mathrm{3D}}{=} 3 Nk_B\\frac{T}{T_F}\\\\ &\\propto T, \\end{align} where we used N=\\frac{2}{3}\\varepsilon_Fg(\\varepsilon_F) and defined the Fermi temperature T_F \\equiv \\varepsilon_F/k_B . So what does all of this mean? Well, our algebraic journey has left us with a heat capacity that is linear in T , which to contrast to the model of Debye, had dependence on T^3 , which fitted the measurements pretty well. Does this mean that we have made things worse? Well let's look closely: The heat capacity of Silver divided by temperature versus the square of the temperature as taken from Atomic Heats of Copper, Silver, and Gold from 1\u00b0K to 5\u00b0K What is the expected behaviour of the plot above as predicted by the Debye model and the Sommerfeld model? We now have two contributions to the heat capacity C_{\\textrm{Sommerfeld}} = \\gamma T and C_{\\textrm{Debye}} = \\alpha T^3 , which should manifest as C = \\gamma T + \\alpha T^3. In the plot above, if we plot C/T , any offest at T=0 is indicative of free-electron behaviour, which we indeed observe. It is worth noting that At room temperature C_{\\textrm{Debye}}\\approx 3Nk_B\\gg C_{\\textrm{Sommerfeld}} \\propto k_B T / T_F , because T \\ll T_F . Near T=0 , the heat capacity due to ocillations C_{\\textrm{Debye}} \\propto k_B (T/T_D)^3 , which becomes smaller than the electron heat capacity at T \\lesssim \\sqrt{T_D^3/T_F}","title":"Crank the handle"},{"location":"1-intoduction/1-4-emetalsII/#the-scaling-of-c","text":"The behavior of contribution to C from the free-electron component at low temperature can be intuited via particles within an energy range of \\sim k_{B}T to the Fermi energy \\varepsilon_F become thermally excited, and each carries an extra energy k_{B}T : N_\\mathrm{exc} \\approx g(\\varepsilon_F)k_BT \\\\ E \\sim N_\\mathrm{exc} k_\\mathrm{B} T","title":"The scaling of C"},{"location":"1-intoduction/1-4-emetalsII/#example-1-3d-free-electrons","text":"In 3D, g(\\varepsilon_F) is roughly constant. Thus the total energy obtained through thermal excitation is proportional to T \\times \\left( T\\times g(\\varepsilon_F) \\right) , from which it follows that C_e \\propto T .","title":"Example 1: 3D free electrons"},{"location":"1-intoduction/1-4-emetalsII/#example-2-graphene","text":"Graphene has a Fermi energy \\varepsilon_F = 0 and a density of states g(\\varepsilon) \\propto \\varepsilon . Therefore, within the energy range of k_BT , g(\\varepsilon) \\propto k_BT . Thus the total energy is proportional to T \\times T^2 and the heat capacity C_e \\propto T^2 .","title":"Example 2: graphene"},{"location":"1-intoduction/1-4-emetalsII/#conclusions","text":"The Sommerfeld free-electron model treats electrons as free particles with energy dispersion \\varepsilon = \\frac{\\hbar^2k^2}{2m} . The Fermi-Dirac distribution gives the probability of an electron state to be occupied. The electron contribution to the heat capacity is proportional to T . It is much lower than the heat capacity due to oscillations at high temperatures, and much higher at low temperatures. The scaling of heat capacity with T can be quickly estimated by estimating the number of particles in an energy range k_\\mathrm{B}T from the Fermi energy.","title":"Conclusions"},{"location":"1-intoduction/1-4-emetalsII/#exercises","text":"","title":"Exercises"},{"location":"1-intoduction/1-4-emetalsII/#preliminary-provocations","text":"Write down the expression for the total energy of particles with the density of states g(\\varepsilon) and the occupation number n_{F}(\\beta(\\varepsilon - \\mu)) . Explain what happens if a material is heated up to its Fermi temperature (assuming that material where this is possible exists). Why can we not use the Sommerfeld expansion with a Fermi energy of the order of the thermal energy? Is the heat capacity of a solid at temperatures near T=0 dominated by electrons or vibrations?","title":"Preliminary provocations"},{"location":"1-intoduction/1-4-emetalsII/#exercise-1-potassium","text":"The Sommerfeld model provides a good description of free electrons in alkali metals such as potassium (element K), which has a Fermi energy of \\varepsilon_{F} = 2.12 eV (data from Ashcroft, N. W. and Mermin, N. D., Solid State Physics, Saunders, 1976.). Check the Fermi surface database . Explain why potassium and (most) other alkali metals can be described well with the Sommerfeld model. Calculate the Fermi temperature, Fermi wave vector and Fermi velocity for potassium. Why is the Fermi temperature much higher than room temperature? Calculate the free electron density n in potassium. Compare this with the actual electron density of potassium, which can be calculated by using the density, atomic mass and atomic number of potassium. What can you conclude from this?","title":"Exercise 1: potassium"},{"location":"1-intoduction/1-4-emetalsII/#exercise-2-a-hypothetical-material","text":"A hypothetical metal has a Fermi energy \\varepsilon_F = 5.2 \\, \\mathrm{eV} and a density of states g(\\varepsilon) = 2 \\times 10^{10} \\, \\mathrm{eV}^{-\\frac{3}{2}} \\sqrt{\\varepsilon} . Give an integral expression for the total energy of the electrons in this hypothetical material in terms of the density of states g(\\varepsilon) , the temperature T and the chemical potential \\mu = \\varepsilon_F . Find the ground state energy at T = 0 . In order to obtain a good approximation of the integral for non-zero T , one can make use of the Sommerfeld expansion (the first equation is all you need and you can neglect the O\\left(\\frac{1}{\\beta \\mu}\\right)^{4} term). Using this expansion, find the difference between the total energy of the electrons for T = 1000 \\, \\mathrm{K} with that of the ground state. Now, find this difference in energy by calculating the integral found in 1 numerically. Compare your result with 3. Hint You can do numerical integration in python with scipy.integrate.quad(func, xmin, xmax) Calculate the heat capacity for T = 1000 \\, \\mathrm{K} in eV/K. Numerically compute the heat capacity by approximating the derivative of energy difference found in 4 with respect to T . To this end, make use of the fact that \\frac{dy}{dx}=\\lim_{\\Delta x \\to 0} \\frac{y(x + \\Delta x) - y(x - \\Delta x)}{2 \\Delta x}. Compare your result with 5.","title":"Exercise 2: a hypothetical material"},{"location":"1-intoduction/1-4-emetalsII/#exercise-3-graphene","text":"One of the most famous recently discovered materials is graphene . It consists of carbon atoms arranged in a 2D honeycomb structure. In this exercise, we will focus on the electrons in bulk graphene. Unlike in metals, electrons in graphene cannot be treated as 'free'. However, close to the Fermi level, the dispersion relation can be approximated by a linear relation: \\varepsilon(\\mathbf{k}) = \\pm c|\\mathbf{k}|. Note that the \\pm here means that there are two energy levels at a specified \\mathbf{k} . The Fermi level is set at \\varepsilon_F = 0 . Make a sketch of the dispersion relation. What other well-known particles have a linear dispersion relation? Using the dispersion relation and assuming periodic boundary conditions, derive an expression for the density of states of graphene. Do not forget spin degeneracy, and take into account that graphene has an additional two-fold 'valley degeneracy' (hence there is a total of a fourfold degeneracy instead of two). Your result should be linear with |\\varepsilon| . Hint It is convenient to first start by only considering the positive energy contributions \\varepsilon(\\mathbf{k}) = + c|\\mathbf{k}| and calculate the density of states for it. Then account for the negative energy contributions \\varepsilon(\\mathbf{k}) = - c|\\mathbf{k}| by adding it to the density of states for the positive energies. You can also make use of \\frac{\\rm{d} |k|}{\\rm{d}k} = \\frac{k}{|k|} . At finite temperatures, assume that electrons close to the Fermi level (i.e. not more than k_B T below the Fermi level) will get thermally excited, thereby increasing their energy by k_B T . Calculate the difference between the energy of the thermally excited state and that of the ground state E(T)-E_0 . To do so, show first that the number of electrons that will get excited is given by n_{ex} = \\frac{1}{2} g(-k_B T) k_B T. Calculate the heat capacity C_e as a function of the temperature T .","title":"Exercise 3: graphene"},{"location":"2-chemistry/2-1-chemistry/","text":"Chemisty 101 \u00b6 Also known as the physicist's guide to chemistry 1 . Introduction \u00b6 Our journey thus far has been considering the basic properties of solids, and our descriptions have been based around the properties of electrons and vibrations in solids, which can provide useful results but our endeavour is much grander: we would like to explain all the properties of all the solids. The first step on this journey is to no longer consider a collection of free electrons; solids are made up of many atoms, so we best find a way of baking this into whatever we do! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: the Schr\u00f6dinger equation, wavefunction of the hydrogen atom, wavefunction for a particle in a box, Dirac notation Mathematics: the variational principle, linear algebra Text reference The material covered here is discussed in section(s) \\S 5, 6.3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A retrospective \u00b6 Up to this point, we have: Introduced reciprocal ( k -space) Postulated the dispersion relations for free electrons and oscillations in a solid Calculated the heat capacity of free electrons and oscillations in a solid which has permitted The understanding of how materials can store heat via oscillations (Debye model) The understanding of how free electrons conduct (Drude model) and store heat/energy (Sommerfeld model) In constructing these models, we have made several approximations and postulations, and there are a few big questions to answer: In the Debye model, why is there a cutoff frequency for oscillations? Why are there no modes of oscillation beyond this frequency? In the Drude model, we have electrons modelled as a gas, but why don't electrons scatter from the atoms in solid? Why are some materials metallic, and others not metallic? If we are to have a hope of understanding any of the above, we are going to have to understand better what is happening on the atomic scale. Atoms, how do they work? \u00b6 It is no exaggeration to say that everything can be pretty-well described by the Schr\u00f6dinger equation: \\hat{H}\\psi = E\\psi, with \\hat{H} the Hamiltonian of the system, that is, the sum of kinetic and potential energies. For the hydrogen atom, the potential is due to the Coulomb interaction between the electron and the nucleus: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r^2}} - \\frac{e^2}{4\\pi\\varepsilon_0|r|} which you will have solved in detail elsewhere. If we move to the next-most-simple atom, helium, the Hamiltonian immediately becomes more complex, containing not just the Coulomb interaction between the individual electrons and the nuclei, but also Coulomb repulsion between the two electrons: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_1^2}} -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_2^2}}- \\frac{2e^2}{4\\pi\\varepsilon_0|r_1|} - \\frac{2e^2}{4\\pi\\varepsilon_0|r_2|} + \\frac{e^2}{4\\pi\\varepsilon_0|r_1 - r_2|}. From a purely mathematical point of view, this means we need find eigenvalues and eigenvectors of a six-dimensional PDE, which is much tougher work than the three-dimensional PDE descirbing hydrogen. You have likely seen the variational method to construct appoximate wavefunctions, any even in this case it is a fair amount of work. Let us now turn to another system, a single copper atom. Copper has 29 electrons, so to find the energy spectrum of copper we would need to solve an 87-dimensional Schr\u00f6dinger equation. Such things are totally intractable: there is simply no way to solve such a thing, even with really big computers. It is this growth in complexity with the number of interacting quantum particles is why many-body quantum physics is very much an active research area in solid-state physics and quantum chemistry. But we need not have an analytic solution for everything in order to be satisfied, on the contrary, we can use physical insight and approximations to construct models of complex systems based on simpler, easier to handle systems. To some extent, this is exactly where chemisty takes over: it is often said that Chemisty is applied physics, but they should simply be thought of as the same thing, but with a focus on different parts of the spectrum: the complexity of solving problems in chemistry is the reason why we need to accept empirical observations as chemical laws , even though they work with limited precision and ultimately consequences of the underlying physics as modelled by the Schr\u00f6dinger equation. Quantum numbers and shell filling \u00b6 The wavefunction for an electron in a hydrogen atom is described by the state |n, l, m_l, m_s\\rangle , where the quantum numbers are: n = 1, 2, \\ldots is the principal (azimuthal) quantum number l = 0, 1, \\ldots, n-1 is the orbital angular momentum (also known as s, p, d, f orbitals) m_l = -l, -l+1 \\ldots, l is the z -component of angular momentum l m_s is the z -component of the electronic spin The low energy states of hydrogen are shown below: Electronic wavefunctions for the low-energy states of Hydrogen. Image source: Wikipedia \u00a9 Geek3 CC-BY-SA To a first approximation, the electronic wavefunctions in other atoms are similar to those of hydrogen; however, the electron energies are very different due to the Coulomb interaction, both between other electrons and the ionic core. Even still, knowledge of the hydrogenic wavefunctions can greatly aid our description of other atoms. If one considers an atom with multiple electrons, as we \"add\" electrons to a given system - for example, if one considers a sequence of elements - it is important to determine the order in which electronic states will be filled. Two empirical rules which can do an excellent job of describing this order are: Aufbau principle : electrons fill the lowest-lying states first and fill these shells to completion Madelung's rule : electrons first occupy shells with the lowest n+l . If there are several orbitals with equal n+l , electrons preferentially occupy those with smaller n Combining the two rules, we obtain the shell filing order: 1s, 2s, 2p, 3s, 3p, 4s, 3d, etc., which is illustrated in the periodic table below: While these rules accurately predict the electronic structure of most elements, they are only approximate, and fail to describe some of the heavier elements . The reason we care about the order in which shells are filled is because as we have seen, the valence electrons, that is, those in the outermost shell dominate the elements properties, including underpinning chemical reactions and conductivity. Inner-shell electrons acts to shield the charge from the valance electrons - resulting in an altered effective nuclear charge as seen by the valance electrons - but beyond this, they are largely inert. Bonds: the ties that bind \u00b6 There exist many different mechanisms for bonding, for example: ionic bonding, covalent bonding, bonding through the van der Waals interaction, and the list continues. At its simplest, one can understand bonding as two (or more) systems combining to reduce the collective energy, that is, reaching a state with lower energy than the sum of energies of the individual states. Ionic bonding \u00b6 The conceptually simplest form of bonding is when an atom that has an electron of which it isn't much fond, meets and atom which really likes collecting electrons. The most common example of this would be the reaction of sodium and chlorine: \\textrm{Na + Cl} \\rightarrow \\textrm{Na}^+ + \\textrm{Cl}^- \\rightarrow \\textrm{NaCl} where one can think of electron transfer from sodium ion to the chlorine ion, rendering sodium and chlorine in their lowest energy states but also leaving them charged, and these ionic species consequently attract and form a compound. To firm up this concept, we can consider the ionisation energy , which is the energy required to remove an electron (and form a positive ion) and the electron affinity , which is the energy associated with the capture of an electron to form a negative ion. Shown below are periodic tables with values for the ionisation energy and the electron affinity indicated. First-ionisation energies for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 Electron affinity (first electron) for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 It should be clear that in a situation where an atom A has a low ionisation energy and an atom B with a large electron efficiency, there will be some energy difference between the states of neutral atoms and ions \\Delta E_{A+B \\rightarrow A^+ + B^-} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} It is also important to consider the energy that comes from the attraction between the oppositely charged ionic species E_{\\textrm{cohesive}} = \\Delta E_{A^+ + B^- \\rightarrow AB} which is mostly determined by the Coulomb interaction between the ions. The energy change for the bonding process in then \\Delta E_{A+B \\rightarrow AB} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} - E_{\\textrm{cohesive}} and the reaction will proceed if \\Delta E_{A+B \\rightarrow AB}<0 . The properties of ionic solids are that they tend to have high melting points - the bond is strong! - and they tend to be electrical insulating (something we will discuss later). Covalent bonding \u00b6 Covalent bonding - as the name suggests - involves the sharing of electrons between atoms. Particles in boxes \u00b6 The simplest way one can model the bonding process is to consider the electronic wavefunctions of the atoms at particles in a box, which is likely the first quantum system you ever discussed in detail. Whilst the picture is not going to produce highly-accurate results, the qualitative behaviour goes a long way to understand the process more generally. Let us consider two identical atoms, where we model the atomic potential as a one-dimensional box with length L , which are far apart to ignore the influence of the other potential, then the energy of each state is given by E = \\frac{\\hbar^2\\pi^2}{2mL^2} If our boxed-like atoms are brought close together such that the boxes merge into one large box of length 2L , the energy of this new bonding state will be given by E = \\frac{\\hbar^2\\pi^2}{2m(2L)^2} If we imagine that our atoms were hydrogenic - that is, with a single valance electron - the electrons from both atoms could occupy this ground state due to the spin degeneracy of the state, and thus a new system with lower energy is formed. It is this interaction which results in Hydrogen existing as a the diatomic molecule \\textrm{H}_2 . Now imagine a system with more electrons, for example, 2 electrons. When these atoms come together, the shared electrons will occupy the lower-energy bonded state, but must also occupy the first excited state, the so-called anti-bonded state which will have energy E = \\frac{2^2\\hbar^2\\pi^2}{2m(2L)^2} Under these circumstances, it is not energetically favourable to a covalent bond, and thus molecules are not formed - see the noble gasses here! Probabilities for electrons in bonding and antibonding states as modelled using square-well potentials The linear combination of atomic orbitals ( LCAO ) \u00b6 The mouthful that is LCAO is also commonly called molecular orbital or Tight-binding theory, and is a way to quantitatively justify the handwaving from above. Consider a system of two atoms next to each other from which we want to calculate their propensity to form a molecule, that is, calculate the energy eigenstates as a function of the separation of the nuclei. As the nuclei are big and heavy, we are going to assume that their positions are fixed for all positions of the electrons (aka the Born-Oppenheimer approximation). For an individual electron, the Hamiltonian is given by \\hat{H} = \\hat{V}_1 + \\hat{V}_2 + \\hat{K}, where V_1 the potential due to the first nucleus, V_2 due to the second nucleus, and K is the kinetic energy of the electron. Whilst it is possible to solve the Schr\u00f6dinger equation directly, it is a fair bit of work, and we want to have a mechanism for solving these systems more generally, so we will seek a solution using the variational method. We begin by denoting the wave function of an electron bound to the first and second atom |1\\rangle and |2\\rangle respectively, which we know to be eigenfunctions of the systems: \\begin{align} (\\hat{V}_1 + \\hat{K})|1\\rangle = \\varepsilon_0 |1\\rangle \\\\ (\\hat{V}_2 + \\hat{K})|2\\rangle = \\varepsilon_0 |2\\rangle \\end{align} We are going to search for a solution in the form: |\\psi\\rangle = \\phi_{1}|1\\rangle + \\phi_{2}|2\\rangle. where \\phi_{1} and \\phi_{2} are the probability amplitudes of the respective electronic states. The state |\\psi\\rangle is called a molecular orbital because it describes the state of the diatomic molecule, and the molecular orbital is created as a Linear Combination of Atomic Orbitals . To increase the tractability of the problem, we are assume that the atomic orbitals are orthogonal, that is \\langle1|2\\rangle=0 ; however, this is not strictly necessary. We can then write the Schr\u00f6dinger equation for our system in matrix form \\sum_j H_{ij} \\phi_j = \\epsilon\\phi_i where the matrix elements of the Hamiltonian are given by H_{ij} = \\langle i | \\hat{H} | j \\rangle 5.1 Compute the matrix elements of the Hamiltonian \\hat{H} Crank the handle! \\begin{align} H_{11} = \\langle 1 | \\hat{H} | 1 \\rangle = \\langle 1 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{22} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 2 | \\hat{V}_1 | 2 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{12} = \\langle 1 | \\hat{H} | 2 \\rangle = \\langle 1 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 1 | \\hat{V}_1 | 2 \\rangle = 0 - t \\\\ H_{21} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 2 | \\hat{V}_2 | 1 \\rangle = 0 - t* \\end{align} where V_{\\textrm{cross}} = \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\langle 2 | \\hat{V}_2 | 2 \\rangle is the potential felt by state | 1 \\rangle due to nucleus 2 and vice versa, and the term t = - \\langle 1 | \\hat{V}_1 | 2 \\rangle = - \\langle 2 | \\hat{V}_2 | 1 \\rangle is the energy associated with a movement between states | 1 \\rangle and | 2 \\rangle The matrix is thus defined by two parameters, the onsite energy which gives the energy of an electron occupying either of the orbitals, and the so-called hopping integral which is the energy associated with the exchange of the electronic states 2 . 5.2 Using the matrix computed above, calculate the energy eigenstates for the system and their energies From above, we have the Schr\u00f6dinger equation in the form \\begin{bmatrix} \\varepsilon_0 + V_{\\textrm{cross}} & -t \\\\ -t* & \\varepsilon_0 + V_{\\textrm{cross}} \\end{bmatrix} \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} = E \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} which has Eigenvalues E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t and Eigenvectors \\begin{align} |\\psi_{+}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle + |2\\rangle) \\\\ |\\psi_{-}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle - |2\\rangle) \\end{align} Bonding vs antibonding \u00b6 In the definition above, we have that \\psi_{+} is symmetric and |\\psi_{-}\\rangle is antisymmetric, so from the node theorem , we can immediately identify that E_+ < E_- , or in a previous terminology, \\psi_{+} is the bonding state and |\\psi_{-}\\rangle is the antibonding state. From the expression for the energy of the states E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t by decreasing the interatomic distance, the two atoms get closer and the atomic orbitals have more overlap, resulting in an increased hopping t . Assuming a hopping factor similar in form the potential, we can plot the energies E_{\\pm} as a function of the inter-atomic distance \\Delta x : When an electron occupies the state |\\psi_+\\rangle , the atoms attract (or bond ) because the total energy is lowered. With an electron in the |\\psi_{-}\\rangle state, the molecular energy increases with decreasing interatomic distance and thus the atoms repel each other. Therefore, if each atom has a single electron in the outermost shell, these atoms attract because the bonding orbital hosts two electrons with opposite spins. On the other hand, if each atom has 0 or 2 electrons in the outermost shell, the net force from the bonding and antibonding orbitals cancels out, but Coulomb repulsion remains. Conclusions \u00b6 Electrons in atoms occupy shells, with only electrons in the outermost shell (valence electrons) contributing to interatomic interactions The molecular orbital can be written as a Linear Combination of Atomic Orbitals ( LCAO ) The LCAO method reduces the full Hamiltonian to a finite size problem written in the basis of individual orbitals If two atoms have one orbital and one electron each, they occupy the bonding orbital Exercises \u00b6 Preliminary provocations \u00b6 Is the assumption that the atomic orbitals are orthogonal always a reasonable assumption? What happens if the hopping t is chosen to be negative? How does the size of the Hamiltonian matrix change with the number of atoms? How does the size of the Hamiltonian matrix change if each atom now has two orbitals? Assuming that we have two atoms with a single orbital each, what is the size of the Hamiltonian matrix if we also consider the spin of the electron? Exercise 1: Shell-filling model of atoms \u00b6 Describe the shell-filling model of atoms. Use Madelung\u2019s rule to deduce the atomic shellfilling configuration of the element tungsten, which has atomic number 74. Although Madelung\u2019s rule for the filling of electronic shells holds extremely well, there are a number of exceptions to the rule. Here are a few of them: \\textrm{Cu} = [\\textrm{Ar}] 4\\textrm{s}^1 3\\textrm{d}^{10} \\textrm{Pd} = [\\textrm{Kr}] 5\\textrm{s}^0 4\\textrm{d}^{10} \\textrm{Ag} = [\\textrm{Kr}] 5\\textrm{s}^1 4\\textrm{d}^{10} \\textrm{Au} = [\\textrm{Xe}] 6\\textrm{s}^1 4\\textrm{f}^{14} 5\\textrm{d}^{10} What should the electron configurations be if these elements followed Madelung\u2019s rule and the Aufbau principle? What could be the reason for the deficiency of Madelung's rule? Exercise 2: Application of the LCAO model to the delta-function potential \u00b6 Consider an electron moving in 1D between two negative delta-function shaped potential wells. The complete Hamiltonian of this one-dimensional system is then: \\hat{H} = \\frac{\\hat{p}^2}{2m}-V_0\\delta(x_1-x)-V_0\\delta(x_2-x), where V_0>0 is the potential strength, \\hat{p} the momentum of the electron, and x_1 , x_2 the positions of the potential wells. Properties of a single \\delta -function potential A delta function \\delta(x_0 - x) centered at x_0 is defined to be zero everywhere except for x_0 , where it tends to infinity. Further, a delta function has the property: \\int_{-\\infty}^{\\infty} f(x)\\delta(x_0-x)dx = f(x_0). The procedure to find the energy and a wave function of a state bound in a \\delta -function potential, V=-V_0\\delta(x-x_0) , is similar to that of a quantum well: Assume that we have a bound state with energy E<0 . Compute the wave function \\phi in different regions of space: namely x < x_0 and x > x_0 . Apply the boundary conditions at x = x_0 . The wave function \\phi must be continuous, but d\\phi/dx is not. Instead due to the presence of the delta-function: \\frac{d\\phi}{dx}\\Bigr|_{x_0+\\epsilon} - \\frac{d\\phi}{dx}\\Bigr|_{x_0-\\epsilon}= -\\frac{2mV_0}{\\hbar^2}\\phi(x_0). Find at which energy the boundary conditions at x = x_0 are satisfied. This is the energy of the bound state. Normalize the wave function. Let us apply the LCAO model to solve this problem. Consider the trial wave function for the ground state to be a linear combination of two orbitals |1\\rangle and |2\\rangle : |\\psi\\rangle = \\phi_1|1\\rangle + \\phi_2|2\\rangle. The orbitals |1\\rangle and |2\\rangle correspond to the wave functions of the electron when there is only a single delta peak present: H_1 |1\\rangle = \\epsilon_1 |1\\rangle, H_2 |2\\rangle = \\epsilon_2 |2\\rangle. We start of by calculating the wavefunction of an electron bound to a single delta-peak. To do so, you first need to set up the Schr\u00f6dinger equation of a single electron bound to a single delta-peak. You do not have to solve the Schr\u00f6dinger equation twice\u2014you can use the symmetry of the system to calculate the wavefunction of the other electron bound to the second delta-peak. Find the expressions for the wave functions of the states |1\\rangle and |2\\rangle : \\psi_1(x) and \\psi_2(x) . Also find an expression for their energies \\epsilon_1 and \\epsilon_2 . Remember that you need to normalize the wave functions. Construct the LCAO Hamiltonian. To simplify the calculations, assume that the orbitals are orthogonal. Diagonalize the LCAO Hamiltonian and find an expression for the eigenenergies of the system. It was previously mentioned that V_0>0 . Using this, determine which energy corresponds to the bonding energy. Exercise 3: Polarisation of a hydrogen molecule \u00b6 Consider a hydrogen molecule as a one-dimensional system with two identical nuclei at x=-\\frac{d}{2} and x=+\\frac{d}{2} , so that the center of the molecule is at x=0 . Each atom contains a single electron with charge -e . The LCAO Hamiltonian of the system is given by H_{\\textrm{eff}} = \\begin{pmatrix} E_0&&-t \\\\ -t&&E_0 \\end{pmatrix}. The electric potential is given by V_{\\mathbf{E}}=-\\int_{C} \\mathbf{E} \\cdot \\mathrm{d} \\boldsymbol{\\ell} Let us add an electric field \\mathcal{E} \\hat{\\bf{x}} to the system. Which term needs to be added to the Hamiltonian of each electron? Compute the LCAO Hamiltonian of the system in presence of the electric field. What are the new onsite energies of the two orbitals? Diagonalize the modified LCAO Hamiltonian. Find the ground state wavefunction \\psi . Find the polarisation P of the molecule in the ground state. Reminder: polarisation The polarisation P of a molecule with n\\leq 2 electrons at its ground state |\\psi\\rangle is: P = n e \\langle\\psi|\\hat{x}|\\psi\\rangle. Use that ground state you found in 3.2 is a linear superposition of two orthogonal orbitals centered at -\\frac{d}{2} and +\\frac{d}{2} . For reasons I simply cannot understand, there is often a tension between chemistry and physics: they are both fantastic and any erosion of barriers between the two disciplines is to be commended. \u21a9 In atomic physics, these terms are called the direct and exchange energies, which makes (in my opinion) makes much more sense; however, solid-state physics reserves these names for other things... \u21a9","title":"2: Chemistry 101"},{"location":"2-chemistry/2-1-chemistry/#chemisty-101","text":"Also known as the physicist's guide to chemistry 1 .","title":"Chemisty 101"},{"location":"2-chemistry/2-1-chemistry/#introduction","text":"Our journey thus far has been considering the basic properties of solids, and our descriptions have been based around the properties of electrons and vibrations in solids, which can provide useful results but our endeavour is much grander: we would like to explain all the properties of all the solids. The first step on this journey is to no longer consider a collection of free electrons; solids are made up of many atoms, so we best find a way of baking this into whatever we do! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: the Schr\u00f6dinger equation, wavefunction of the hydrogen atom, wavefunction for a particle in a box, Dirac notation Mathematics: the variational principle, linear algebra Text reference The material covered here is discussed in section(s) \\S 5, 6.3 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"2-chemistry/2-1-chemistry/#a-retrospective","text":"Up to this point, we have: Introduced reciprocal ( k -space) Postulated the dispersion relations for free electrons and oscillations in a solid Calculated the heat capacity of free electrons and oscillations in a solid which has permitted The understanding of how materials can store heat via oscillations (Debye model) The understanding of how free electrons conduct (Drude model) and store heat/energy (Sommerfeld model) In constructing these models, we have made several approximations and postulations, and there are a few big questions to answer: In the Debye model, why is there a cutoff frequency for oscillations? Why are there no modes of oscillation beyond this frequency? In the Drude model, we have electrons modelled as a gas, but why don't electrons scatter from the atoms in solid? Why are some materials metallic, and others not metallic? If we are to have a hope of understanding any of the above, we are going to have to understand better what is happening on the atomic scale.","title":"A retrospective"},{"location":"2-chemistry/2-1-chemistry/#atoms-how-do-they-work","text":"It is no exaggeration to say that everything can be pretty-well described by the Schr\u00f6dinger equation: \\hat{H}\\psi = E\\psi, with \\hat{H} the Hamiltonian of the system, that is, the sum of kinetic and potential energies. For the hydrogen atom, the potential is due to the Coulomb interaction between the electron and the nucleus: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r^2}} - \\frac{e^2}{4\\pi\\varepsilon_0|r|} which you will have solved in detail elsewhere. If we move to the next-most-simple atom, helium, the Hamiltonian immediately becomes more complex, containing not just the Coulomb interaction between the individual electrons and the nuclei, but also Coulomb repulsion between the two electrons: H=-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_1^2}} -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial {\\mathbf r_2^2}}- \\frac{2e^2}{4\\pi\\varepsilon_0|r_1|} - \\frac{2e^2}{4\\pi\\varepsilon_0|r_2|} + \\frac{e^2}{4\\pi\\varepsilon_0|r_1 - r_2|}. From a purely mathematical point of view, this means we need find eigenvalues and eigenvectors of a six-dimensional PDE, which is much tougher work than the three-dimensional PDE descirbing hydrogen. You have likely seen the variational method to construct appoximate wavefunctions, any even in this case it is a fair amount of work. Let us now turn to another system, a single copper atom. Copper has 29 electrons, so to find the energy spectrum of copper we would need to solve an 87-dimensional Schr\u00f6dinger equation. Such things are totally intractable: there is simply no way to solve such a thing, even with really big computers. It is this growth in complexity with the number of interacting quantum particles is why many-body quantum physics is very much an active research area in solid-state physics and quantum chemistry. But we need not have an analytic solution for everything in order to be satisfied, on the contrary, we can use physical insight and approximations to construct models of complex systems based on simpler, easier to handle systems. To some extent, this is exactly where chemisty takes over: it is often said that Chemisty is applied physics, but they should simply be thought of as the same thing, but with a focus on different parts of the spectrum: the complexity of solving problems in chemistry is the reason why we need to accept empirical observations as chemical laws , even though they work with limited precision and ultimately consequences of the underlying physics as modelled by the Schr\u00f6dinger equation.","title":"Atoms, how do they work?"},{"location":"2-chemistry/2-1-chemistry/#quantum-numbers-and-shell-filling","text":"The wavefunction for an electron in a hydrogen atom is described by the state |n, l, m_l, m_s\\rangle , where the quantum numbers are: n = 1, 2, \\ldots is the principal (azimuthal) quantum number l = 0, 1, \\ldots, n-1 is the orbital angular momentum (also known as s, p, d, f orbitals) m_l = -l, -l+1 \\ldots, l is the z -component of angular momentum l m_s is the z -component of the electronic spin The low energy states of hydrogen are shown below: Electronic wavefunctions for the low-energy states of Hydrogen. Image source: Wikipedia \u00a9 Geek3 CC-BY-SA To a first approximation, the electronic wavefunctions in other atoms are similar to those of hydrogen; however, the electron energies are very different due to the Coulomb interaction, both between other electrons and the ionic core. Even still, knowledge of the hydrogenic wavefunctions can greatly aid our description of other atoms. If one considers an atom with multiple electrons, as we \"add\" electrons to a given system - for example, if one considers a sequence of elements - it is important to determine the order in which electronic states will be filled. Two empirical rules which can do an excellent job of describing this order are: Aufbau principle : electrons fill the lowest-lying states first and fill these shells to completion Madelung's rule : electrons first occupy shells with the lowest n+l . If there are several orbitals with equal n+l , electrons preferentially occupy those with smaller n Combining the two rules, we obtain the shell filing order: 1s, 2s, 2p, 3s, 3p, 4s, 3d, etc., which is illustrated in the periodic table below: While these rules accurately predict the electronic structure of most elements, they are only approximate, and fail to describe some of the heavier elements . The reason we care about the order in which shells are filled is because as we have seen, the valence electrons, that is, those in the outermost shell dominate the elements properties, including underpinning chemical reactions and conductivity. Inner-shell electrons acts to shield the charge from the valance electrons - resulting in an altered effective nuclear charge as seen by the valance electrons - but beyond this, they are largely inert.","title":"Quantum numbers and shell filling"},{"location":"2-chemistry/2-1-chemistry/#bonds-the-ties-that-bind","text":"There exist many different mechanisms for bonding, for example: ionic bonding, covalent bonding, bonding through the van der Waals interaction, and the list continues. At its simplest, one can understand bonding as two (or more) systems combining to reduce the collective energy, that is, reaching a state with lower energy than the sum of energies of the individual states.","title":"Bonds: the ties that bind"},{"location":"2-chemistry/2-1-chemistry/#ionic-bonding","text":"The conceptually simplest form of bonding is when an atom that has an electron of which it isn't much fond, meets and atom which really likes collecting electrons. The most common example of this would be the reaction of sodium and chlorine: \\textrm{Na + Cl} \\rightarrow \\textrm{Na}^+ + \\textrm{Cl}^- \\rightarrow \\textrm{NaCl} where one can think of electron transfer from sodium ion to the chlorine ion, rendering sodium and chlorine in their lowest energy states but also leaving them charged, and these ionic species consequently attract and form a compound. To firm up this concept, we can consider the ionisation energy , which is the energy required to remove an electron (and form a positive ion) and the electron affinity , which is the energy associated with the capture of an electron to form a negative ion. Shown below are periodic tables with values for the ionisation energy and the electron affinity indicated. First-ionisation energies for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 Electron affinity (first electron) for various elements. Image source: LibreTexts which is used under licence CC-BY-NC-SA 3.0 It should be clear that in a situation where an atom A has a low ionisation energy and an atom B with a large electron efficiency, there will be some energy difference between the states of neutral atoms and ions \\Delta E_{A+B \\rightarrow A^+ + B^-} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} It is also important to consider the energy that comes from the attraction between the oppositely charged ionic species E_{\\textrm{cohesive}} = \\Delta E_{A^+ + B^- \\rightarrow AB} which is mostly determined by the Coulomb interaction between the ions. The energy change for the bonding process in then \\Delta E_{A+B \\rightarrow AB} = E_{\\textrm{ionisation},~A} - E_{\\textrm{affinity},~B} - E_{\\textrm{cohesive}} and the reaction will proceed if \\Delta E_{A+B \\rightarrow AB}<0 . The properties of ionic solids are that they tend to have high melting points - the bond is strong! - and they tend to be electrical insulating (something we will discuss later).","title":"Ionic bonding"},{"location":"2-chemistry/2-1-chemistry/#covalent-bonding","text":"Covalent bonding - as the name suggests - involves the sharing of electrons between atoms.","title":"Covalent bonding"},{"location":"2-chemistry/2-1-chemistry/#particles-in-boxes","text":"The simplest way one can model the bonding process is to consider the electronic wavefunctions of the atoms at particles in a box, which is likely the first quantum system you ever discussed in detail. Whilst the picture is not going to produce highly-accurate results, the qualitative behaviour goes a long way to understand the process more generally. Let us consider two identical atoms, where we model the atomic potential as a one-dimensional box with length L , which are far apart to ignore the influence of the other potential, then the energy of each state is given by E = \\frac{\\hbar^2\\pi^2}{2mL^2} If our boxed-like atoms are brought close together such that the boxes merge into one large box of length 2L , the energy of this new bonding state will be given by E = \\frac{\\hbar^2\\pi^2}{2m(2L)^2} If we imagine that our atoms were hydrogenic - that is, with a single valance electron - the electrons from both atoms could occupy this ground state due to the spin degeneracy of the state, and thus a new system with lower energy is formed. It is this interaction which results in Hydrogen existing as a the diatomic molecule \\textrm{H}_2 . Now imagine a system with more electrons, for example, 2 electrons. When these atoms come together, the shared electrons will occupy the lower-energy bonded state, but must also occupy the first excited state, the so-called anti-bonded state which will have energy E = \\frac{2^2\\hbar^2\\pi^2}{2m(2L)^2} Under these circumstances, it is not energetically favourable to a covalent bond, and thus molecules are not formed - see the noble gasses here! Probabilities for electrons in bonding and antibonding states as modelled using square-well potentials","title":"Particles in boxes"},{"location":"2-chemistry/2-1-chemistry/#the-linear-combination-of-atomic-orbitals-lcao","text":"The mouthful that is LCAO is also commonly called molecular orbital or Tight-binding theory, and is a way to quantitatively justify the handwaving from above. Consider a system of two atoms next to each other from which we want to calculate their propensity to form a molecule, that is, calculate the energy eigenstates as a function of the separation of the nuclei. As the nuclei are big and heavy, we are going to assume that their positions are fixed for all positions of the electrons (aka the Born-Oppenheimer approximation). For an individual electron, the Hamiltonian is given by \\hat{H} = \\hat{V}_1 + \\hat{V}_2 + \\hat{K}, where V_1 the potential due to the first nucleus, V_2 due to the second nucleus, and K is the kinetic energy of the electron. Whilst it is possible to solve the Schr\u00f6dinger equation directly, it is a fair bit of work, and we want to have a mechanism for solving these systems more generally, so we will seek a solution using the variational method. We begin by denoting the wave function of an electron bound to the first and second atom |1\\rangle and |2\\rangle respectively, which we know to be eigenfunctions of the systems: \\begin{align} (\\hat{V}_1 + \\hat{K})|1\\rangle = \\varepsilon_0 |1\\rangle \\\\ (\\hat{V}_2 + \\hat{K})|2\\rangle = \\varepsilon_0 |2\\rangle \\end{align} We are going to search for a solution in the form: |\\psi\\rangle = \\phi_{1}|1\\rangle + \\phi_{2}|2\\rangle. where \\phi_{1} and \\phi_{2} are the probability amplitudes of the respective electronic states. The state |\\psi\\rangle is called a molecular orbital because it describes the state of the diatomic molecule, and the molecular orbital is created as a Linear Combination of Atomic Orbitals . To increase the tractability of the problem, we are assume that the atomic orbitals are orthogonal, that is \\langle1|2\\rangle=0 ; however, this is not strictly necessary. We can then write the Schr\u00f6dinger equation for our system in matrix form \\sum_j H_{ij} \\phi_j = \\epsilon\\phi_i where the matrix elements of the Hamiltonian are given by H_{ij} = \\langle i | \\hat{H} | j \\rangle 5.1 Compute the matrix elements of the Hamiltonian \\hat{H} Crank the handle! \\begin{align} H_{11} = \\langle 1 | \\hat{H} | 1 \\rangle = \\langle 1 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{22} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 2 | \\hat{V}_1 | 2 \\rangle = \\varepsilon_0 + V_{\\textrm{cross}} \\\\ H_{12} = \\langle 1 | \\hat{H} | 2 \\rangle = \\langle 1 | \\hat{V}_2 + \\hat{K} | 2 \\rangle + \\langle 1 | \\hat{V}_1 | 2 \\rangle = 0 - t \\\\ H_{21} = \\langle 2 | \\hat{H} | 2 \\rangle = \\langle 2 | \\hat{V}_1 + \\hat{K} | 1 \\rangle + \\langle 2 | \\hat{V}_2 | 1 \\rangle = 0 - t* \\end{align} where V_{\\textrm{cross}} = \\langle 1 | \\hat{V}_2 | 1 \\rangle = \\langle 2 | \\hat{V}_2 | 2 \\rangle is the potential felt by state | 1 \\rangle due to nucleus 2 and vice versa, and the term t = - \\langle 1 | \\hat{V}_1 | 2 \\rangle = - \\langle 2 | \\hat{V}_2 | 1 \\rangle is the energy associated with a movement between states | 1 \\rangle and | 2 \\rangle The matrix is thus defined by two parameters, the onsite energy which gives the energy of an electron occupying either of the orbitals, and the so-called hopping integral which is the energy associated with the exchange of the electronic states 2 . 5.2 Using the matrix computed above, calculate the energy eigenstates for the system and their energies From above, we have the Schr\u00f6dinger equation in the form \\begin{bmatrix} \\varepsilon_0 + V_{\\textrm{cross}} & -t \\\\ -t* & \\varepsilon_0 + V_{\\textrm{cross}} \\end{bmatrix} \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} = E \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\end{bmatrix} which has Eigenvalues E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t and Eigenvectors \\begin{align} |\\psi_{+}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle + |2\\rangle) \\\\ |\\psi_{-}\\rangle = \\tfrac{1}{\\sqrt{2}}(|1\\rangle - |2\\rangle) \\end{align}","title":"The linear combination of atomic orbitals (LCAO)"},{"location":"2-chemistry/2-1-chemistry/#bonding-vs-antibonding","text":"In the definition above, we have that \\psi_{+} is symmetric and |\\psi_{-}\\rangle is antisymmetric, so from the node theorem , we can immediately identify that E_+ < E_- , or in a previous terminology, \\psi_{+} is the bonding state and |\\psi_{-}\\rangle is the antibonding state. From the expression for the energy of the states E_{\\pm} = \\varepsilon_0 + V_{\\textrm{cross}} \\mp t by decreasing the interatomic distance, the two atoms get closer and the atomic orbitals have more overlap, resulting in an increased hopping t . Assuming a hopping factor similar in form the potential, we can plot the energies E_{\\pm} as a function of the inter-atomic distance \\Delta x : When an electron occupies the state |\\psi_+\\rangle , the atoms attract (or bond ) because the total energy is lowered. With an electron in the |\\psi_{-}\\rangle state, the molecular energy increases with decreasing interatomic distance and thus the atoms repel each other. Therefore, if each atom has a single electron in the outermost shell, these atoms attract because the bonding orbital hosts two electrons with opposite spins. On the other hand, if each atom has 0 or 2 electrons in the outermost shell, the net force from the bonding and antibonding orbitals cancels out, but Coulomb repulsion remains.","title":"Bonding vs antibonding"},{"location":"2-chemistry/2-1-chemistry/#conclusions","text":"Electrons in atoms occupy shells, with only electrons in the outermost shell (valence electrons) contributing to interatomic interactions The molecular orbital can be written as a Linear Combination of Atomic Orbitals ( LCAO ) The LCAO method reduces the full Hamiltonian to a finite size problem written in the basis of individual orbitals If two atoms have one orbital and one electron each, they occupy the bonding orbital","title":"Conclusions"},{"location":"2-chemistry/2-1-chemistry/#exercises","text":"","title":"Exercises"},{"location":"2-chemistry/2-1-chemistry/#preliminary-provocations","text":"Is the assumption that the atomic orbitals are orthogonal always a reasonable assumption? What happens if the hopping t is chosen to be negative? How does the size of the Hamiltonian matrix change with the number of atoms? How does the size of the Hamiltonian matrix change if each atom now has two orbitals? Assuming that we have two atoms with a single orbital each, what is the size of the Hamiltonian matrix if we also consider the spin of the electron?","title":"Preliminary provocations"},{"location":"2-chemistry/2-1-chemistry/#exercise-1-shell-filling-model-of-atoms","text":"Describe the shell-filling model of atoms. Use Madelung\u2019s rule to deduce the atomic shellfilling configuration of the element tungsten, which has atomic number 74. Although Madelung\u2019s rule for the filling of electronic shells holds extremely well, there are a number of exceptions to the rule. Here are a few of them: \\textrm{Cu} = [\\textrm{Ar}] 4\\textrm{s}^1 3\\textrm{d}^{10} \\textrm{Pd} = [\\textrm{Kr}] 5\\textrm{s}^0 4\\textrm{d}^{10} \\textrm{Ag} = [\\textrm{Kr}] 5\\textrm{s}^1 4\\textrm{d}^{10} \\textrm{Au} = [\\textrm{Xe}] 6\\textrm{s}^1 4\\textrm{f}^{14} 5\\textrm{d}^{10} What should the electron configurations be if these elements followed Madelung\u2019s rule and the Aufbau principle? What could be the reason for the deficiency of Madelung's rule?","title":"Exercise 1: Shell-filling model of atoms"},{"location":"2-chemistry/2-1-chemistry/#exercise-2-application-of-the-lcao-model-to-the-delta-function-potential","text":"Consider an electron moving in 1D between two negative delta-function shaped potential wells. The complete Hamiltonian of this one-dimensional system is then: \\hat{H} = \\frac{\\hat{p}^2}{2m}-V_0\\delta(x_1-x)-V_0\\delta(x_2-x), where V_0>0 is the potential strength, \\hat{p} the momentum of the electron, and x_1 , x_2 the positions of the potential wells. Properties of a single \\delta -function potential A delta function \\delta(x_0 - x) centered at x_0 is defined to be zero everywhere except for x_0 , where it tends to infinity. Further, a delta function has the property: \\int_{-\\infty}^{\\infty} f(x)\\delta(x_0-x)dx = f(x_0). The procedure to find the energy and a wave function of a state bound in a \\delta -function potential, V=-V_0\\delta(x-x_0) , is similar to that of a quantum well: Assume that we have a bound state with energy E<0 . Compute the wave function \\phi in different regions of space: namely x < x_0 and x > x_0 . Apply the boundary conditions at x = x_0 . The wave function \\phi must be continuous, but d\\phi/dx is not. Instead due to the presence of the delta-function: \\frac{d\\phi}{dx}\\Bigr|_{x_0+\\epsilon} - \\frac{d\\phi}{dx}\\Bigr|_{x_0-\\epsilon}= -\\frac{2mV_0}{\\hbar^2}\\phi(x_0). Find at which energy the boundary conditions at x = x_0 are satisfied. This is the energy of the bound state. Normalize the wave function. Let us apply the LCAO model to solve this problem. Consider the trial wave function for the ground state to be a linear combination of two orbitals |1\\rangle and |2\\rangle : |\\psi\\rangle = \\phi_1|1\\rangle + \\phi_2|2\\rangle. The orbitals |1\\rangle and |2\\rangle correspond to the wave functions of the electron when there is only a single delta peak present: H_1 |1\\rangle = \\epsilon_1 |1\\rangle, H_2 |2\\rangle = \\epsilon_2 |2\\rangle. We start of by calculating the wavefunction of an electron bound to a single delta-peak. To do so, you first need to set up the Schr\u00f6dinger equation of a single electron bound to a single delta-peak. You do not have to solve the Schr\u00f6dinger equation twice\u2014you can use the symmetry of the system to calculate the wavefunction of the other electron bound to the second delta-peak. Find the expressions for the wave functions of the states |1\\rangle and |2\\rangle : \\psi_1(x) and \\psi_2(x) . Also find an expression for their energies \\epsilon_1 and \\epsilon_2 . Remember that you need to normalize the wave functions. Construct the LCAO Hamiltonian. To simplify the calculations, assume that the orbitals are orthogonal. Diagonalize the LCAO Hamiltonian and find an expression for the eigenenergies of the system. It was previously mentioned that V_0>0 . Using this, determine which energy corresponds to the bonding energy.","title":"Exercise 2: Application of the LCAO model to the delta-function potential"},{"location":"2-chemistry/2-1-chemistry/#exercise-3-polarisation-of-a-hydrogen-molecule","text":"Consider a hydrogen molecule as a one-dimensional system with two identical nuclei at x=-\\frac{d}{2} and x=+\\frac{d}{2} , so that the center of the molecule is at x=0 . Each atom contains a single electron with charge -e . The LCAO Hamiltonian of the system is given by H_{\\textrm{eff}} = \\begin{pmatrix} E_0&&-t \\\\ -t&&E_0 \\end{pmatrix}. The electric potential is given by V_{\\mathbf{E}}=-\\int_{C} \\mathbf{E} \\cdot \\mathrm{d} \\boldsymbol{\\ell} Let us add an electric field \\mathcal{E} \\hat{\\bf{x}} to the system. Which term needs to be added to the Hamiltonian of each electron? Compute the LCAO Hamiltonian of the system in presence of the electric field. What are the new onsite energies of the two orbitals? Diagonalize the modified LCAO Hamiltonian. Find the ground state wavefunction \\psi . Find the polarisation P of the molecule in the ground state. Reminder: polarisation The polarisation P of a molecule with n\\leq 2 electrons at its ground state |\\psi\\rangle is: P = n e \\langle\\psi|\\hat{x}|\\psi\\rangle. Use that ground state you found in 3.2 is a linear superposition of two orthogonal orbitals centered at -\\frac{d}{2} and +\\frac{d}{2} . For reasons I simply cannot understand, there is often a tension between chemistry and physics: they are both fantastic and any erosion of barriers between the two disciplines is to be commended. \u21a9 In atomic physics, these terms are called the direct and exchange energies, which makes (in my opinion) makes much more sense; however, solid-state physics reserves these names for other things... \u21a9","title":"Exercise 3: Polarisation of a hydrogen molecule"},{"location":"3-1d/3-1-vibrations/","text":"Vibrations \u00b6 Introduction \u00b6 We can consider our journey thus far a preparation: we have seen how quantum mechanics forced its way into statistical mechanics with much success, but also started a discussion of using quantum mechanics to describe complex systems, rather than just applying a finishing touch of quantum to a classical system. Beginning here, we are going to set out to conquer solid-stead state systems from a quantum standpoint; if someone ever asks: have you studied solid-state physics? it is the content in this, and the following sections, about which they are asking. The concepts are core to understanding basically everything we do from here on out, so strap in and let's get into it! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: Mechanics: Normal modes of an oscillator Mathematics: Series expansions of functions (Maclaurin/Taylor series) Text reference The material covered here is discussed in section(s) \\S 8, 9 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here Material properties from the interatomic potential \u00b6 In the previous section, we looked at the energy eigenstates of neighbouring atoms which were sufficiently close to display bonding behaviour. The energy eigenstates \\psi_{+} and |\\psi_{-}\\rangle had energies as a function of atomic separation as shown below: Such a model is a good start, but it is unphysical as the bonding energy does continue to decrease with decreasing interatomic distance, rather, it reaches some minimum value before it starts to increase as the nuclei begin to repel. The exact nature of this repulsion is not so important, but it its existence is extremely important. A more realistic form of the energy for the bonding state is shown below, which includes a repulsion term when the atoms are sufficiently close: The physics in which we are interested, that is, solids, necessitates that we are interested in the regime where bonding occurs, so we are going to be most interested in the region of the energy minimum. To begin, let us consider two atoms that are in equilibrium and at the bottom of the potential well as shown in the image above. We are going to denote the spacing which corresponds to this minimum \\delta x = x_{\\mathrm{eq}} , the equilibrium separation. As the potential is nicely behaved in this region (and indeed everywhere else!), we can then express the potential V \\approx V_0(x_{\\mathrm{eq}}) + \\frac{\\kappa}{2!} (r - x_{\\mathrm{eq}})^2 - \\frac{\\kappa_3}{3!} (r - x_{\\mathrm{eq}})^3 + \\ldots we have deliberately excluded term linear in x_{\\mathrm{eq}} , as its inclusion would remove the minimum! In the usual way, we are only going to consider small displacements around x_{\\mathrm{eq}} and then throw all terms other than the constant and the quadratic in the bin, greatly simplifying our life. 3.1.1: With a potential of the form V(x) = V_0 + a x^2 , what will be the motion of an atom in the potential? Compressibility \u00b6 With our potential approximated as a quadratic, let us consider what happens when we compress or stretch the system. Consider a material with length L ; if the equilibrium distance between atoms is x_{\\mathrm{eq}} , then there are N = L/x_{\\mathrm{eq}} atoms in the material. Now lets change the material to length L + \\delta L , which will lead to a change in the interatomic distance of \\delta x = \\delta L/N . With only small changes in x we can then write F = - \\frac{\\mathrm{d} V}{\\mathrm{d} x} \\Bigr|_{x = x_{\\mathrm{eq}} + \\delta x} = \\kappa a \\frac{\\delta L}{L}. This is a description of the compressibility (or elasticity) of a solid, and from a mechanics, the compressibility \\beta 1 is usually defined through the relation \\beta = -\\frac{1}{V} \\frac{\\partial V}{\\partial P} for a three dimensional system. In the one dimensional case, this simplifies to \\beta = -\\frac{1}{L} \\frac{\\partial L}{\\partial F}= \\frac{1}{\\kappa x_{\\mathrm{eq}}} \\equiv \\frac{1}{\\kappa a} where we have introduced a , the symbol used to denote the distance between identical particles, which in this case is our equilibrium interatomic spacing x_{\\mathrm{eq}} Sound velocity \u00b6 The compressibility \\beta allows us to calculate the speed of sound by using the usual relation between the speed v and the density \\rho , and the bulk modulus B : v = \\sqrt{\\frac{B}{\\rho}} = \\sqrt{\\frac{1}{\\rho \\beta}} and because in one-dimension \\rho = m/a (in 1D), we can predict that v = \\sqrt{\\frac{\\kappa a^2}{m}}. Let us put a pin in this result, as we are about to construct a detailed microscopic model for a solid which will allow for calculation of v , it will be useful to compare the results. Vibrations in one dimension \u00b6 To emphasize the similarities and the differences between electrons and phonons, we will deal with both types of particles at once. Up to this point, we have seen the Boltzmann, Einstein, and Debye models of solids, which at the core are models of vibrations in a solid. In the case of Debye, the language around this was much more explicit; however, in all cases, we were considering the motion of the constituent atoms and how ultimately, this allowed for the storage of energy. Here we are going to construct an atomic-scale model of an atomic lattice with an attempt to better understand both the successes and failures of these early models of solids An infinite one-dimensional chain of identical atoms \u00b6 We begin by considering the amalgamation of the concepts ripped from physicists' bingo , namely: a low-dimensional model, and infinite model, a model stripped of unnecessary complexity, and a model centred around a harmonic potential. But in all seriousness, the model is incredibly rich and goes a long way to understanding solids! Consider a chain of identical atoms of mass m with equilibrium spacing a , connected to nearest-neighbours by springs with spring constant \\kappa . Hopefully it is clear from the discussion above, that by using a spring model, we are explicitly using a Harmonic potential which we assume well-approximates the inter-atomic potential for bound systems and small oscillation amplitudes, that is, low temperature. We define the position of the n^{\\textrm{th}} atom to be x_n and the equilibrium position of the n^{\\textrm{th}} atom to be x_{n, \\textrm{eq}}=na . We then write the displacement of an atom from its equilibrium position \\delta x_n = x_n - x_{n, \\textrm{eq}}. The potential energy for this harmonic chain is then the sum over all harmonic potentials \\begin{align} V & = \\sum_i V(x_{i+1}-x_i) = \\sum_i \\kappa/2 (x_{i+1}-x_i-a)^2 \\\\ & = \\sum_i \\kappa/2 (\\delta x_{i+1}- \\delta x_i)^2 \\end{align} from which we can write the force on atom n as F_n = - \\frac{\\partial V}{\\partial x_n} = \\kappa (\\delta x_{n+1}- \\delta x_n) + \\kappa (\\delta x_{n-1}- \\delta x_n) which can be expressed as a 2^{\\mathrm{nd}} -order ODE in \\delta x_n : m (\\ddot{\\delta x_n}) = \\kappa (\\delta x_{n+1} + \\delta x_{n-1} - 2\\delta x_n). With systems such as this, one is usually interested in the normal modes of the system: oscillations where all particles move with the same frequency. Given we are persuing vibrations in the systems this is a worthwhile thing in which to be interested, but moreover, there is a deep connection between normal modes of a classical system and the energy Eigenstates of the equivalent quantum system, so let's consider progress in this direction an down payment of sorts. Given that the structure of the equation of motion is the identical no matter what value of n we choose, and since these equations define the solutions, one can reason that the solutions should also be independent of the choice of n . This, combined with us actively looking for wave solutions, leads us to assume solutions in the form of plane waves, with the same amplitude for each atom: \\delta x_n = A e^{i \\omega t - i k x_{n, \\textrm{eq}}} = A e^{i \\omega t - i k n a} 3.1.2: Assuming plain-wave solutions, show that \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| It is worth stopping to emphasise that we now have a non-trivial dispersion relation \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| which in contrast to that we have seen previously in the Debye model, \\omega = v_s |k| , is very different. 3.1.3: What can one say about the relationship between the dispersion relationship for a 1D chain of oscillators and that of the Debye model? The reciprocal lattice \u00b6 Let's go ahead an plot the dispersion relation: where we can see that the dispersion relationship is periodic . This is clearly evident from the functional form of the relation, but just let that idea marinate for a minute: the dispersion, that is, the relationship between the oscillation frequency \\omega and k is periodic. What does that even mean? Well in the same way that we saw periodic boundary conditions resulting an a discretisation of k- space in the Debye model, a more general principle is that if a system is period in real space (that is, in position space) with a periodicity a , it will also be periodic in k- space with periodicity 2\\pi/a . Mathematically, it is clear why this happens, but what is the physical interpretation? Let's start by taking advantage of this periodicity by considering only the \"unit cell\" - the periodic unit in reciprocal ( k- ) space - which is called the Brillouin zone . A plot of the Brillouin zone for the dispersion curve above is shown below: As the dispersion is periodic with period 2\\pi/a , we need only look at the Brillouin zone to understand how a material behaver for all k . But how does this all work? For example, consider our oscillation modes \\delta x_n = A e^{i \\omega t - i k n a} 3.1.4: What happens to a given mode of oscillation under the transformation k \\rightarrow k + 2\\pi/a ? Given the periodicity of k space, we can think about the set of points which are equivalent to the point k=0 ( k = \\pm n \\times 2\\pi/a ), which is called the reciprocal lattice , in contrast to the real-space lattice x_n = n a . A useful observation for all points G_m in the reciprocal lattice and points x_n in the real-space lattice is that e^{i G_m x_n} = 1 BUT WHAT IS ACTUALLY GOING ON?!?! \u00b6 If we think about life outside of periodic systems, for example, the propagation of light in a vacuum, one of our favourite relationships is c = \\frac{\\omega}{k} So what does it mean to talk about a (phase) velocity when k is periodic? And what about the wavelength? We are pretty fond on the relation \\lambda = \\frac{2\\pi}{k}, but if k and k + n \\times 2\\pi/a are equivalent, what is heck is going on? 3.1.5: What the heck is going on? Counting normal modes \u00b6 As is (hopefully) becoming a normal question to ask when we consider modes of oscillation: how many modes are there? Well let us consider a chain of N masses, and once again we are going to assume periodic boundary conditions, with the same justification as last time, namely that we are interested in bulk quantities and we will make sure that we don't take look at things happening near boundaries. In a periodic system, we can consider the chain wrapped around back onto itself, such that e^{i\\omega t - ikna} = e^{i\\omega t - ik(N+n)a} which enforces the relation e^{ikNa} = 1 and subsequently restricts the possible values of k : k = \\frac{2\\pi q}{N a} = \\frac{2\\pi q}{L} \\textrm{ where } q \\in \\mathbb{Z}. This is exactly as we saw with the Debye model , where the imposition of periodic boundary conditions discretised reciprocal space. There we saw that points were equally spaced with a separation of 2\\pi/L , which in this case is equivalent to 2\\pi/Na . Now, to count all the modes, we can simply compute the ration of the possible values of k by the spacing between modes. As discussed above, we need only consider the Brillouin zone, as the modes are periodic in k with a period of 2\\pi/a , therefore the number of modes is \\frac{2\\pi/a}{2\\pi/Na} = N which again, is what Debye had predicted - although he just plucked the result from the air - whereas we now have a grounding to say that there is one normal mode per mass in the system. \"Quantum vibrations\" \u00b6 Our discussion up to this point has been purely classical, but thanks to firm foundations, mapping our classical system onto the equivalent quantum system is not so difficult. Explicitly, classical harmonic systems - and thus normal modes - map directly onto \"equivalent\" quantum systems, and a normal mode of oscillation at frequency \\omega will have eigenstates with energy E_n = \\hbar \\omega \\left(n + \\frac{1}{2}\\right). This means that for a given mode with wavevector k , there are multiple energy eigenstates, each separated in energy by \\hbar\\omega(k) . We can now start to think of excitations (and de-excitations) of this particular mode, a quantum of energy \\hbar\\omega(k) , which are known as phonons . Phonons are much like photons - especially in the under the formalism of second quantisation - but obviously are not quanta of the electromagnetic field, but rather the oscillation modes of a solid. In the same way phonons can occupy the same state - i.e. they are bosons - phonons are also bosons, so we can describe the occupation of a given mode though the Bose factor n_\\mathrm{B} . Therefore we can write an expression for the energy associated with the wavevector k as E_k = \\hbar\\omega(k)\\left(n_\\mathrm{B}(\\beta\\hbar\\omega(k))+\\frac{1}{2}\\right) 3.1.6: Use the energy E_k to get obtain an expression for the total energy in the system in terms of an integral over k With this, we now have a well-formulated quantum system from which we can calculate quantities of interest, but also extend to model systems that are not just infinite chains of identical particles. Conclusions \u00b6 The interatomic potential describes the compressibility of a material A system which is periodic in real space with period a in periodic in reciprocal space with period 2\\pi/a The Brillouin zone contains all values of k , as modes separated by 2\\pi/a are identical A normal modes of frequency \\omega is mapped to the eigenstate with energy $E_n = \\hbar \\omega \\left( n + 1/2 \\right) Exercises \u00b6 Preliminary provocations \u00b6 What is the motion of adjacent masses when the chain is oscillating at the its maximum frequency? Exercise 1: Lennard-Jones potential \u00b6 A simple model approximating the interaction between a pair of noble gas atoms such as Argon is the Lennard-Jones potential , in which the potential energy as a function of interatomic distance is U(r) = 4\\epsilon \\left[\\big(\\frac{\\sigma}{r}\\big)^{12}-\\big(\\frac{\\sigma}{r}\\big)^6\\right] where r is the distance between two atoms, \\epsilon is the depth of the potential well, and \\sigma is the distance at which the inter-particle potential is zero. Sketch U(r) as a function of interatomic distance and mark the regions of repulsive and attractive forces acting between the atoms. Find the distance, r_0 (bond length), at which the potential energy is minimal and find the value of the potential energy at this distance (binding energy of the molecule). Expand U(r) in a Taylor series around r_0 up to second order. By considering a second-order (=harmonic) potential approximation around the minimum ( r_0 ), find an expression for the spring constant, \\kappa , in terms of \\epsilon and \\sigma . Using the spring constant \\kappa you found earlier, find the ground state energy of the molecule by comparing the molecule to a quantum harmonic oscillator. What is the energy required to break the molecule apart? What is the approximate number of phonons that can occupy this mode before the potential becomes anharmonic? Hint Because the diatomic molecule is modelled as a one-body problem (in the center of mass rest frame of the molecule), the mass should be replaced by the reduced mass . Exercise 2: Vibrational heat capacity of a 1D monatomic chain \u00b6 Give an integral expression for the heat capacity C . Compute the heat capacity numerically, using e.g. Python. Do the same for C in the Debye model and compare the two. What differences do you see? Exercise 3: A finite chain \u00b6 Consider a chain of only 3 atoms. We can then write the equations of motion \\begin{aligned} m \\ddot{\\delta x}_1 &= - \\kappa (\\delta x_1 - \\delta x_2) \\\\ m \\ddot{\\delta x}_2 &= - \\kappa (\\delta x_2 - \\delta x_1) - \\kappa (\\delta x_2 - \\delta x_3) \\\\ m \\ddot{\\delta x}_3 &= - \\kappa (\\delta x_3 - \\delta x_2) \\end{aligned}, and write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} Hint In Python use the function numpy.diag Define a matrix that relates forces to displacements in a linear 1D chain containing N=5 atoms. Repeat for N=200 . You may assume that the masses and spring constants are equivalent throughout the chain. Using numerical diagonalization ( numpy.linalg.eigvalsh ), compute the eigenfrequencies of this atomic chain. Plot a histogram of these eigenfrequencies. Make the masses of every even atom different from the masses of every odd atom. Compute the eigenfrequencies of this atomic chain and plot a histogram. Now make the masses of even and odd atoms equivalent again. Furthermore, make the spring constants of every even spring different from the odd spring. Compute the eigenfrequencies of this atomic chain and plot a histogram. It is unfortunately that there are only so many letters in the Greek alphabet, especially when looking at mechanical and thermodynamic quantities! \u21a9","title":"3.1: Vibrations"},{"location":"3-1d/3-1-vibrations/#vibrations","text":"","title":"Vibrations"},{"location":"3-1d/3-1-vibrations/#introduction","text":"We can consider our journey thus far a preparation: we have seen how quantum mechanics forced its way into statistical mechanics with much success, but also started a discussion of using quantum mechanics to describe complex systems, rather than just applying a finishing touch of quantum to a classical system. Beginning here, we are going to set out to conquer solid-stead state systems from a quantum standpoint; if someone ever asks: have you studied solid-state physics? it is the content in this, and the following sections, about which they are asking. The concepts are core to understanding basically everything we do from here on out, so strap in and let's get into it! Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Quantum mechanics: Mechanics: Normal modes of an oscillator Mathematics: Series expansions of functions (Maclaurin/Taylor series) Text reference The material covered here is discussed in section(s) \\S 8, 9 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here","title":"Introduction"},{"location":"3-1d/3-1-vibrations/#material-properties-from-the-interatomic-potential","text":"In the previous section, we looked at the energy eigenstates of neighbouring atoms which were sufficiently close to display bonding behaviour. The energy eigenstates \\psi_{+} and |\\psi_{-}\\rangle had energies as a function of atomic separation as shown below: Such a model is a good start, but it is unphysical as the bonding energy does continue to decrease with decreasing interatomic distance, rather, it reaches some minimum value before it starts to increase as the nuclei begin to repel. The exact nature of this repulsion is not so important, but it its existence is extremely important. A more realistic form of the energy for the bonding state is shown below, which includes a repulsion term when the atoms are sufficiently close: The physics in which we are interested, that is, solids, necessitates that we are interested in the regime where bonding occurs, so we are going to be most interested in the region of the energy minimum. To begin, let us consider two atoms that are in equilibrium and at the bottom of the potential well as shown in the image above. We are going to denote the spacing which corresponds to this minimum \\delta x = x_{\\mathrm{eq}} , the equilibrium separation. As the potential is nicely behaved in this region (and indeed everywhere else!), we can then express the potential V \\approx V_0(x_{\\mathrm{eq}}) + \\frac{\\kappa}{2!} (r - x_{\\mathrm{eq}})^2 - \\frac{\\kappa_3}{3!} (r - x_{\\mathrm{eq}})^3 + \\ldots we have deliberately excluded term linear in x_{\\mathrm{eq}} , as its inclusion would remove the minimum! In the usual way, we are only going to consider small displacements around x_{\\mathrm{eq}} and then throw all terms other than the constant and the quadratic in the bin, greatly simplifying our life. 3.1.1: With a potential of the form V(x) = V_0 + a x^2 , what will be the motion of an atom in the potential?","title":"Material properties from the interatomic potential"},{"location":"3-1d/3-1-vibrations/#compressibility","text":"With our potential approximated as a quadratic, let us consider what happens when we compress or stretch the system. Consider a material with length L ; if the equilibrium distance between atoms is x_{\\mathrm{eq}} , then there are N = L/x_{\\mathrm{eq}} atoms in the material. Now lets change the material to length L + \\delta L , which will lead to a change in the interatomic distance of \\delta x = \\delta L/N . With only small changes in x we can then write F = - \\frac{\\mathrm{d} V}{\\mathrm{d} x} \\Bigr|_{x = x_{\\mathrm{eq}} + \\delta x} = \\kappa a \\frac{\\delta L}{L}. This is a description of the compressibility (or elasticity) of a solid, and from a mechanics, the compressibility \\beta 1 is usually defined through the relation \\beta = -\\frac{1}{V} \\frac{\\partial V}{\\partial P} for a three dimensional system. In the one dimensional case, this simplifies to \\beta = -\\frac{1}{L} \\frac{\\partial L}{\\partial F}= \\frac{1}{\\kappa x_{\\mathrm{eq}}} \\equiv \\frac{1}{\\kappa a} where we have introduced a , the symbol used to denote the distance between identical particles, which in this case is our equilibrium interatomic spacing x_{\\mathrm{eq}}","title":"Compressibility"},{"location":"3-1d/3-1-vibrations/#sound-velocity","text":"The compressibility \\beta allows us to calculate the speed of sound by using the usual relation between the speed v and the density \\rho , and the bulk modulus B : v = \\sqrt{\\frac{B}{\\rho}} = \\sqrt{\\frac{1}{\\rho \\beta}} and because in one-dimension \\rho = m/a (in 1D), we can predict that v = \\sqrt{\\frac{\\kappa a^2}{m}}. Let us put a pin in this result, as we are about to construct a detailed microscopic model for a solid which will allow for calculation of v , it will be useful to compare the results.","title":"Sound velocity"},{"location":"3-1d/3-1-vibrations/#vibrations-in-one-dimension","text":"To emphasize the similarities and the differences between electrons and phonons, we will deal with both types of particles at once. Up to this point, we have seen the Boltzmann, Einstein, and Debye models of solids, which at the core are models of vibrations in a solid. In the case of Debye, the language around this was much more explicit; however, in all cases, we were considering the motion of the constituent atoms and how ultimately, this allowed for the storage of energy. Here we are going to construct an atomic-scale model of an atomic lattice with an attempt to better understand both the successes and failures of these early models of solids","title":"Vibrations in one dimension"},{"location":"3-1d/3-1-vibrations/#an-infinite-one-dimensional-chain-of-identical-atoms","text":"We begin by considering the amalgamation of the concepts ripped from physicists' bingo , namely: a low-dimensional model, and infinite model, a model stripped of unnecessary complexity, and a model centred around a harmonic potential. But in all seriousness, the model is incredibly rich and goes a long way to understanding solids! Consider a chain of identical atoms of mass m with equilibrium spacing a , connected to nearest-neighbours by springs with spring constant \\kappa . Hopefully it is clear from the discussion above, that by using a spring model, we are explicitly using a Harmonic potential which we assume well-approximates the inter-atomic potential for bound systems and small oscillation amplitudes, that is, low temperature. We define the position of the n^{\\textrm{th}} atom to be x_n and the equilibrium position of the n^{\\textrm{th}} atom to be x_{n, \\textrm{eq}}=na . We then write the displacement of an atom from its equilibrium position \\delta x_n = x_n - x_{n, \\textrm{eq}}. The potential energy for this harmonic chain is then the sum over all harmonic potentials \\begin{align} V & = \\sum_i V(x_{i+1}-x_i) = \\sum_i \\kappa/2 (x_{i+1}-x_i-a)^2 \\\\ & = \\sum_i \\kappa/2 (\\delta x_{i+1}- \\delta x_i)^2 \\end{align} from which we can write the force on atom n as F_n = - \\frac{\\partial V}{\\partial x_n} = \\kappa (\\delta x_{n+1}- \\delta x_n) + \\kappa (\\delta x_{n-1}- \\delta x_n) which can be expressed as a 2^{\\mathrm{nd}} -order ODE in \\delta x_n : m (\\ddot{\\delta x_n}) = \\kappa (\\delta x_{n+1} + \\delta x_{n-1} - 2\\delta x_n). With systems such as this, one is usually interested in the normal modes of the system: oscillations where all particles move with the same frequency. Given we are persuing vibrations in the systems this is a worthwhile thing in which to be interested, but moreover, there is a deep connection between normal modes of a classical system and the energy Eigenstates of the equivalent quantum system, so let's consider progress in this direction an down payment of sorts. Given that the structure of the equation of motion is the identical no matter what value of n we choose, and since these equations define the solutions, one can reason that the solutions should also be independent of the choice of n . This, combined with us actively looking for wave solutions, leads us to assume solutions in the form of plane waves, with the same amplitude for each atom: \\delta x_n = A e^{i \\omega t - i k x_{n, \\textrm{eq}}} = A e^{i \\omega t - i k n a} 3.1.2: Assuming plain-wave solutions, show that \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| It is worth stopping to emphasise that we now have a non-trivial dispersion relation \\omega = 2 \\sqrt{\\kappa/m} |\\sin(ka/2)| which in contrast to that we have seen previously in the Debye model, \\omega = v_s |k| , is very different. 3.1.3: What can one say about the relationship between the dispersion relationship for a 1D chain of oscillators and that of the Debye model?","title":"An infinite one-dimensional chain of identical atoms"},{"location":"3-1d/3-1-vibrations/#the-reciprocal-lattice","text":"Let's go ahead an plot the dispersion relation: where we can see that the dispersion relationship is periodic . This is clearly evident from the functional form of the relation, but just let that idea marinate for a minute: the dispersion, that is, the relationship between the oscillation frequency \\omega and k is periodic. What does that even mean? Well in the same way that we saw periodic boundary conditions resulting an a discretisation of k- space in the Debye model, a more general principle is that if a system is period in real space (that is, in position space) with a periodicity a , it will also be periodic in k- space with periodicity 2\\pi/a . Mathematically, it is clear why this happens, but what is the physical interpretation? Let's start by taking advantage of this periodicity by considering only the \"unit cell\" - the periodic unit in reciprocal ( k- ) space - which is called the Brillouin zone . A plot of the Brillouin zone for the dispersion curve above is shown below: As the dispersion is periodic with period 2\\pi/a , we need only look at the Brillouin zone to understand how a material behaver for all k . But how does this all work? For example, consider our oscillation modes \\delta x_n = A e^{i \\omega t - i k n a} 3.1.4: What happens to a given mode of oscillation under the transformation k \\rightarrow k + 2\\pi/a ? Given the periodicity of k space, we can think about the set of points which are equivalent to the point k=0 ( k = \\pm n \\times 2\\pi/a ), which is called the reciprocal lattice , in contrast to the real-space lattice x_n = n a . A useful observation for all points G_m in the reciprocal lattice and points x_n in the real-space lattice is that e^{i G_m x_n} = 1","title":"The reciprocal lattice"},{"location":"3-1d/3-1-vibrations/#but-what-is-actually-going-on","text":"If we think about life outside of periodic systems, for example, the propagation of light in a vacuum, one of our favourite relationships is c = \\frac{\\omega}{k} So what does it mean to talk about a (phase) velocity when k is periodic? And what about the wavelength? We are pretty fond on the relation \\lambda = \\frac{2\\pi}{k}, but if k and k + n \\times 2\\pi/a are equivalent, what is heck is going on? 3.1.5: What the heck is going on?","title":"BUT WHAT IS ACTUALLY GOING ON?!?!"},{"location":"3-1d/3-1-vibrations/#counting-normal-modes","text":"As is (hopefully) becoming a normal question to ask when we consider modes of oscillation: how many modes are there? Well let us consider a chain of N masses, and once again we are going to assume periodic boundary conditions, with the same justification as last time, namely that we are interested in bulk quantities and we will make sure that we don't take look at things happening near boundaries. In a periodic system, we can consider the chain wrapped around back onto itself, such that e^{i\\omega t - ikna} = e^{i\\omega t - ik(N+n)a} which enforces the relation e^{ikNa} = 1 and subsequently restricts the possible values of k : k = \\frac{2\\pi q}{N a} = \\frac{2\\pi q}{L} \\textrm{ where } q \\in \\mathbb{Z}. This is exactly as we saw with the Debye model , where the imposition of periodic boundary conditions discretised reciprocal space. There we saw that points were equally spaced with a separation of 2\\pi/L , which in this case is equivalent to 2\\pi/Na . Now, to count all the modes, we can simply compute the ration of the possible values of k by the spacing between modes. As discussed above, we need only consider the Brillouin zone, as the modes are periodic in k with a period of 2\\pi/a , therefore the number of modes is \\frac{2\\pi/a}{2\\pi/Na} = N which again, is what Debye had predicted - although he just plucked the result from the air - whereas we now have a grounding to say that there is one normal mode per mass in the system.","title":"Counting normal modes"},{"location":"3-1d/3-1-vibrations/#quantum-vibrations","text":"Our discussion up to this point has been purely classical, but thanks to firm foundations, mapping our classical system onto the equivalent quantum system is not so difficult. Explicitly, classical harmonic systems - and thus normal modes - map directly onto \"equivalent\" quantum systems, and a normal mode of oscillation at frequency \\omega will have eigenstates with energy E_n = \\hbar \\omega \\left(n + \\frac{1}{2}\\right). This means that for a given mode with wavevector k , there are multiple energy eigenstates, each separated in energy by \\hbar\\omega(k) . We can now start to think of excitations (and de-excitations) of this particular mode, a quantum of energy \\hbar\\omega(k) , which are known as phonons . Phonons are much like photons - especially in the under the formalism of second quantisation - but obviously are not quanta of the electromagnetic field, but rather the oscillation modes of a solid. In the same way phonons can occupy the same state - i.e. they are bosons - phonons are also bosons, so we can describe the occupation of a given mode though the Bose factor n_\\mathrm{B} . Therefore we can write an expression for the energy associated with the wavevector k as E_k = \\hbar\\omega(k)\\left(n_\\mathrm{B}(\\beta\\hbar\\omega(k))+\\frac{1}{2}\\right) 3.1.6: Use the energy E_k to get obtain an expression for the total energy in the system in terms of an integral over k With this, we now have a well-formulated quantum system from which we can calculate quantities of interest, but also extend to model systems that are not just infinite chains of identical particles.","title":"\"Quantum vibrations\""},{"location":"3-1d/3-1-vibrations/#conclusions","text":"The interatomic potential describes the compressibility of a material A system which is periodic in real space with period a in periodic in reciprocal space with period 2\\pi/a The Brillouin zone contains all values of k , as modes separated by 2\\pi/a are identical A normal modes of frequency \\omega is mapped to the eigenstate with energy $E_n = \\hbar \\omega \\left( n + 1/2 \\right)","title":"Conclusions"},{"location":"3-1d/3-1-vibrations/#exercises","text":"","title":"Exercises"},{"location":"3-1d/3-1-vibrations/#preliminary-provocations","text":"What is the motion of adjacent masses when the chain is oscillating at the its maximum frequency?","title":"Preliminary provocations"},{"location":"3-1d/3-1-vibrations/#exercise-1-lennard-jones-potential","text":"A simple model approximating the interaction between a pair of noble gas atoms such as Argon is the Lennard-Jones potential , in which the potential energy as a function of interatomic distance is U(r) = 4\\epsilon \\left[\\big(\\frac{\\sigma}{r}\\big)^{12}-\\big(\\frac{\\sigma}{r}\\big)^6\\right] where r is the distance between two atoms, \\epsilon is the depth of the potential well, and \\sigma is the distance at which the inter-particle potential is zero. Sketch U(r) as a function of interatomic distance and mark the regions of repulsive and attractive forces acting between the atoms. Find the distance, r_0 (bond length), at which the potential energy is minimal and find the value of the potential energy at this distance (binding energy of the molecule). Expand U(r) in a Taylor series around r_0 up to second order. By considering a second-order (=harmonic) potential approximation around the minimum ( r_0 ), find an expression for the spring constant, \\kappa , in terms of \\epsilon and \\sigma . Using the spring constant \\kappa you found earlier, find the ground state energy of the molecule by comparing the molecule to a quantum harmonic oscillator. What is the energy required to break the molecule apart? What is the approximate number of phonons that can occupy this mode before the potential becomes anharmonic? Hint Because the diatomic molecule is modelled as a one-body problem (in the center of mass rest frame of the molecule), the mass should be replaced by the reduced mass .","title":"Exercise 1: Lennard-Jones potential"},{"location":"3-1d/3-1-vibrations/#exercise-2-vibrational-heat-capacity-of-a-1d-monatomic-chain","text":"Give an integral expression for the heat capacity C . Compute the heat capacity numerically, using e.g. Python. Do the same for C in the Debye model and compare the two. What differences do you see?","title":"Exercise 2: Vibrational heat capacity of a 1D monatomic chain"},{"location":"3-1d/3-1-vibrations/#exercise-3-a-finite-chain","text":"Consider a chain of only 3 atoms. We can then write the equations of motion \\begin{aligned} m \\ddot{\\delta x}_1 &= - \\kappa (\\delta x_1 - \\delta x_2) \\\\ m \\ddot{\\delta x}_2 &= - \\kappa (\\delta x_2 - \\delta x_1) - \\kappa (\\delta x_2 - \\delta x_3) \\\\ m \\ddot{\\delta x}_3 &= - \\kappa (\\delta x_3 - \\delta x_2) \\end{aligned}, and write this system of equations in matrix form m \\ddot{\\mathbf{u}} = -\\kappa \\begin{pmatrix} 1 & -1 & 0\\\\ -1 & 2 & -1\\\\ 0 & -1 & 1 \\end{pmatrix}\\mathbf{u} Hint In Python use the function numpy.diag Define a matrix that relates forces to displacements in a linear 1D chain containing N=5 atoms. Repeat for N=200 . You may assume that the masses and spring constants are equivalent throughout the chain. Using numerical diagonalization ( numpy.linalg.eigvalsh ), compute the eigenfrequencies of this atomic chain. Plot a histogram of these eigenfrequencies. Make the masses of every even atom different from the masses of every odd atom. Compute the eigenfrequencies of this atomic chain and plot a histogram. Now make the masses of even and odd atoms equivalent again. Furthermore, make the spring constants of every even spring different from the odd spring. Compute the eigenfrequencies of this atomic chain and plot a histogram. It is unfortunately that there are only so many letters in the Greek alphabet, especially when looking at mechanical and thermodynamic quantities! \u21a9","title":"Exercise 3: A finite chain"},{"location":"3-1d/3-2-diatomic/","text":"The diatomic chain \u00b6 Vibrations in one-dimension with extra pizzazz. Introduction \u00b6 Modelling a solid as an infinite one-dimensional chain of identical atoms went a long way to Accurately predicting the observed behaviour of solids whilst both incorporating ideas from previous models, and reconciling many of their shortcomings Providing a microscopic picture of what is going on, both in a classical and quantum mechanical sense Unfortunately for us, not everything is an infinite 1D chain of the same atom. In order to make our model more realistic and more broadly applicable, we are going to make some changes: let's look at an infinite one-dimensional chain of identical pairs of atoms. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: Eigenvalue equations Text reference The material covered here is discussed in section(s) \\S 10 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here Expanding the circle of concern \u00b6 In the previous , we modelled vibrations in a solid using a one-dimensional homogeneous chain of atoms. The model gave us insight into the vibrational modes of a solid, and in the quantum mechanical case precipitated the identification of phonons, quanta of vibrations. To expand our model to include more solids, we shall consider a chain of atoms with two distinct particles, but the method we use here is applicable to the inclusion of more particles and interactions. In the case of a diatomic system, the masses of each of the constituent atoms can be different ( m_1 and m_2 ), but as can the harmonic potential between the atoms ( \\kappa_1 and \\kappa_2 ). Following the text, we are going to consider the case of identical masses but varying spring constants as the algebra is ever so slightly simpler, and the qualitative behaviour we are trying to observe is present in all cases. A schematic of the system in shown below: The first observation to make is that the repeated pattern, that is, the unit cell is not longer just one atom, but rather to length which characterises the periodicity is now 2a . We note that within this unit cell, there two degrees of freedom, namely the position of particle x and particle y , which are distinguished by their relative location to harmonic potentials with force constants \\kappa_1 and \\kappa_2 as illustrated above. This is in contrast to the monatomic chain model, whereby there was one degree of freedom per unit cell. Equations of motion \u00b6 Following a similar path as the monatomic case, we begin be writing the equations of motion: \\begin{aligned} m (\\ddot{\\delta x_n}) & = \\kappa_2 (\\delta y_{n} - \\delta x_{n}) + \\kappa_1 (\\delta y_{n-1} - \\delta x_{n}) \\\\ m (\\ddot{\\delta y_n}) & = \\kappa_1 (\\delta x_{n+1} - \\delta y_{n}) + \\kappa_2 (\\delta x_{n} - \\delta y_{n}) \\end{aligned} and employing the same intuition as last time, namely that we expect wave solutions that should have unit cells oscillating at the same frequency, we seek solutions of the form: \\begin{pmatrix} \\delta x_n\\\\ \\delta y_n \\end{pmatrix} = e^{i\\omega t - ik na} \\begin{pmatrix} A_{x}\\\\ A_{y} \\end{pmatrix}. When these solutions are deployed, one obtains the matrix equation m\\omega^2\\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} = \\begin{pmatrix} \\kappa_1 + \\kappa_2 & -\\kappa_2 - \\kappa_1 e^{ika}\\\\ -\\kappa_2 - \\kappa_1 e^{-ika} & \\kappa_1 + \\kappa_2 \\end{pmatrix} \\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} 3.2.1: Explicitly verify that the trail solutions lead to the above matrix equation This is an eigenvalue equation, and this can be solved in the usual way to obtain \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} 3.2.2: Solve the eigenvalue equation and verify the result for \\omega_\\pm Dispersion \u00b6 Immediately we notice that as compared to the monatomic chain, there are now two modes of oscillation for each value of k , and we denote the frequencies associated with the modes \\omega_{\\pm} , as shown in the plot below: Looking at the associated eigenvectors, we can see that these frequencies correspond to in-phase ( \\omega_- ) and out-of-phase ( \\omega_+ ) motion. Just like last time, the plot is periodic in k with values of k shifted by 2 \\pi / a corresponding to the same solution, and therefore look only at the unique values of k , that is, those in the Brillouin zone, shown below: One may ask why the second branch appears, which is an good question, and is understood as with a diatomic system, we have added a second degree of freedom per unit cell. Indeed, had we a unit cell with 3 atoms, that is 3 degrees of freedom, we would find that there are three branches, or three frequencies, for each value of k . Looking at the \\omega_- mode, we notice that as k \\rightarrow 0 , the dispersion relationship is linear. In this regime, the modes behave like sound waves (i.e. \\omega = v |k| ) and consequently, the mode is referred to as the acoustic mode . On the other hand, modes which intersect with optical dispersion modes (i.e. \\omega = c|k| ) are referred to as optical modes . 3.2.3: Plot the dispersion modes, along with an optical dispersion curve for visible light. What are the implications intersecting curves? We can look at the behaviour of the curves at specific values of k , for example at the zone boundary, \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_1/m} and \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_2/m} . At this point, it is useful to consider the idea of an extended Brillouin zone: the Brillouin zone houses all unique values of k , but in this case, there are multiple frequencies for each value of k . By extending the Brillouin zone, unfolding the higher-order mode(s) out to a the neighbouring unit cell in reciprocal space, we can obtain a plot which was unique frequencies for each k . A plot illustrating the extended scheme is shown below: But why would we do this? Well, consider the case of \\kappa_1 \\approx \\kappa_2 and the limit of \\kappa_1 = \\kappa_2 . We can clearly observe an \"opening\" between the bands which depends on the values of \\kappa_1 and \\kappa_2 , but physically, what would one expect to happen when \\kappa_1 = \\kappa_2 = \\kappa ? 3.2.4: Make a prediction for what will be the consequences of setting \\kappa_1 = \\kappa_2 and how this relates to the monatomic chain. Hopefully it is clear that with identical spring constants, we are back at the case of a monatomic chain. In the case of the band splitting, this will go to zero as the difference between \\kappa_1 and \\kappa_2 goes to zero: But now we have a bit of a quandary, namely, in order to capture all unique values of k , we now have a plot Brillouin zone that runs from -2\\pi/a to 2\\pi/a (see above), whereas previously this ran from -\\pi/a to \\pi/a . Any what about our number of states? The solution comes in the recognition that in the diatomic case, one has a unit cell length of a , which is 2 \\times a_\\mathrm{monatomic} , so the \"true\" Brillouin zone for the monatomic chain extents from -\\pi/a_\\mathrm{monatomic} to \\pi/a_\\mathrm{monatomic} which is equivalent to -2\\pi/a to 2\\pi/a . Similarly, the number of atoms per unit cell as changed from 2 to 1, meaning that everything doesn't fall apart. YAY! Density of states \u00b6 As we have seen previously, the density of states is g(\\omega)\\textrm{d}\\omega = \\textrm{d}N and thus g(\\omega) = \\frac{\\textrm{d}N}{\\textrm{d}\\omega} = \\frac{L}{2\\pi} \\sum | \\textrm{d}k/\\textrm{d} \\omega| where sum goes over all states at a given energy. In this case, we must ensure that we include the contribution to the DoS from both the positive and negative momenta. Since the energy of this system is symmetric with respect to k , this sum will simply introduce a factor of 2. In an attempt to build an intuition for visualising and understanding the density of states, it is possible to think of the DoS as a histogram of the energy samples drawn from the dispersion relation \u03c9(k) . Remember that by enforcing periodic boundary conditions on our system, we discretise k space with equally spaced points separated by distance 2\\pi/L . Conclusions \u00b6 The normal modes of a diatomic chain of atoms were found by once again intuiting that plane waves in real would be a well-suited solution Systems with more than one degree of freedom per unit cell result in independent oscillation amplitudes for each degree of freedom, leading to multiple bands The density of states can be derived graphically from the dispersion relation Exercises \u00b6 Preliminary provocations \u00b6 Verify that the expression for \\omega^2 is always positive. Why is this important? When calculating the DOS, we only look at the first Brillouin zone. Why? Exercise 1: analysing the diatomic vibrating chain \u00b6 As we have shown, the normal modes of oscillation of a diatomic chain occur at frequencies: \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} where the plus sign corresponds to the optical branch and the minus sign to the acoustic branch. Hint The final form of \\omega_\\pm as given is not always the most useful: sometimes the complex exponential form - see 3.2.2 - can make life easier Find the magnitude of the group velocity near k=0 for the acoustic branch. Show that the group velocity at k=0 for the optical branch is zero. Derive an expression of the density of states g(\u03c9) for the acoustic branch and small k . Make use of your expression of the group velocity in 1. Compare this expression with that of the derived density of states from exercise 1 of the Debye lecture. Exercise 2: atomic chain with 3 different spring constants \u00b6 Suppose we have a vibrating 1D atomic chain with 3 different spring constants alternating like \\kappa_ 1 , \\kappa_2 , \\kappa_3 , \\kappa_1 , etc. All the atoms in the chain have an equal mass m . Hint To solve the eigenvalue problem quickly, make use of the fact that the mass-spring matrix in that case commutes with the matrix X = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}. What can be said about eigenvectors of two matrices that commute? Make a sketch of this chain and indicate the length of the unit cell a in this sketch. Derive the equations of motion for this chain. By filling in the trial solutions into the equations of motion (which should be similar to those used in the dual spring constants case), show that the eigenvalue problem is \\omega^2 \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} = \\frac{1}{m} \\begin{pmatrix} \\kappa_1 + \\kappa_ 3 & -\\kappa_ 1 & -\\kappa_ 3 e^{i k a} \\\\ -\\kappa_ 1 & \\kappa_1+\\kappa_2 & -\\kappa_ 2 \\\\ -\\kappa_ 3 e^{-i k a} & -\\kappa_2 & \\kappa_2 + \\kappa_ 3 \\end{pmatrix} \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} In general, the eigenvalue problem above cannot be solved analytically, and can only be solved in specific cases. Find the eigenvalues \u03c9^2 when k a = \\pi and \\kappa_1 = \u03ba_2 = q . What will happen to the periodicity of the band structure if \\kappa_ 1 = \\kappa_ 2 = \\kappa_3 ?","title":"3.2: The diatomic chain"},{"location":"3-1d/3-2-diatomic/#the-diatomic-chain","text":"Vibrations in one-dimension with extra pizzazz.","title":"The diatomic chain"},{"location":"3-1d/3-2-diatomic/#introduction","text":"Modelling a solid as an infinite one-dimensional chain of identical atoms went a long way to Accurately predicting the observed behaviour of solids whilst both incorporating ideas from previous models, and reconciling many of their shortcomings Providing a microscopic picture of what is going on, both in a classical and quantum mechanical sense Unfortunately for us, not everything is an infinite 1D chain of the same atom. In order to make our model more realistic and more broadly applicable, we are going to make some changes: let's look at an infinite one-dimensional chain of identical pairs of atoms. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Mathematics: Eigenvalue equations Text reference The material covered here is discussed in section(s) \\S 10 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: A program used to visualise oscillations in one dimension ( chainplot as written by Mike Glazer ) can be downloaded here","title":"Introduction"},{"location":"3-1d/3-2-diatomic/#expanding-the-circle-of-concern","text":"In the previous , we modelled vibrations in a solid using a one-dimensional homogeneous chain of atoms. The model gave us insight into the vibrational modes of a solid, and in the quantum mechanical case precipitated the identification of phonons, quanta of vibrations. To expand our model to include more solids, we shall consider a chain of atoms with two distinct particles, but the method we use here is applicable to the inclusion of more particles and interactions. In the case of a diatomic system, the masses of each of the constituent atoms can be different ( m_1 and m_2 ), but as can the harmonic potential between the atoms ( \\kappa_1 and \\kappa_2 ). Following the text, we are going to consider the case of identical masses but varying spring constants as the algebra is ever so slightly simpler, and the qualitative behaviour we are trying to observe is present in all cases. A schematic of the system in shown below: The first observation to make is that the repeated pattern, that is, the unit cell is not longer just one atom, but rather to length which characterises the periodicity is now 2a . We note that within this unit cell, there two degrees of freedom, namely the position of particle x and particle y , which are distinguished by their relative location to harmonic potentials with force constants \\kappa_1 and \\kappa_2 as illustrated above. This is in contrast to the monatomic chain model, whereby there was one degree of freedom per unit cell.","title":"Expanding the circle of concern"},{"location":"3-1d/3-2-diatomic/#equations-of-motion","text":"Following a similar path as the monatomic case, we begin be writing the equations of motion: \\begin{aligned} m (\\ddot{\\delta x_n}) & = \\kappa_2 (\\delta y_{n} - \\delta x_{n}) + \\kappa_1 (\\delta y_{n-1} - \\delta x_{n}) \\\\ m (\\ddot{\\delta y_n}) & = \\kappa_1 (\\delta x_{n+1} - \\delta y_{n}) + \\kappa_2 (\\delta x_{n} - \\delta y_{n}) \\end{aligned} and employing the same intuition as last time, namely that we expect wave solutions that should have unit cells oscillating at the same frequency, we seek solutions of the form: \\begin{pmatrix} \\delta x_n\\\\ \\delta y_n \\end{pmatrix} = e^{i\\omega t - ik na} \\begin{pmatrix} A_{x}\\\\ A_{y} \\end{pmatrix}. When these solutions are deployed, one obtains the matrix equation m\\omega^2\\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} = \\begin{pmatrix} \\kappa_1 + \\kappa_2 & -\\kappa_2 - \\kappa_1 e^{ika}\\\\ -\\kappa_2 - \\kappa_1 e^{-ika} & \\kappa_1 + \\kappa_2 \\end{pmatrix} \\begin{pmatrix} A_x\\\\ A_y \\end{pmatrix} 3.2.1: Explicitly verify that the trail solutions lead to the above matrix equation This is an eigenvalue equation, and this can be solved in the usual way to obtain \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} 3.2.2: Solve the eigenvalue equation and verify the result for \\omega_\\pm","title":"Equations of motion"},{"location":"3-1d/3-2-diatomic/#dispersion","text":"Immediately we notice that as compared to the monatomic chain, there are now two modes of oscillation for each value of k , and we denote the frequencies associated with the modes \\omega_{\\pm} , as shown in the plot below: Looking at the associated eigenvectors, we can see that these frequencies correspond to in-phase ( \\omega_- ) and out-of-phase ( \\omega_+ ) motion. Just like last time, the plot is periodic in k with values of k shifted by 2 \\pi / a corresponding to the same solution, and therefore look only at the unique values of k , that is, those in the Brillouin zone, shown below: One may ask why the second branch appears, which is an good question, and is understood as with a diatomic system, we have added a second degree of freedom per unit cell. Indeed, had we a unit cell with 3 atoms, that is 3 degrees of freedom, we would find that there are three branches, or three frequencies, for each value of k . Looking at the \\omega_- mode, we notice that as k \\rightarrow 0 , the dispersion relationship is linear. In this regime, the modes behave like sound waves (i.e. \\omega = v |k| ) and consequently, the mode is referred to as the acoustic mode . On the other hand, modes which intersect with optical dispersion modes (i.e. \\omega = c|k| ) are referred to as optical modes . 3.2.3: Plot the dispersion modes, along with an optical dispersion curve for visible light. What are the implications intersecting curves? We can look at the behaviour of the curves at specific values of k , for example at the zone boundary, \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_1/m} and \\omega_{+}(\\pi/a) = \\sqrt{2\\kappa_2/m} . At this point, it is useful to consider the idea of an extended Brillouin zone: the Brillouin zone houses all unique values of k , but in this case, there are multiple frequencies for each value of k . By extending the Brillouin zone, unfolding the higher-order mode(s) out to a the neighbouring unit cell in reciprocal space, we can obtain a plot which was unique frequencies for each k . A plot illustrating the extended scheme is shown below: But why would we do this? Well, consider the case of \\kappa_1 \\approx \\kappa_2 and the limit of \\kappa_1 = \\kappa_2 . We can clearly observe an \"opening\" between the bands which depends on the values of \\kappa_1 and \\kappa_2 , but physically, what would one expect to happen when \\kappa_1 = \\kappa_2 = \\kappa ? 3.2.4: Make a prediction for what will be the consequences of setting \\kappa_1 = \\kappa_2 and how this relates to the monatomic chain. Hopefully it is clear that with identical spring constants, we are back at the case of a monatomic chain. In the case of the band splitting, this will go to zero as the difference between \\kappa_1 and \\kappa_2 goes to zero: But now we have a bit of a quandary, namely, in order to capture all unique values of k , we now have a plot Brillouin zone that runs from -2\\pi/a to 2\\pi/a (see above), whereas previously this ran from -\\pi/a to \\pi/a . Any what about our number of states? The solution comes in the recognition that in the diatomic case, one has a unit cell length of a , which is 2 \\times a_\\mathrm{monatomic} , so the \"true\" Brillouin zone for the monatomic chain extents from -\\pi/a_\\mathrm{monatomic} to \\pi/a_\\mathrm{monatomic} which is equivalent to -2\\pi/a to 2\\pi/a . Similarly, the number of atoms per unit cell as changed from 2 to 1, meaning that everything doesn't fall apart. YAY!","title":"Dispersion"},{"location":"3-1d/3-2-diatomic/#density-of-states","text":"As we have seen previously, the density of states is g(\\omega)\\textrm{d}\\omega = \\textrm{d}N and thus g(\\omega) = \\frac{\\textrm{d}N}{\\textrm{d}\\omega} = \\frac{L}{2\\pi} \\sum | \\textrm{d}k/\\textrm{d} \\omega| where sum goes over all states at a given energy. In this case, we must ensure that we include the contribution to the DoS from both the positive and negative momenta. Since the energy of this system is symmetric with respect to k , this sum will simply introduce a factor of 2. In an attempt to build an intuition for visualising and understanding the density of states, it is possible to think of the DoS as a histogram of the energy samples drawn from the dispersion relation \u03c9(k) . Remember that by enforcing periodic boundary conditions on our system, we discretise k space with equally spaced points separated by distance 2\\pi/L .","title":"Density of states"},{"location":"3-1d/3-2-diatomic/#conclusions","text":"The normal modes of a diatomic chain of atoms were found by once again intuiting that plane waves in real would be a well-suited solution Systems with more than one degree of freedom per unit cell result in independent oscillation amplitudes for each degree of freedom, leading to multiple bands The density of states can be derived graphically from the dispersion relation","title":"Conclusions"},{"location":"3-1d/3-2-diatomic/#exercises","text":"","title":"Exercises"},{"location":"3-1d/3-2-diatomic/#preliminary-provocations","text":"Verify that the expression for \\omega^2 is always positive. Why is this important? When calculating the DOS, we only look at the first Brillouin zone. Why?","title":"Preliminary provocations"},{"location":"3-1d/3-2-diatomic/#exercise-1-analysing-the-diatomic-vibrating-chain","text":"As we have shown, the normal modes of oscillation of a diatomic chain occur at frequencies: \\omega_\\pm^2 = \\frac{\\kappa_1+\\kappa_2}{m} \\pm \\sqrt{\\frac{(\\kappa_1+\\kappa_2)^2 - 4\\kappa_1\\kappa_2\\sin^2(ka/2)}{m^2}} where the plus sign corresponds to the optical branch and the minus sign to the acoustic branch. Hint The final form of \\omega_\\pm as given is not always the most useful: sometimes the complex exponential form - see 3.2.2 - can make life easier Find the magnitude of the group velocity near k=0 for the acoustic branch. Show that the group velocity at k=0 for the optical branch is zero. Derive an expression of the density of states g(\u03c9) for the acoustic branch and small k . Make use of your expression of the group velocity in 1. Compare this expression with that of the derived density of states from exercise 1 of the Debye lecture.","title":"Exercise 1: analysing the diatomic vibrating chain"},{"location":"3-1d/3-2-diatomic/#exercise-2-atomic-chain-with-3-different-spring-constants","text":"Suppose we have a vibrating 1D atomic chain with 3 different spring constants alternating like \\kappa_ 1 , \\kappa_2 , \\kappa_3 , \\kappa_1 , etc. All the atoms in the chain have an equal mass m . Hint To solve the eigenvalue problem quickly, make use of the fact that the mass-spring matrix in that case commutes with the matrix X = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix}. What can be said about eigenvectors of two matrices that commute? Make a sketch of this chain and indicate the length of the unit cell a in this sketch. Derive the equations of motion for this chain. By filling in the trial solutions into the equations of motion (which should be similar to those used in the dual spring constants case), show that the eigenvalue problem is \\omega^2 \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} = \\frac{1}{m} \\begin{pmatrix} \\kappa_1 + \\kappa_ 3 & -\\kappa_ 1 & -\\kappa_ 3 e^{i k a} \\\\ -\\kappa_ 1 & \\kappa_1+\\kappa_2 & -\\kappa_ 2 \\\\ -\\kappa_ 3 e^{-i k a} & -\\kappa_2 & \\kappa_2 + \\kappa_ 3 \\end{pmatrix} \\begin{pmatrix} A_1 \\\\ A_2 \\\\ A_3 \\end{pmatrix} In general, the eigenvalue problem above cannot be solved analytically, and can only be solved in specific cases. Find the eigenvalues \u03c9^2 when k a = \\pi and \\kappa_1 = \u03ba_2 = q . What will happen to the periodicity of the band structure if \\kappa_ 1 = \\kappa_ 2 = \\kappa_3 ?","title":"Exercise 2: atomic chain with 3 different spring constants"},{"location":"3-1d/3-3-tightbinding/","text":"The tight binding model \u00b6 One-dimensional chains have been doing well, so why stop? Introduction \u00b6 The model of Sommerfeld saw the Fermionic nature of electrons cemented in Drude's kinetic theory of electrons, but as we saw in our discussion of chemistry , electrons tend not to be free, but rather occupy states as governed by surrounding nuclei. It is from this point that we aim to marry our discussion of bonding theory in the LCAO framework with our recent progress on modelling solids in one dimension. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Solid-state physics: 1D model of a solid, LCAO Text reference The material covered here is discussed in section(s) \\S 11 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below: Tight binding \u00b6 In our discussion of covalent bonding, we were able to obtain a solution to the Schr\u00f6dinger equation using the variational method in the framework known as LCAO . We are going to apply the same framework to an infinite chain of atoms, but before we do this, we are going to consider a gentle extension to the simple 2 atom case that we originally considered. A triatomic molecule \u00b6 Let us consider a one-dimensional triatomic system as illustrated below: We are going to make the exact same assumptions that we made when we formulated LCAO, namely we are looking at a frozen molecule (no vibrations) with fixed nuclear positions (the Born-Oppenheimer approximation). We assume that the atoms are identical, and the electrons are tightly bound to a given nucleus so that we have (\\hat{K} + \\hat{V}_n)|n\\rangle = \\epsilon_\\textrm{atomic} |n\\rangle where \\hat{K} is the usual kinetic energy operator, \\hat{V}_n is the potential due to the nucleus n and \\epsilon is the energy of the bound electronic state. We again make the crude approximate that our orbitals are orthogonal 1 , such that \\langle i|j\\rangle = \\delta_{i,j}. Now, we can do exactly what we did for a diatomic molecule, namely, consider the trail wavefunction: \\vert \\psi \\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. By performing the exact same logic and mathematics we arrive at the matrix equation E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} \\epsilon_0 & -t & 0 \\\\ -t & \\epsilon_0 & -t \\\\ 0 & -t & \\epsilon_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix} where \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 with V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle the energy shift due to the presence of all other nuclei. It is assumed that hopping occurs only between the nearest neighbours such that \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t. 3.3.1: Apply the exact same logic and mathematics to arrive at the above matrix equation Obtaining solutions to such a problem can be unwieldy, but fortunately, we have the perfect tool for the job. Number cruncher \u00b6 We can use our computational toolkit to solve for the eigenvalues of a matrix. For example, with \\epsilon_0 = 2 and t=1 , we find that the eigenvalues for the matrix above are approximately (0.59, 2.00, 3.41) . But what use is this? Well, let's look at a simple case of varying t . 3.3.2: Make a plot of the energy eigenvalues as a function of t running from (0, \\epsilon) and interpret your result. Ensure to include your code. Perhaps the most useful thing to do is similar to that which was introduced in the last section , which is using to produce a histogram sampling of the energy eigenvalues in order to visualise the density of states. If we go ahead an do this: The density of states is not a particularly useful concept with few states, but fortunately, this system scales well and \"grows diagonally\" with an increased number of atoms: as the hopping is limited to nearest neighbours and states are orthogonal, only the diagonal and first-off diagonal elements will be non-zero. So let's go ahead and crank the handle! To infinity and beyond n = 3 n = 10 n = 100 n = 1000 n = 10000 n = 100000 LOL: good luck! That would be one big matrix, much large than my meagre computer can handle. 3.3.3: How do these results compare with what we have seen previously? An infinite chain \u00b6 The above extension of the diatomic system to a triatomic system, especially when coupled with some computational assistance to hone our intuition, we are now going to consider the case of an infinite number of atoms, as depicted below: The potential for the above system, assuming that the locations of atoms are x_n = na with n \\in \\mathbb{Z} , is then: Now we can redeploy the machinery that we used for the diatomic and triatomic systems, namely we formulate the molecular orbital via the LCAO model: \\vert \\Psi \\rangle = \\sum_n \\phi_n |n \\rangle and write the effective Schr\u00f6dinger as \\sum_m H_{nm} \\phi_m = E \\phi_n where the matrix elements of the Hamiltonian are given by H_{nm} = \\langle n | \\hat{H} | m \\rangle It is a bit of work, but one can show that these matrix elements evaluate to H_{nm} = \\epsilon_0 \\delta_{n,m} - t \\left(\\delta_{n+1,m} + \\delta_{n-1,m}\\right) with the same definitions for \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 and V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle as in the triatomic case. Solving the tight-binding chain \u00b6 It will perhaps come as little surprise that the method that we are going to use to solve our effective Schr\u00f6dinger equation is similar to the method we have use to solve for vibrations in a chain, namely to look for plane-wave solutions: \\phi_n = \\frac{e^{-ikna}}{\\sqrt{N}} 3.3.4: How does this form of solution compare to the assumed form of solutions for oscillations? We then crank the handle: \\begin{align} E \\phi_n & = E \\times \\frac{e^{-ikna}}{\\sqrt{N}} \\\\ & = \\sum_m H_{nm} \\phi_m = \\epsilon_0 \\frac{e^{-ikna}}{\\sqrt{N}} - t \\left(\\frac{e^{-ik(n+1)a}}{\\sqrt{N}} - \\frac{e^{-ik(n-1)a}}{\\sqrt{N}}\\right) \\end{align} which yields the energy eigenvalue E = \\epsilon_0 - t \\left(e^{-ika} + e^{ika}\\right) = \\epsilon_0 - 2t\\cos(ka) which is plotted below: There are obvious similarities to the dispersion of oscillations, or phonons, but there are stark differences as compared to the Sommerfeld free electron model: now only a range of energies can be occupied, and this range is referred to as an energy band, and the energy difference between the top and the bottom of this band is called the bandwidth 2 . Further considering the band structure as compared to the free electron model, let us focus on the dispersion relation close to the base of the band at k=0 , where we approximate the energy as E \\approx \\epsilon_0 - 2t + t (ka)^2 \\equiv E_0 + t (ka)^2. If we compare this to the free-electron dispersion relation E=\\frac{\\hbar^2 k^2}{2m} we see that the band structure is similar, but with the lowest available energy is E_0 instead of 0 , and the electrons behave as if they had a different effective mass m^*=\\frac{\\hbar^2}{2ta^2}. For the moment, we leave this as an observation, but we shall return to it later in the course. Filling bands \u00b6 Let us now consider a system of N atoms. Due to the spin degeneracy, at each value k there are 2 possible states which can be occupied, which means for the system there are 2N possible states. The implications of this are significant. Band filling Divalent system Let us consider an atom with two valance electrons: there will be as many electrons as states and thus the every state in the band will be occupied, as illustrated below: Monovalent system Now let us consider monovalent atoms: there will be twice as many states as there are electrons, and thus the band will only be half filled: Note that there is now a Fermi surface - well, Fermi points. Monovalent system with electric field A Fermi surface means that nearby electrons can occupy neighbouring states, for example, under the influence of an electric field: 3.3.5: What is the implication of the plots shown in the band filling content block? Conclusions \u00b6 The density of states can be effectively visualised by sampling the dispersion relation and producing a histogram. Moreover, numerical tools are useful, especially when dealing with systems of many particles! Solving Schr\u00f6dinger's equation in the tight-binding framework using LCOA is surprisingly similar to a one-dimensional harmonic chain The tight binding model gives rise to electron bands At the bottom of bands, electrons have a similar dispersion relation to free electrons, but with altered mass Exercises \u00b6 Preliminary provocations \u00b6 Compare the expression of the effective mass with Newton's second law. Do you observe any similarities? Check the units of the effective mass. Is it what you expect? Calculate the effective mass of the free-electron dispersion relation. Is this what was expected? Under what condition is the effective mass the same for each electron? Exercise 1: The next-nearest neighbour chain \u00b6 Let's expand our one-dimensional chain model by extending the range of interaction to include the next-nearest neighbours: \\langle \\phi_n | H | \\phi_{n+2}\\rangle \\equiv -t' \u2260 0. Write down the new Schr\u00f6dinger equation for this system. Solve the Schr\u00f6dinger equation to find the dispersion relation E(k) . Calculate the effective mass m^* . Sketch the effective mass as a function of k for the cases t=2t' , t=4t' and t=10t' . This is not necessary but makes the calculation much simpler, and the qualitative behaviour reproduces that in which we are interested \u21a9 Original, hey? \u21a9","title":"3.3: The tight binding model"},{"location":"3-1d/3-3-tightbinding/#the-tight-binding-model","text":"One-dimensional chains have been doing well, so why stop?","title":"The tight binding model"},{"location":"3-1d/3-3-tightbinding/#introduction","text":"The model of Sommerfeld saw the Fermionic nature of electrons cemented in Drude's kinetic theory of electrons, but as we saw in our discussion of chemistry , electrons tend not to be free, but rather occupy states as governed by surrounding nuclei. It is from this point that we aim to marry our discussion of bonding theory in the LCAO framework with our recent progress on modelling solids in one dimension. Expected competencies It is assumed that you have familiarity with the following concepts/techniques: Solid-state physics: 1D model of a solid, LCAO Text reference The material covered here is discussed in section(s) \\S 11 of The Oxford Solid State Basics Computational content The Jupyter notebook associated with this section can be accessed by clicking the icon below:","title":"Introduction"},{"location":"3-1d/3-3-tightbinding/#tight-binding","text":"In our discussion of covalent bonding, we were able to obtain a solution to the Schr\u00f6dinger equation using the variational method in the framework known as LCAO . We are going to apply the same framework to an infinite chain of atoms, but before we do this, we are going to consider a gentle extension to the simple 2 atom case that we originally considered.","title":"Tight binding"},{"location":"3-1d/3-3-tightbinding/#a-triatomic-molecule","text":"Let us consider a one-dimensional triatomic system as illustrated below: We are going to make the exact same assumptions that we made when we formulated LCAO, namely we are looking at a frozen molecule (no vibrations) with fixed nuclear positions (the Born-Oppenheimer approximation). We assume that the atoms are identical, and the electrons are tightly bound to a given nucleus so that we have (\\hat{K} + \\hat{V}_n)|n\\rangle = \\epsilon_\\textrm{atomic} |n\\rangle where \\hat{K} is the usual kinetic energy operator, \\hat{V}_n is the potential due to the nucleus n and \\epsilon is the energy of the bound electronic state. We again make the crude approximate that our orbitals are orthogonal 1 , such that \\langle i|j\\rangle = \\delta_{i,j}. Now, we can do exactly what we did for a diatomic molecule, namely, consider the trail wavefunction: \\vert \\psi \\rangle = \\varphi_1 |1\\rangle + \\varphi_2 |2\\rangle + \\varphi_3 |3\\rangle. By performing the exact same logic and mathematics we arrive at the matrix equation E \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3 \\end{pmatrix} = \\begin{pmatrix} \\epsilon_0 & -t & 0 \\\\ -t & \\epsilon_0 & -t \\\\ 0 & -t & \\epsilon_0 \\end{pmatrix} \\begin{pmatrix} \\varphi_1 \\\\ \\varphi_2 \\\\ \\varphi_3. \\end{pmatrix} where \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 with V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle the energy shift due to the presence of all other nuclei. It is assumed that hopping occurs only between the nearest neighbours such that \\langle1|H|2\\rangle = \\langle2|H|3\\rangle = -t. 3.3.1: Apply the exact same logic and mathematics to arrive at the above matrix equation Obtaining solutions to such a problem can be unwieldy, but fortunately, we have the perfect tool for the job.","title":"A triatomic molecule"},{"location":"3-1d/3-3-tightbinding/#number-cruncher","text":"We can use our computational toolkit to solve for the eigenvalues of a matrix. For example, with \\epsilon_0 = 2 and t=1 , we find that the eigenvalues for the matrix above are approximately (0.59, 2.00, 3.41) . But what use is this? Well, let's look at a simple case of varying t . 3.3.2: Make a plot of the energy eigenvalues as a function of t running from (0, \\epsilon) and interpret your result. Ensure to include your code. Perhaps the most useful thing to do is similar to that which was introduced in the last section , which is using to produce a histogram sampling of the energy eigenvalues in order to visualise the density of states. If we go ahead an do this: The density of states is not a particularly useful concept with few states, but fortunately, this system scales well and \"grows diagonally\" with an increased number of atoms: as the hopping is limited to nearest neighbours and states are orthogonal, only the diagonal and first-off diagonal elements will be non-zero. So let's go ahead and crank the handle! To infinity and beyond n = 3 n = 10 n = 100 n = 1000 n = 10000 n = 100000 LOL: good luck! That would be one big matrix, much large than my meagre computer can handle. 3.3.3: How do these results compare with what we have seen previously?","title":"Number cruncher"},{"location":"3-1d/3-3-tightbinding/#an-infinite-chain","text":"The above extension of the diatomic system to a triatomic system, especially when coupled with some computational assistance to hone our intuition, we are now going to consider the case of an infinite number of atoms, as depicted below: The potential for the above system, assuming that the locations of atoms are x_n = na with n \\in \\mathbb{Z} , is then: Now we can redeploy the machinery that we used for the diatomic and triatomic systems, namely we formulate the molecular orbital via the LCAO model: \\vert \\Psi \\rangle = \\sum_n \\phi_n |n \\rangle and write the effective Schr\u00f6dinger as \\sum_m H_{nm} \\phi_m = E \\phi_n where the matrix elements of the Hamiltonian are given by H_{nm} = \\langle n | \\hat{H} | m \\rangle It is a bit of work, but one can show that these matrix elements evaluate to H_{nm} = \\epsilon_0 \\delta_{n,m} - t \\left(\\delta_{n+1,m} + \\delta_{n-1,m}\\right) with the same definitions for \\epsilon_0 = \\epsilon_\\textrm{atomic} + V_0 and V_0 = \\sum_{m \\ne j} \\langle m | V_j | m \\rangle as in the triatomic case.","title":"An infinite chain"},{"location":"3-1d/3-3-tightbinding/#solving-the-tight-binding-chain","text":"It will perhaps come as little surprise that the method that we are going to use to solve our effective Schr\u00f6dinger equation is similar to the method we have use to solve for vibrations in a chain, namely to look for plane-wave solutions: \\phi_n = \\frac{e^{-ikna}}{\\sqrt{N}} 3.3.4: How does this form of solution compare to the assumed form of solutions for oscillations? We then crank the handle: \\begin{align} E \\phi_n & = E \\times \\frac{e^{-ikna}}{\\sqrt{N}} \\\\ & = \\sum_m H_{nm} \\phi_m = \\epsilon_0 \\frac{e^{-ikna}}{\\sqrt{N}} - t \\left(\\frac{e^{-ik(n+1)a}}{\\sqrt{N}} - \\frac{e^{-ik(n-1)a}}{\\sqrt{N}}\\right) \\end{align} which yields the energy eigenvalue E = \\epsilon_0 - t \\left(e^{-ika} + e^{ika}\\right) = \\epsilon_0 - 2t\\cos(ka) which is plotted below: There are obvious similarities to the dispersion of oscillations, or phonons, but there are stark differences as compared to the Sommerfeld free electron model: now only a range of energies can be occupied, and this range is referred to as an energy band, and the energy difference between the top and the bottom of this band is called the bandwidth 2 . Further considering the band structure as compared to the free electron model, let us focus on the dispersion relation close to the base of the band at k=0 , where we approximate the energy as E \\approx \\epsilon_0 - 2t + t (ka)^2 \\equiv E_0 + t (ka)^2. If we compare this to the free-electron dispersion relation E=\\frac{\\hbar^2 k^2}{2m} we see that the band structure is similar, but with the lowest available energy is E_0 instead of 0 , and the electrons behave as if they had a different effective mass m^*=\\frac{\\hbar^2}{2ta^2}. For the moment, we leave this as an observation, but we shall return to it later in the course.","title":"Solving the tight-binding chain"},{"location":"3-1d/3-3-tightbinding/#filling-bands","text":"Let us now consider a system of N atoms. Due to the spin degeneracy, at each value k there are 2 possible states which can be occupied, which means for the system there are 2N possible states. The implications of this are significant. Band filling Divalent system Let us consider an atom with two valance electrons: there will be as many electrons as states and thus the every state in the band will be occupied, as illustrated below: Monovalent system Now let us consider monovalent atoms: there will be twice as many states as there are electrons, and thus the band will only be half filled: Note that there is now a Fermi surface - well, Fermi points. Monovalent system with electric field A Fermi surface means that nearby electrons can occupy neighbouring states, for example, under the influence of an electric field: 3.3.5: What is the implication of the plots shown in the band filling content block?","title":"Filling bands"},{"location":"3-1d/3-3-tightbinding/#conclusions","text":"The density of states can be effectively visualised by sampling the dispersion relation and producing a histogram. Moreover, numerical tools are useful, especially when dealing with systems of many particles! Solving Schr\u00f6dinger's equation in the tight-binding framework using LCOA is surprisingly similar to a one-dimensional harmonic chain The tight binding model gives rise to electron bands At the bottom of bands, electrons have a similar dispersion relation to free electrons, but with altered mass","title":"Conclusions"},{"location":"3-1d/3-3-tightbinding/#exercises","text":"","title":"Exercises"},{"location":"3-1d/3-3-tightbinding/#preliminary-provocations","text":"Compare the expression of the effective mass with Newton's second law. Do you observe any similarities? Check the units of the effective mass. Is it what you expect? Calculate the effective mass of the free-electron dispersion relation. Is this what was expected? Under what condition is the effective mass the same for each electron?","title":"Preliminary provocations"},{"location":"3-1d/3-3-tightbinding/#exercise-1-the-next-nearest-neighbour-chain","text":"Let's expand our one-dimensional chain model by extending the range of interaction to include the next-nearest neighbours: \\langle \\phi_n | H | \\phi_{n+2}\\rangle \\equiv -t' \u2260 0. Write down the new Schr\u00f6dinger equation for this system. Solve the Schr\u00f6dinger equation to find the dispersion relation E(k) . Calculate the effective mass m^* . Sketch the effective mass as a function of k for the cases t=2t' , t=4t' and t=10t' . This is not necessary but makes the calculation much simpler, and the qualitative behaviour reproduces that in which we are interested \u21a9 Original, hey? \u21a9","title":"Exercise 1: The next-nearest neighbour chain"},{"location":"solutions/3-3-solutions/","text":"Solutions to exercises \u00b6 Solutions for The specific heat of solids I exercises \u00b6 Solutions for lecture 7 exercises \u00b6 Warm up exercises \u00b6 Check by yourself 2. [m^*] = \\frac{[p^2]}{[E]} = kg 3. m^* = m_e, where m_e is the free electron mass. This is expected because the free elctrons are not subject to a potential If the dispersion relation is parabolic, so in the free electron model. Exercise 1: Next-nearest neighbours chain \u00b6 1. The Schr\u00f6dinger equation is given by: |\\Psi\\rangle = \\sum_n \\phi_n |n\\rangle such that we find E\\phi_n = E_0\\phi_n - t\\phi_{n-1} - t\\phi_{n+1} - t'\\phi_{n-2} - t'\\phi_{n+2} 2. Solving the Schr\u00f6dinger equation yields dispersion: E(k) = E_0 -2t\\cos(ka) -2t'\\cos(2ka) 3. m^* = \\frac{\\hbar^2}{2a^2}\\frac{1}{t\\cos(ka)+4t'\\cos(2ka)} Plot for t=t': k1 = np . linspace ( - pi , - pi / 2 - 0.01 , 300 ); k2 = np . linspace ( - pi / 2 + 0.01 , pi / 2 - 0.01 , 300 ); k3 = np . linspace ( pi / 2 + 0.01 , pi , 300 ); pyplot . plot ( k1 , 1 / ( 5 * np . cos ( k1 )), 'b' ); pyplot . plot ( k2 , 1 / ( 5 * np . cos ( k2 )), 'b' ); pyplot . plot ( k3 , 1 / ( 5 * np . cos ( k3 )), 'b' ); pyplot . xlabel ( r '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - pi , 0 , pi ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([],[]); pyplot . tight_layout (); 4. Plots for 2t'=t, 4t'=t and 10t'=t: def m ( k , t ): return 1 / ( np . cos ( k ) + 4 * t * np . cos ( 2 * k )) k1 = np . linspace ( - 1.6 , - 0.83 , 300 ); k2 = np . linspace ( - 0.826 , 0.826 , 300 ); k3 = np . linspace ( 0.83 , 1.6 , 300 ); pyplot . plot ( k1 , m ( k1 , 2 ), 'b' ); pyplot . plot ( k2 , m ( k2 , 2 ), 'b' ); pyplot . plot ( k3 , m ( k3 , 2 ), 'b' , label = 't=2t \\' ' ); pyplot . xlabel ( '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - 1.6 , 0 , 1.6 ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([ 0 ],[]); pyplot . tight_layout (); k1 = np . linspace ( - 1.58 , - 0.81 , 300 ); k2 = np . linspace ( - 0.804 , 0.804 , 300 ); k3 = np . linspace ( 0.81 , 1.58 , 300 ); pyplot . plot ( k1 , m ( k1 , 4 ), 'r' ); pyplot . plot ( k2 , m ( k2 , 4 ), 'r' ); pyplot . plot ( k3 , m ( k3 , 4 ), 'r' , label = 't=4t \\' ' ); k1 = np . linspace ( - 1.575 , - 0.798 , 300 ); k2 = np . linspace ( - 0.790 , 0.790 , 300 ); k3 = np . linspace ( 0.798 , 1.575 , 300 ); pyplot . plot ( k1 , m ( k1 , 10 ), 'k' ); pyplot . plot ( k2 , m ( k2 , 10 ), 'k' ); pyplot . plot ( k3 , m ( k3 , 10 ), 'k' , label = 't=10t \\' ' ); pyplot . legend ();","title":"Solutions to exercises"},{"location":"solutions/3-3-solutions/#solutions-to-exercises","text":"","title":"Solutions to exercises"},{"location":"solutions/3-3-solutions/#solutions-for-the-specific-heat-of-solids-i-exercises","text":"","title":"Solutions for The specific heat of solids I exercises"},{"location":"solutions/3-3-solutions/#solutions-for-lecture-7-exercises","text":"","title":"Solutions for lecture 7 exercises"},{"location":"solutions/3-3-solutions/#warm-up-exercises","text":"Check by yourself 2. [m^*] = \\frac{[p^2]}{[E]} = kg 3. m^* = m_e, where m_e is the free electron mass. This is expected because the free elctrons are not subject to a potential If the dispersion relation is parabolic, so in the free electron model.","title":"Warm up exercises"},{"location":"solutions/3-3-solutions/#exercise-1-next-nearest-neighbours-chain","text":"1. The Schr\u00f6dinger equation is given by: |\\Psi\\rangle = \\sum_n \\phi_n |n\\rangle such that we find E\\phi_n = E_0\\phi_n - t\\phi_{n-1} - t\\phi_{n+1} - t'\\phi_{n-2} - t'\\phi_{n+2} 2. Solving the Schr\u00f6dinger equation yields dispersion: E(k) = E_0 -2t\\cos(ka) -2t'\\cos(2ka) 3. m^* = \\frac{\\hbar^2}{2a^2}\\frac{1}{t\\cos(ka)+4t'\\cos(2ka)} Plot for t=t': k1 = np . linspace ( - pi , - pi / 2 - 0.01 , 300 ); k2 = np . linspace ( - pi / 2 + 0.01 , pi / 2 - 0.01 , 300 ); k3 = np . linspace ( pi / 2 + 0.01 , pi , 300 ); pyplot . plot ( k1 , 1 / ( 5 * np . cos ( k1 )), 'b' ); pyplot . plot ( k2 , 1 / ( 5 * np . cos ( k2 )), 'b' ); pyplot . plot ( k3 , 1 / ( 5 * np . cos ( k3 )), 'b' ); pyplot . xlabel ( r '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - pi , 0 , pi ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([],[]); pyplot . tight_layout (); 4. Plots for 2t'=t, 4t'=t and 10t'=t: def m ( k , t ): return 1 / ( np . cos ( k ) + 4 * t * np . cos ( 2 * k )) k1 = np . linspace ( - 1.6 , - 0.83 , 300 ); k2 = np . linspace ( - 0.826 , 0.826 , 300 ); k3 = np . linspace ( 0.83 , 1.6 , 300 ); pyplot . plot ( k1 , m ( k1 , 2 ), 'b' ); pyplot . plot ( k2 , m ( k2 , 2 ), 'b' ); pyplot . plot ( k3 , m ( k3 , 2 ), 'b' , label = 't=2t \\' ' ); pyplot . xlabel ( '$k$' ); pyplot . ylabel ( '$m_ {eff} (k)$' ); pyplot . xticks ([ - 1.6 , 0 , 1.6 ],[ r '$-\\pi/a$' , 0 , r '$\\pi/a$' ]); pyplot . yticks ([ 0 ],[]); pyplot . tight_layout (); k1 = np . linspace ( - 1.58 , - 0.81 , 300 ); k2 = np . linspace ( - 0.804 , 0.804 , 300 ); k3 = np . linspace ( 0.81 , 1.58 , 300 ); pyplot . plot ( k1 , m ( k1 , 4 ), 'r' ); pyplot . plot ( k2 , m ( k2 , 4 ), 'r' ); pyplot . plot ( k3 , m ( k3 , 4 ), 'r' , label = 't=4t \\' ' ); k1 = np . linspace ( - 1.575 , - 0.798 , 300 ); k2 = np . linspace ( - 0.790 , 0.790 , 300 ); k3 = np . linspace ( 0.798 , 1.575 , 300 ); pyplot . plot ( k1 , m ( k1 , 10 ), 'k' ); pyplot . plot ( k2 , m ( k2 , 10 ), 'k' ); pyplot . plot ( k3 , m ( k3 , 10 ), 'k' , label = 't=10t \\' ' ); pyplot . legend ();","title":"Exercise 1: Next-nearest neighbours chain"},{"location":"solutions/solutions/","text":"Solutions to exercises \u00b6 Solutions for The specific heat of solids I exercises \u00b6 Preliminary provocations \u00b6 C = 2k_B . Exercise 1: Total heat capacity of a diatomic material \u00b6 Use the formula \\omega = \\sqrt{\\frac{k}{m}} . Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}(2 + 1/2) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}(4 + 1/2). Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}\\left(n_B(\\beta\\hbar\\omega_{^6Li}) + \\frac{1}{2}\\right) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}\\left(n_B(\\beta\\hbar\\omega_{^7Li}) + \\frac{1}{2}\\right). Heat capacity per atom is given by C = \\frac{N_{^6Li}}{N}C_{^6Li} + \\frac{N_{^7Li}}{N}C_{^7Li}, Solutions for The specific heat of solids II exercises \u00b6 Preliminary provocations \u00b6 The polarization is related to the direction of the amplitudes of the waves with respect to the direction of the wave. In 3D, there are only 3 different amplitude directions possible. One can convert the integral as follows: \\int k_x k_y \\rightarrow \\int_{0}^{2\\pi} \\mathrm{d} \\theta \\int_{0}^{\\infty} k \\mathrm{d} k = 2\\pi \\int_{0}^{\\infty} k \\mathrm{d} k The Debye frequency \\omega_D . From the definition of the Debye frequency, one can calculate that the wavelength is of the order of the interatomic spacing: \\lambda = (\\frac{4}{3}\\pi)^{1/3} a. Exercise 1: Debye model - concepts. \u00b6 It is clear that n=4 , and thus k = \\frac{2 \\pi n}{L} = \\pm \\frac{4\\pi}{L} . 2. The number of states per k or per frequency. 4. g(\\omega) = \\frac{dN}{d\\omega} = \\frac{dN}{dk}\\frac{dk}{d\\omega} = \\frac{1}{v}\\frac{dN}{dk}. We assume that in d dimensions there are d polarizations. For 1D we have that N = \\frac{L}{2\\pi}\\int_{-k}^{k} dk , hence g(\\omega) = \\frac{L}{\\pi v} . For 2D we have that N = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int d^2k = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int 2\\pi kdk , hence g(\\omega) = \\frac{L^2\\omega}{\\pi v^2} . For 3D we have that N = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int 4\\pi k^2dk , hence g(\\omega) = \\frac{3L^3\\omega^2}{2\\pi^2v^3} . Exercise 2: Debye model in 2D. \u00b6 See lecture notes. \\begin{align} E &= \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega \\\\ &= \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^2}{e^{x} - 1}dx + C. \\end{align} High temperature implies \\beta \\rightarrow 0 , hence E = \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\frac{(\\beta\\hbar\\omega_D)^2}{2} + C , and then C = \\frac{k_BL^2\\omega^2_D}{2\\pi v^2} = 2Nk_B . We've used the value for \\omega_D calculated from 2N = \\int_{0}^{\\omega_D}g(\\omega)d\\omega . In the low temperature limit we have that \\beta \\rightarrow \\infty , hence E \\approx \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx + C = \\frac{2\\zeta(3)L^2}{\\pi v^2\\hbar^2\\beta^3} + C . Finally C = \\frac{6\\zeta(3)k^3_BL^2}{\\pi v^2\\hbar^2}T^2 . We used the fact that \\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx = 2\\zeta(3) where \\zeta is the Riemann zeta function. Exercise 3: Different oscillation modes. \u00b6 g(\\omega) = \\sum_{\\text{polarizations}}\\frac{dN}{dk}\\frac{dk}{d\\omega} = \\left(\\frac{L}{2\\pi}\\right)^3\\sum_{\\text{polarizations}}4\\pi k^2\\frac{dk}{d\\omega} = \\frac{L^3}{2\\pi^2}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\omega^2 E = \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega = \\frac{L^3}{2\\pi^2\\hbar^3\\beta^4}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^3}{e^{x} - 1}dx + C. Note that we can get \\omega_D from 3N = \\int_{0}^{\\omega_D}g(\\omega) so everything cancels as usual and we are left with the Dulong-Petit law C = 3Nk_B . In the low temperature limit we have that C \\sim \\frac{2\\pi^2k_B^4L^3}{15\\hbar^3}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)T^3 . We used that \\int_{0}^{\\infty}\\frac{x^3}{e^{x} - 1}dx = \\frac{\\pi^4}{15} . Exercise 4: Anisotropic sound velocities. \u00b6 \\begin{align} E & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k\\hbar\\omega(\\mathbf{k})\\left(n_B(\\beta\\hbar\\omega(\\mathbf{k})) + \\frac{1}{2}\\right) \\\\ & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\frac{1}{v_xv_yv_z}\\int d^3\\kappa\\frac{\\hbar\\kappa}{e^{\\beta\\hbar\\kappa} - 1} + C, \\end{align} where we used the substitutions \\kappa_x = k_xv_x,\\kappa_y = k_yv_y, \\kappa_z = k_zv_z . Finally E = \\frac{3\\hbar L^3}{2\\pi^2}\\frac{1}{v_xv_yv_z}\\int_0^{\\kappa_D} d\\kappa\\frac{\\kappa^3}{e^{\\beta\\hbar\\kappa} - 1} + C = \\frac{3L^3}{2\\pi^2\\hbar^3\\beta^4}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} + C, hence C = \\frac{\\partial E}{\\partial T} = \\frac{6k_B^4L^3T^3}{\\pi^2\\hbar^3}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} . We see that the result is similar to the one with the linear dispersion, the only difference is the factor 1/v_xv_yv_z instead of 1/v^3 . Solutions for Electrons in metals I exercises \u00b6 Preliminary provocations \u00b6 How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way. Exercise 1: Extracting quantities from basic Hall measurements \u00b6 We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Hall voltage is measured across the sample width. Hence, V_H = -\\int_{0}^{W} E_ydy where E_y = -v_xB . R_{xy} = -\\frac{B}{ne} , so it does not depend on the sample geometry. Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? If hall resistance and magnetic field are known, the charge density is calculated from R_{xy} = -\\frac{B}{ne} . As V_x = -\\frac{I_x}{ne}B , a stronger field makes Hall voltages easier to measure. Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample? R_{xx} = \\frac{\\rho_{xx}L}{W} where \\rho_{xx} = \\frac{m_e}{ne^2\\tau} . Therefore, scattering time ( \\tau ) is known and R_{xx} depend upon the sample geometry. Exercise 2: Motion of an electron in a magnetic and an electric field \u00b6 1. m\\frac{d\\bf v}{dt} = -e(\\bf v \\times \\bf B) Magnetic field affects only the velocities along x and y, i.e., v_x(t) and v_y(t) as they are perpendicular to it. Therefore, the equations of motion for the electron are \\frac{dv_x}{dt} = -\\frac{e v_y B_z}{m} \\frac{dv_y}{dt} = \\frac{e v_x B_z}{m} We can compute v_x(t) and v_y(t) by solving the differential equations in 1. From v_x'' = -\\frac{e^2B_z^2}{m^2}v_x and the initial conditions, we find v_x(t) = v_0 \\cos(\\omega_c t) with \\omega_c=eB_z/m . From this we can derive v_y(t)=v_0\\sin(\\omega_c t) . We now calculate the particle position using x(t)=x(0) + \\int_0^t v_x(t')dt' (and similar for y(t) ). From this we can find a relation between the x - and y -coordinates of the particle (x(t) - x_0)^2 + (y(t) - y_0)^2 = \\frac{v_0^2}{\\omega_c^2}. This equation describes a circular motion around the point x_0=x(0), y_0=y(0)+v_0/\\omega , where the characteristic frequency \\omega_c is called the cyclotron frequency. Intuition: \\frac{mv^2}{r} = evB (centripetal force = Lorentz force due to magnetic field). Due to the applied electric field \\bf E in the x -direction, the equations of motion acquire an extra term: m v_x' = -e(E_x + v_yB_z). Differentiating w.r.t. time leads to the same 2nd-order D.E. for v_x as above. However, for v_y we get v_y'' = -\\omega_c^2(v_d+v_y), where we defined v_d=\\frac{E_x}{B_z} . The general solutions are v_y(t) = c_1\\sin(\\omega_c t)+ c_2\\cos(\\omega_c t) -v_d \\\\ v_x(t) = c_3\\sin(\\omega_c t)+ c_4\\cos(\\omega_c t). Using the initial conditions v_x(0)=v_0 and v_y(0)=0 and the 1st order D.E. above, we can show v_y(t) = v_0\\sin(\\omega_c t)+ v_d\\cos(\\omega_c t) -v_d \\\\ v_x(t) = v_d\\sin(\\omega_c t)+ v_0\\cos(\\omega_c t). By integrating the expressions for the velocity we find: (x(t)-x_0)^2 + (y(t) - y_0 + v_d t))^2 = \\frac{v_0^2}{\\omega_c^2}. This represents a cycloid : a circular motion around a point that moves in the y -direction with velocity v_d=\\frac{E_x}{B_z} . Exercise 3: Temperature dependence of resistance in the Drude model \u00b6 Find electron density from n_e = \\frac{Z n N_A}{W} where Z is valence of copper atom, n is density, N_A is Avogadro constant and W is atomic weight. Use \\rho = 1/\\sigma from the lecture notes to calculate scattering time. \\sigma = \\frac{n e^2 \\tau}{m} \\lambda = \\langle v \\rangle\\tau Scattering time \\tau \\propto \\frac{1}{\\sqrt{T}} ; \\rho \\propto \\sqrt{T} In general, \\rho \\propto T as the phonons in the system scales linearly with T (remember high temperature limit of Bose-Einstein factor becomes \\frac{kT}{\\hbar\\omega} leading to \\rho \\propto T ). Inability to explain this linear dependence is a failure of the Drude model. Exercise 4: The Hall conductivity matrix and the Hall coefficient \u00b6 \\rho_{xx} is independent of B and \\rho_{xy} \\propto B 2. \\sigma_{xx} = \\frac{\\rho_{xx}}{\\rho_{xx}^2 + \\rho_{xy}^2} \\sigma_{xy} = \\frac{-\\rho_{yx}}{\\rho_{xx}^2 + \\rho_{xy}^2} This describes a Lorentzian . Solutions for Electrons in metals II exercises \u00b6 Warm-up questions \u00b6 1. E = \\int \\limits_0^{\\infty} g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\varepsilon \\mathrm{d} \\varepsilon The electronic heat capacity C_e approaches 3N k_B . Thermal smearing is too significant and we can not accurately approximate the fraction of the excited electron with triangles anymore. Thus the Sommerfeld expansion breaks down. Electrons. Exercise 1: potassium \u00b6 Alkali metals mostly have a spherical Fermi surface. Their energy depends only on the magnitude of the Fermi wavevector. Refer to the lecture notes. Electrons are fermions and obey the Pauli exclusion principle. As electrons cannot occupy the same state, they are forced to occupy higher energy states resulting in high Fermi energy and high Fermi temperature. 4. n = \\frac{N}{V} = \\frac{1}{3 \\pi^{2} \\hbar^{3}}\\left(2 m \\varepsilon_{F}\\right)^{3 / 2} 5. n = \\frac{\\rho N_A Z}{M}, where \\rho is the density, N_A is the Avogadro's constant, M is molar mass and Z is the valence of potassium atom. Comparing total and free electron density, only few electrons are available for conduction which is roughly 1 free electron per potassium atom. Exercise 2: a hypothetical material \u00b6 1. E = \\int_{0}^{\\infty}\\varepsilon g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\textrm{d} \\varepsilon = 2.10^{10}eV^{-\\frac{3}{2}} \\int_{0}^{\\infty}\\frac{\\varepsilon^{\\frac{3}{2}}}{e^\\frac{\\varepsilon-5.2}{k_BT}+1} \\textrm{d} \\varepsilon 2. E = \\frac{4}{5} (5.2)^{\\frac{5}{2}} 10^{10} eV 3. \\begin{align} E(T)-E(T=0) &= \\frac{\\pi^2}{6}(k_B T)^2\\frac{\\partial}{\\partial \\varepsilon}\\left(\\varepsilon g(\\varepsilon)\\right)\\bigg|_{\\varepsilon=\\varepsilon _F}\\\\ &\\approx 8.356 10^8 eV \\end{align} 5. C_v = 1.6713.10^6 eV/K 4, 6. mu = 5.2 kB = 8.617343e-5 T = 1000 #kelvin import numpy as np from scipy import integrate np . seterr ( over = 'ignore' ) # Fermi-Dirac distribution def f ( E , T ): return 1 / ( np . exp (( E - mu ) / ( kB * T )) + 1 ) # Density of states def g ( E ): return 2e10 * np . sqrt ( E ) #integration function def integral ( E , T ): return f ( E , T ) * g ( E ) * E ## Solve integral numerically using scipy's integrate dE = integrate . quad ( integral , 0 , 1e1 , args = ( T ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dT = 0.001 dEplus = integrate . quad ( integral , 0 , 1e1 , args = ( T + dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dEmin = integrate . quad ( integral , 0 , 1e1 , args = ( T - dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) CV = ( dEplus - dEmin ) / ( 2 * dT ); print ( f 'dE = { dE : .4e } eV' ) print ( f 'Cv = { CV : .4e } eV/K' ) Check the source code written in python for solving integral using midpoint rule. Exercise 3: graphene \u00b6 1. import numpy as np import matplotlib.pyplot as plt x = np . linspace ( - 1 , 1 , 100 ) fig , ax = plt . subplots ( figsize = ( 7 , 5 )) ax . plot ( x , x , 'b' ) ax . plot ( x , - x , 'b' ) ax . spines [ 'left' ] . set_position ( 'center' ) ax . spines [ 'bottom' ] . set_position (( 'data' , 0.0 )) # Eliminate upper and right axes ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . set_xticks ([]) ax . set_yticks ([]) ax . set_xlabel ( r '$\\mid \\vec k \\mid$' , fontsize = 14 ) ax . set_ylabel ( r '$\\varepsilon$' , fontsize = 18 , rotation = 'horizontal' ) ax . yaxis . set_label_coords ( 0.5 , 1 ) ax . xaxis . set_label_coords ( 1.0 , 0.49 ) 2.The DOS for the positive energies is given by g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{\\varepsilon}{c^2}, where 2_s is the spin degeneracy and 2_v is the valley degeneracy. If we account for the negative energies as well, we obtain g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{|\\varepsilon|}{c^2}. 3. g(\\varepsilon) vs \\varepsilon is a linear plot. Here, the region marked by -k_B T is a triangle whose area gives the number of electrons that can be excited: \\begin{align} n_{ex} &= \\frac{1}{2} g(-k_B T) k_B T\\\\ &= \\frac{L^2 k_B^2T^2}{\\pi c^2}. \\end{align} From this it follows that the energy difference is given by E(T) - E_0 = \\frac{L^2 k_B^3T^3}{\\pi c^2}. 4. C_v(T) = \\frac{\\partial E}{\\partial T} = \\frac{3L^2k_B^3T^2}{\\pi c^2} Solutions for Chemistry 101 exercises \u00b6 Exercise 1: Shell-filling model of atoms \u00b6 See lecture notes. The atomic number of Tungsten is 74: 1\\textrm{s}^2 2\\textrm{s}^2 2\\textrm{p}^6 3\\textrm{s}^23p^64s^23d^{10}4p^65s^24d^{10}5p^66s^24f^{14}5d^4 \\begin{align} \\textrm{Cu} &= [\\textrm{Ar}]4s^23d^9\\\\ \\textrm{Pd} &= [\\textrm{Kr}]5s^24d^8\\\\ \\textrm{Ag} &= [\\textrm{Kr}]5s^24d^9\\\\ \\textrm{Au} &= [\\textrm{Xe}]6s^24f^{14}5d^9 \\end{align} The wheels fall off as V(\\mathbf{r}) \\ne 1/|r| Exercise 2: Application of the LCAO model to the delta-function potential \u00b6 \\psi(x) = \\begin{cases} &\\sqrt{\u03ba}e^{\u03ba(x-x_1)}, x<x_1\\\\ &\\sqrt{\u03ba}e^{-\u03ba(x-x_1)}, x>x_1 \\end{cases} Where \u03ba = \\sqrt{\\frac{-2mE}{\u0127^2}} = \\frac{mV_0}{\u0127^2} . The energy is given by \u03f5_1 = \u03f5_2 = -\\frac{mV_0^2}{2\u0127^2} The wave function of a single delta peak is given by \\psi_1(x) = \\frac{\\sqrt{mV_0}}{\u0127}e^{-\\frac{mV_0}{\u0127^2}|x-x_1|} \\psi_2(x) can be found by replacing x_1 by x_2 H = -\\frac{mV_0^2}{\u0127^2}\\begin{pmatrix} 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) & \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|)\\\\ \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|) & 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) \\end{pmatrix} \u03f5_{\\pm} = \\beta(1/2+\\exp(-2\\alpha) \\pm \\exp(-\\alpha)) Where \\beta = -\\frac{mV_0^2}{\u0127^2} and \u03b1 = \\frac{mV_0}{\u0127^2}|x_2-x_1| Exercise 3: Polarization of a hydrogen molecule \u00b6 1. H_{\\mathcal{E}} = ex\\mathcal{E}, 2. \\hat{H} = \\begin{pmatrix} E_0 & -t\\\\ -t & E_0 \\end{pmatrix} +\\begin{pmatrix} \u27e81|ex\\mathcal{E}|1\u27e9 & \u27e81|ex\\mathcal{E}|2\u27e9\\\\ \u27e82|ex\\mathcal{E}|1\u27e9 & \u27e82|ex\\mathcal{E}|2\u27e9 \\end{pmatrix} = \\begin{pmatrix} E_0 - \\gamma & -t\\\\ -t & E_0 + \\gamma \\end{pmatrix}, where \\gamma = e d \\mathcal{E}/2 and have used \u27e81|ex\\mathcal{E}|1\u27e9 = -e d \\mathcal{E}/2\u27e81|1\u27e9 = -e d \\mathcal{E}/2 3. The eigenstates of the Hamiltonian are given by: E_{\\pm} = E_0\\pm\\sqrt{t^2+\\gamma^2} The ground state wave function is: \\begin{split} |\\psi\u27e9 &= \\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}\\begin{pmatrix} \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{t}\\\\ 1 \\end{pmatrix}\\\\ |\\psi\u27e9 &= \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|1\u27e9+\\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|2\u27e9 \\end{split} 4. P = -\\frac{2\\gamma^2}{\\mathcal{E}}(\\frac{1}{\\sqrt{\\gamma^2+t^2}})","title":"Solutions to exercises"},{"location":"solutions/solutions/#solutions-to-exercises","text":"","title":"Solutions to exercises"},{"location":"solutions/solutions/#solutions-for-the-specific-heat-of-solids-i-exercises","text":"","title":"Solutions for The specific heat of solids I exercises"},{"location":"solutions/solutions/#preliminary-provocations","text":"C = 2k_B .","title":"Preliminary provocations"},{"location":"solutions/solutions/#exercise-1-total-heat-capacity-of-a-diatomic-material","text":"Use the formula \\omega = \\sqrt{\\frac{k}{m}} . Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}(2 + 1/2) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}(4 + 1/2). Energy per atom is given by E = \\frac{N_{^6Li}}{N}\\hbar\\omega_{^6Li}\\left(n_B(\\beta\\hbar\\omega_{^6Li}) + \\frac{1}{2}\\right) + \\frac{N_{^7Li}}{N}\\hbar\\omega_{^7Li}\\left(n_B(\\beta\\hbar\\omega_{^7Li}) + \\frac{1}{2}\\right). Heat capacity per atom is given by C = \\frac{N_{^6Li}}{N}C_{^6Li} + \\frac{N_{^7Li}}{N}C_{^7Li},","title":"Exercise 1: Total heat capacity of a diatomic material"},{"location":"solutions/solutions/#solutions-for-the-specific-heat-of-solids-ii-exercises","text":"","title":"Solutions for The specific heat of solids II exercises"},{"location":"solutions/solutions/#preliminary-provocations_1","text":"The polarization is related to the direction of the amplitudes of the waves with respect to the direction of the wave. In 3D, there are only 3 different amplitude directions possible. One can convert the integral as follows: \\int k_x k_y \\rightarrow \\int_{0}^{2\\pi} \\mathrm{d} \\theta \\int_{0}^{\\infty} k \\mathrm{d} k = 2\\pi \\int_{0}^{\\infty} k \\mathrm{d} k The Debye frequency \\omega_D . From the definition of the Debye frequency, one can calculate that the wavelength is of the order of the interatomic spacing: \\lambda = (\\frac{4}{3}\\pi)^{1/3} a.","title":"Preliminary provocations"},{"location":"solutions/solutions/#exercise-1-debye-model-concepts","text":"It is clear that n=4 , and thus k = \\frac{2 \\pi n}{L} = \\pm \\frac{4\\pi}{L} . 2. The number of states per k or per frequency. 4. g(\\omega) = \\frac{dN}{d\\omega} = \\frac{dN}{dk}\\frac{dk}{d\\omega} = \\frac{1}{v}\\frac{dN}{dk}. We assume that in d dimensions there are d polarizations. For 1D we have that N = \\frac{L}{2\\pi}\\int_{-k}^{k} dk , hence g(\\omega) = \\frac{L}{\\pi v} . For 2D we have that N = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int d^2k = 2\\left(\\frac{L}{2\\pi}\\right)^2\\int 2\\pi kdk , hence g(\\omega) = \\frac{L^2\\omega}{\\pi v^2} . For 3D we have that N = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int 4\\pi k^2dk , hence g(\\omega) = \\frac{3L^3\\omega^2}{2\\pi^2v^3} .","title":"Exercise 1: Debye model - concepts."},{"location":"solutions/solutions/#exercise-2-debye-model-in-2d","text":"See lecture notes. \\begin{align} E &= \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega \\\\ &= \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^2}{e^{x} - 1}dx + C. \\end{align} High temperature implies \\beta \\rightarrow 0 , hence E = \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\frac{(\\beta\\hbar\\omega_D)^2}{2} + C , and then C = \\frac{k_BL^2\\omega^2_D}{2\\pi v^2} = 2Nk_B . We've used the value for \\omega_D calculated from 2N = \\int_{0}^{\\omega_D}g(\\omega)d\\omega . In the low temperature limit we have that \\beta \\rightarrow \\infty , hence E \\approx \\frac{L^2}{\\pi v^2\\hbar^2\\beta^3}\\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx + C = \\frac{2\\zeta(3)L^2}{\\pi v^2\\hbar^2\\beta^3} + C . Finally C = \\frac{6\\zeta(3)k^3_BL^2}{\\pi v^2\\hbar^2}T^2 . We used the fact that \\int_{0}^{\\infty}\\frac{x^2}{e^{x} - 1}dx = 2\\zeta(3) where \\zeta is the Riemann zeta function.","title":"Exercise 2: Debye model in 2D."},{"location":"solutions/solutions/#exercise-3-different-oscillation-modes","text":"g(\\omega) = \\sum_{\\text{polarizations}}\\frac{dN}{dk}\\frac{dk}{d\\omega} = \\left(\\frac{L}{2\\pi}\\right)^3\\sum_{\\text{polarizations}}4\\pi k^2\\frac{dk}{d\\omega} = \\frac{L^3}{2\\pi^2}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\omega^2 E = \\int_{0}^{\\omega_D}g(\\omega)\\hbar\\omega\\left(\\frac{1}{e^{\\beta\\hbar\\omega} - 1} + \\frac{1}{2}\\right)d\\omega = \\frac{L^3}{2\\pi^2\\hbar^3\\beta^4}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)\\int_{0}^{\\beta\\hbar\\omega_D}\\frac{x^3}{e^{x} - 1}dx + C. Note that we can get \\omega_D from 3N = \\int_{0}^{\\omega_D}g(\\omega) so everything cancels as usual and we are left with the Dulong-Petit law C = 3Nk_B . In the low temperature limit we have that C \\sim \\frac{2\\pi^2k_B^4L^3}{15\\hbar^3}\\left(\\frac{2}{v_\\perp^3} + \\frac{1}{v_\\parallel^3}\\right)T^3 . We used that \\int_{0}^{\\infty}\\frac{x^3}{e^{x} - 1}dx = \\frac{\\pi^4}{15} .","title":"Exercise 3: Different oscillation modes."},{"location":"solutions/solutions/#exercise-4-anisotropic-sound-velocities","text":"\\begin{align} E & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\int d^3k\\hbar\\omega(\\mathbf{k})\\left(n_B(\\beta\\hbar\\omega(\\mathbf{k})) + \\frac{1}{2}\\right) \\\\ & = 3\\left(\\frac{L}{2\\pi}\\right)^3\\frac{1}{v_xv_yv_z}\\int d^3\\kappa\\frac{\\hbar\\kappa}{e^{\\beta\\hbar\\kappa} - 1} + C, \\end{align} where we used the substitutions \\kappa_x = k_xv_x,\\kappa_y = k_yv_y, \\kappa_z = k_zv_z . Finally E = \\frac{3\\hbar L^3}{2\\pi^2}\\frac{1}{v_xv_yv_z}\\int_0^{\\kappa_D} d\\kappa\\frac{\\kappa^3}{e^{\\beta\\hbar\\kappa} - 1} + C = \\frac{3L^3}{2\\pi^2\\hbar^3\\beta^4}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} + C, hence C = \\frac{\\partial E}{\\partial T} = \\frac{6k_B^4L^3T^3}{\\pi^2\\hbar^3}\\frac{1}{v_xv_yv_z}\\int_0^{\\beta\\hbar\\kappa_D} dx\\frac{x^3}{e^{x} - 1} . We see that the result is similar to the one with the linear dispersion, the only difference is the factor 1/v_xv_yv_z instead of 1/v^3 .","title":"Exercise 4: Anisotropic sound velocities."},{"location":"solutions/solutions/#solutions-for-electrons-in-metals-i-exercises","text":"","title":"Solutions for Electrons in metals I exercises"},{"location":"solutions/solutions/#preliminary-provocations_2","text":"How does the resistance of a purely 2D material depend on its size? Check that the units of mobility and the Hall coefficient are correct. (As you should always do!) Explain why the scattering times due to different types of scattering events add up in a reciprocal way.","title":"Preliminary provocations"},{"location":"solutions/solutions/#exercise-1-extracting-quantities-from-basic-hall-measurements","text":"We apply a magnetic field \\bf B along the z -direction to a planar (two-dimensional) sample that sits in the xy plane. The sample has width W in the y -direction, length L in the x -direction and we apply a current I along the x -direction. What is the relation between the electric field and the electric potential? V_b - V_a = -\\int_{\\Gamma} \\mathbf{E} \\cdot d\\mathbf{\\ell} if \\Gamma is a path from a to b . Suppose we measure a Hall voltage V_H . Express the Hall resistance R_{xy} = V_H/I as a function of magnetic field. Does R_{xy} depend on the geometry of the sample? Also express R_{xy} in terms of the Hall coefficient R_H . Hall voltage is measured across the sample width. Hence, V_H = -\\int_{0}^{W} E_ydy where E_y = -v_xB . R_{xy} = -\\frac{B}{ne} , so it does not depend on the sample geometry. Assuming we control the magnetic field \\mathbf{B} , what quantity can we extract from a measurement of the Hall resistance? Would a large or a small magnetic field give a Hall voltage that is easier to measure? If hall resistance and magnetic field are known, the charge density is calculated from R_{xy} = -\\frac{B}{ne} . As V_x = -\\frac{I_x}{ne}B , a stronger field makes Hall voltages easier to measure. Express the longitudinal resistance R=V/I , where V is the voltage difference over the sample along the x direction, in terms of the longitudinal resistivity \u03c1_{xx} . Suppose we extracted n from a measurement of the Hall resistance, what quantity can we extract from a measurement of the longitudinal resistance? Does the result depend on the geometry of the sample? R_{xx} = \\frac{\\rho_{xx}L}{W} where \\rho_{xx} = \\frac{m_e}{ne^2\\tau} . Therefore, scattering time ( \\tau ) is known and R_{xx} depend upon the sample geometry.","title":"Exercise 1: Extracting quantities from basic Hall measurements"},{"location":"solutions/solutions/#exercise-2-motion-of-an-electron-in-a-magnetic-and-an-electric-field","text":"1. m\\frac{d\\bf v}{dt} = -e(\\bf v \\times \\bf B) Magnetic field affects only the velocities along x and y, i.e., v_x(t) and v_y(t) as they are perpendicular to it. Therefore, the equations of motion for the electron are \\frac{dv_x}{dt} = -\\frac{e v_y B_z}{m} \\frac{dv_y}{dt} = \\frac{e v_x B_z}{m} We can compute v_x(t) and v_y(t) by solving the differential equations in 1. From v_x'' = -\\frac{e^2B_z^2}{m^2}v_x and the initial conditions, we find v_x(t) = v_0 \\cos(\\omega_c t) with \\omega_c=eB_z/m . From this we can derive v_y(t)=v_0\\sin(\\omega_c t) . We now calculate the particle position using x(t)=x(0) + \\int_0^t v_x(t')dt' (and similar for y(t) ). From this we can find a relation between the x - and y -coordinates of the particle (x(t) - x_0)^2 + (y(t) - y_0)^2 = \\frac{v_0^2}{\\omega_c^2}. This equation describes a circular motion around the point x_0=x(0), y_0=y(0)+v_0/\\omega , where the characteristic frequency \\omega_c is called the cyclotron frequency. Intuition: \\frac{mv^2}{r} = evB (centripetal force = Lorentz force due to magnetic field). Due to the applied electric field \\bf E in the x -direction, the equations of motion acquire an extra term: m v_x' = -e(E_x + v_yB_z). Differentiating w.r.t. time leads to the same 2nd-order D.E. for v_x as above. However, for v_y we get v_y'' = -\\omega_c^2(v_d+v_y), where we defined v_d=\\frac{E_x}{B_z} . The general solutions are v_y(t) = c_1\\sin(\\omega_c t)+ c_2\\cos(\\omega_c t) -v_d \\\\ v_x(t) = c_3\\sin(\\omega_c t)+ c_4\\cos(\\omega_c t). Using the initial conditions v_x(0)=v_0 and v_y(0)=0 and the 1st order D.E. above, we can show v_y(t) = v_0\\sin(\\omega_c t)+ v_d\\cos(\\omega_c t) -v_d \\\\ v_x(t) = v_d\\sin(\\omega_c t)+ v_0\\cos(\\omega_c t). By integrating the expressions for the velocity we find: (x(t)-x_0)^2 + (y(t) - y_0 + v_d t))^2 = \\frac{v_0^2}{\\omega_c^2}. This represents a cycloid : a circular motion around a point that moves in the y -direction with velocity v_d=\\frac{E_x}{B_z} .","title":"Exercise 2: Motion of an electron in a magnetic and an electric field"},{"location":"solutions/solutions/#exercise-3-temperature-dependence-of-resistance-in-the-drude-model","text":"Find electron density from n_e = \\frac{Z n N_A}{W} where Z is valence of copper atom, n is density, N_A is Avogadro constant and W is atomic weight. Use \\rho = 1/\\sigma from the lecture notes to calculate scattering time. \\sigma = \\frac{n e^2 \\tau}{m} \\lambda = \\langle v \\rangle\\tau Scattering time \\tau \\propto \\frac{1}{\\sqrt{T}} ; \\rho \\propto \\sqrt{T} In general, \\rho \\propto T as the phonons in the system scales linearly with T (remember high temperature limit of Bose-Einstein factor becomes \\frac{kT}{\\hbar\\omega} leading to \\rho \\propto T ). Inability to explain this linear dependence is a failure of the Drude model.","title":"Exercise 3: Temperature dependence of resistance in the Drude model"},{"location":"solutions/solutions/#exercise-4-the-hall-conductivity-matrix-and-the-hall-coefficient","text":"\\rho_{xx} is independent of B and \\rho_{xy} \\propto B 2. \\sigma_{xx} = \\frac{\\rho_{xx}}{\\rho_{xx}^2 + \\rho_{xy}^2} \\sigma_{xy} = \\frac{-\\rho_{yx}}{\\rho_{xx}^2 + \\rho_{xy}^2} This describes a Lorentzian .","title":"Exercise 4: The Hall conductivity matrix and the Hall coefficient"},{"location":"solutions/solutions/#solutions-for-electrons-in-metals-ii-exercises","text":"","title":"Solutions for Electrons in metals II exercises"},{"location":"solutions/solutions/#warm-up-questions","text":"1. E = \\int \\limits_0^{\\infty} g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\varepsilon \\mathrm{d} \\varepsilon The electronic heat capacity C_e approaches 3N k_B . Thermal smearing is too significant and we can not accurately approximate the fraction of the excited electron with triangles anymore. Thus the Sommerfeld expansion breaks down. Electrons.","title":"Warm-up questions"},{"location":"solutions/solutions/#exercise-1-potassium","text":"Alkali metals mostly have a spherical Fermi surface. Their energy depends only on the magnitude of the Fermi wavevector. Refer to the lecture notes. Electrons are fermions and obey the Pauli exclusion principle. As electrons cannot occupy the same state, they are forced to occupy higher energy states resulting in high Fermi energy and high Fermi temperature. 4. n = \\frac{N}{V} = \\frac{1}{3 \\pi^{2} \\hbar^{3}}\\left(2 m \\varepsilon_{F}\\right)^{3 / 2} 5. n = \\frac{\\rho N_A Z}{M}, where \\rho is the density, N_A is the Avogadro's constant, M is molar mass and Z is the valence of potassium atom. Comparing total and free electron density, only few electrons are available for conduction which is roughly 1 free electron per potassium atom.","title":"Exercise 1: potassium"},{"location":"solutions/solutions/#exercise-2-a-hypothetical-material","text":"1. E = \\int_{0}^{\\infty}\\varepsilon g(\\varepsilon) n_{F}(\\beta (\\varepsilon - \\mu)) \\textrm{d} \\varepsilon = 2.10^{10}eV^{-\\frac{3}{2}} \\int_{0}^{\\infty}\\frac{\\varepsilon^{\\frac{3}{2}}}{e^\\frac{\\varepsilon-5.2}{k_BT}+1} \\textrm{d} \\varepsilon 2. E = \\frac{4}{5} (5.2)^{\\frac{5}{2}} 10^{10} eV 3. \\begin{align} E(T)-E(T=0) &= \\frac{\\pi^2}{6}(k_B T)^2\\frac{\\partial}{\\partial \\varepsilon}\\left(\\varepsilon g(\\varepsilon)\\right)\\bigg|_{\\varepsilon=\\varepsilon _F}\\\\ &\\approx 8.356 10^8 eV \\end{align} 5. C_v = 1.6713.10^6 eV/K 4, 6. mu = 5.2 kB = 8.617343e-5 T = 1000 #kelvin import numpy as np from scipy import integrate np . seterr ( over = 'ignore' ) # Fermi-Dirac distribution def f ( E , T ): return 1 / ( np . exp (( E - mu ) / ( kB * T )) + 1 ) # Density of states def g ( E ): return 2e10 * np . sqrt ( E ) #integration function def integral ( E , T ): return f ( E , T ) * g ( E ) * E ## Solve integral numerically using scipy's integrate dE = integrate . quad ( integral , 0 , 1e1 , args = ( T ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dT = 0.001 dEplus = integrate . quad ( integral , 0 , 1e1 , args = ( T + dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) dEmin = integrate . quad ( integral , 0 , 1e1 , args = ( T - dT ))[ 0 ] - 0.8e10 * 5.2 ** ( 5. / 2 ) CV = ( dEplus - dEmin ) / ( 2 * dT ); print ( f 'dE = { dE : .4e } eV' ) print ( f 'Cv = { CV : .4e } eV/K' ) Check the source code written in python for solving integral using midpoint rule.","title":"Exercise 2: a hypothetical material"},{"location":"solutions/solutions/#exercise-3-graphene","text":"1. import numpy as np import matplotlib.pyplot as plt x = np . linspace ( - 1 , 1 , 100 ) fig , ax = plt . subplots ( figsize = ( 7 , 5 )) ax . plot ( x , x , 'b' ) ax . plot ( x , - x , 'b' ) ax . spines [ 'left' ] . set_position ( 'center' ) ax . spines [ 'bottom' ] . set_position (( 'data' , 0.0 )) # Eliminate upper and right axes ax . spines [ 'right' ] . set_color ( 'none' ) ax . spines [ 'top' ] . set_color ( 'none' ) ax . set_xticks ([]) ax . set_yticks ([]) ax . set_xlabel ( r '$\\mid \\vec k \\mid$' , fontsize = 14 ) ax . set_ylabel ( r '$\\varepsilon$' , fontsize = 18 , rotation = 'horizontal' ) ax . yaxis . set_label_coords ( 0.5 , 1 ) ax . xaxis . set_label_coords ( 1.0 , 0.49 ) 2.The DOS for the positive energies is given by g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{\\varepsilon}{c^2}, where 2_s is the spin degeneracy and 2_v is the valley degeneracy. If we account for the negative energies as well, we obtain g(\\varepsilon) = 2_s 2_v 2 \\pi \\left(\\frac{L}{2 \\pi}\\right)^2 \\frac{|\\varepsilon|}{c^2}. 3. g(\\varepsilon) vs \\varepsilon is a linear plot. Here, the region marked by -k_B T is a triangle whose area gives the number of electrons that can be excited: \\begin{align} n_{ex} &= \\frac{1}{2} g(-k_B T) k_B T\\\\ &= \\frac{L^2 k_B^2T^2}{\\pi c^2}. \\end{align} From this it follows that the energy difference is given by E(T) - E_0 = \\frac{L^2 k_B^3T^3}{\\pi c^2}. 4. C_v(T) = \\frac{\\partial E}{\\partial T} = \\frac{3L^2k_B^3T^2}{\\pi c^2}","title":"Exercise 3: graphene"},{"location":"solutions/solutions/#solutions-for-chemistry-101-exercises","text":"","title":"Solutions for Chemistry 101 exercises"},{"location":"solutions/solutions/#exercise-1-shell-filling-model-of-atoms","text":"See lecture notes. The atomic number of Tungsten is 74: 1\\textrm{s}^2 2\\textrm{s}^2 2\\textrm{p}^6 3\\textrm{s}^23p^64s^23d^{10}4p^65s^24d^{10}5p^66s^24f^{14}5d^4 \\begin{align} \\textrm{Cu} &= [\\textrm{Ar}]4s^23d^9\\\\ \\textrm{Pd} &= [\\textrm{Kr}]5s^24d^8\\\\ \\textrm{Ag} &= [\\textrm{Kr}]5s^24d^9\\\\ \\textrm{Au} &= [\\textrm{Xe}]6s^24f^{14}5d^9 \\end{align} The wheels fall off as V(\\mathbf{r}) \\ne 1/|r|","title":"Exercise 1: Shell-filling model of atoms"},{"location":"solutions/solutions/#exercise-2-application-of-the-lcao-model-to-the-delta-function-potential","text":"\\psi(x) = \\begin{cases} &\\sqrt{\u03ba}e^{\u03ba(x-x_1)}, x<x_1\\\\ &\\sqrt{\u03ba}e^{-\u03ba(x-x_1)}, x>x_1 \\end{cases} Where \u03ba = \\sqrt{\\frac{-2mE}{\u0127^2}} = \\frac{mV_0}{\u0127^2} . The energy is given by \u03f5_1 = \u03f5_2 = -\\frac{mV_0^2}{2\u0127^2} The wave function of a single delta peak is given by \\psi_1(x) = \\frac{\\sqrt{mV_0}}{\u0127}e^{-\\frac{mV_0}{\u0127^2}|x-x_1|} \\psi_2(x) can be found by replacing x_1 by x_2 H = -\\frac{mV_0^2}{\u0127^2}\\begin{pmatrix} 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) & \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|)\\\\ \\exp(-\\frac{mV_0}{\u0127^2}|x_2-x_1|) & 1/2+\\exp(-\\frac{2mV_0}{\u0127^2}|x_2-x_1|) \\end{pmatrix} \u03f5_{\\pm} = \\beta(1/2+\\exp(-2\\alpha) \\pm \\exp(-\\alpha)) Where \\beta = -\\frac{mV_0^2}{\u0127^2} and \u03b1 = \\frac{mV_0}{\u0127^2}|x_2-x_1|","title":"Exercise 2: Application of the LCAO model to the delta-function potential"},{"location":"solutions/solutions/#exercise-3-polarization-of-a-hydrogen-molecule","text":"1. H_{\\mathcal{E}} = ex\\mathcal{E}, 2. \\hat{H} = \\begin{pmatrix} E_0 & -t\\\\ -t & E_0 \\end{pmatrix} +\\begin{pmatrix} \u27e81|ex\\mathcal{E}|1\u27e9 & \u27e81|ex\\mathcal{E}|2\u27e9\\\\ \u27e82|ex\\mathcal{E}|1\u27e9 & \u27e82|ex\\mathcal{E}|2\u27e9 \\end{pmatrix} = \\begin{pmatrix} E_0 - \\gamma & -t\\\\ -t & E_0 + \\gamma \\end{pmatrix}, where \\gamma = e d \\mathcal{E}/2 and have used \u27e81|ex\\mathcal{E}|1\u27e9 = -e d \\mathcal{E}/2\u27e81|1\u27e9 = -e d \\mathcal{E}/2 3. The eigenstates of the Hamiltonian are given by: E_{\\pm} = E_0\\pm\\sqrt{t^2+\\gamma^2} The ground state wave function is: \\begin{split} |\\psi\u27e9 &= \\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}\\begin{pmatrix} \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{t}\\\\ 1 \\end{pmatrix}\\\\ |\\psi\u27e9 &= \\frac{\\gamma+\\sqrt{t^2+\\gamma^2}}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|1\u27e9+\\frac{t}{\\sqrt{(\\gamma+\\sqrt{\\gamma^2+t^2})^2+t^2}}|2\u27e9 \\end{split} 4. P = -\\frac{2\\gamma^2}{\\mathcal{E}}(\\frac{1}{\\sqrt{\\gamma^2+t^2}})","title":"Exercise 3: Polarization of a hydrogen molecule"}]}